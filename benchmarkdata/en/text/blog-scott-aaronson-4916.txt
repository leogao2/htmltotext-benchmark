The Busy Beaver Frontier
Update (July 27): I now have a substantially revised and expanded version (now revised and expanded even a second time), which incorporates (among other things) the extensive feedback that I got from this blog post. There are new philosophical remarks, some lovely new open problems, and an even-faster-growing (!) integer sequence. Check it out!

A life that was all covid, cancellations, and Trump, all desperate rearguard defense of the beleaguered ideals of the Enlightenment, would hardly be worth living. So it was an exquisite delight, these past two weeks, to forget current events and write an 18-page survey article about the Busy Beaver function: the staggeringly quickly-growing function that probably encodes a huge portion of all interesting mathematical truth in its first hundred values, if only we could know those values or exploit them if we did.

Without further ado, hereâ€™s the title, abstract, and link:

The Busy Beaver Frontier
by Scott Aaronson

The Busy Beaver function, with its incomprehensibly rapid growth, has captivated generations of computer scientists, mathematicians, and hobbyists. In this survey, I offer a personal view of the BB function 58 years after its introduction, emphasizing lesser-known insights, recent progress, and especially favorite open problems. Examples of such problems include: when does the BB function first exceed the Ackermann function? Is the value of BB(20) independent of set theory? Can we prove that BB(n+1)>2BB(n) for large enough n? Given BB(n), how many advice bits are needed to compute BB(n+1)? Do all Busy Beavers halt on all inputs, not just the 0 input? Is it decidable whether BB(n) is even or odd?

The article is slated to appear soon in SIGACT News. Iâ€™m grateful to Bill Gasarch for suggesting itâ€”even with everything else going on, this was a commission I felt I couldnâ€™t turn down!

Besides Bill, Iâ€™m grateful to the various Busy Beaver experts who answered my inquiries, to Marijn Heule and Andy Drucker for suggesting some of the open problems, to Marijn for creating a figure, and to Lily, my 7-year-old daughter, for raising the question about the first value of n at which the Busy Beaver function exceeds the Ackermann function. (Yes, Lilyâ€™s covid homeschooling has included multiple lessons on very large positive integers.)

There are still a few days until I have to deliver the final version. So if you spot anything wrong or in need of improvement, donâ€™t hesitate to leave a comment or send an email. Thanks in advance!

Of course Busy Beaver has been an obsession that Iâ€™ve returned to many times in my life: for example, in that Who Can Name the Bigger Number? essay that I wrote way back when I was 18, in Quantum Computing Since Democritus, in my public lecture at Festivaletteratura, and in my 2016 paper with Adam Yedidia that showed that the values of all Busy Beaver numbers beyond the 7910th are independent of the axioms of set theory (Stefan Oâ€™Rear has since shown that independence starts at the 748th value or sooner). This survey, however, represents the first time Iâ€™ve tried to take stock of BusyBeaverology as a research topicâ€”collecting in one place all the lesser-known theorems and empirical observations and open problems that I found the most striking, in the hope of inspiring not just contemplation or wonderment but actual progress.

Within the last few months, the world of deep mathematics that you can actually explain to a child lost two of its greatest giants: John Conway (who died of covid, and who I eulogized here) and Ron Graham. One thing I found poignant, and that I didnâ€™t know before I started writing, is that Conway and Graham both play significant roles in the story of the Busy Beaver function. Conway, because most of the best known candidates for Busy Beaver Turing machines turn out, when you analyze them, to be testing variants of the notorious Collatz Conjectureâ€”and Conway is the one who proved, in 1972, that the set of â€œCollatz-like questionsâ€ is Turing-undecidable. And Graham because of Grahamâ€™s number from Ramsey theoryâ€”a candidate for the biggest number thatâ€™s ever played a role in mathematical researchâ€”and because of the discovery, four years ago, that the 18th Busy Beaver number exceeds Grahamâ€™s number.

(â€œJust how big is Grahamâ€™s number? So big that the 17th Busy Beaver number is not yet known to exceed it!â€)

Anyway, I tried to make the survey pretty accessible, while still providing enough technical content to sink oneâ€™s two overgrown front teeth into (donâ€™t worry, there are no such puns in the piece itself). I hope you like reading it at least 1/BB(10) as much as I liked writing it.

Update (July 24): Longtime commenter Joshua Zelinsky gently reminded me that one of the main questions discussed in the surveyâ€”namely, whether we can prove BB(n+1)>2BB(n) for all large enough nâ€”was first brought to my attention by him, Joshua, in a 2013 Ask-Me-Anything session on this blog! I apologize to Joshua for the major oversight, which has now been corrected. On the positive side, we just got a powerful demonstration both of the intellectual benefits of blogging, and of the benefits of sharing paper drafts on oneâ€™s blog before sending them to the editor!

Email, RSS Follow
This entry was posted on Thursday, July 23rd, 2020 at 1:42 am and is filed under Complexity, Nerd Interest. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.

310 Responses to â€œThe Busy Beaver Frontierâ€
Sniffnoy Says:
Comment #1 July 23rd, 2020 at 2:57 am
Interesting summary!

Two quick notes:

1. Conjectures 11 and 12 are interesting. Particularly because, I remember back in the thread where this was all being discussed, I asked Stefan Oâ€™Rear if he could get fewer states by focusing on PA rather than ZF, and he was like, actually, ZF is easier! But likeâ€¦ ultimately that canâ€™t be the case, right? And yet it is surprising that at the moment nobody sems to know how to do better on PA than on ZF.

2. I have to nitpick, but my understanding is that the significance of Grahamâ€™s number is largely a fiction; there was never actually any version of Grahamâ€™s proof that used this number as an upper bound. Or at least thatâ€™s what John Baez says Graham told him.

And, heh, I see you closed down the other thread before I had a chance to go back and reply to stuffâ€¦ ah well, maybe best to avoid getting into such arguments, heh.

Harvey Friedman Says:
Comment #2 July 23rd, 2020 at 6:03 am
â€œthe values of all Busy Beaver numbers beyond the 7910th are independent of the axioms of set theory (Stefan Oâ€™Rear has since shown that independence starts at 748th value or sooner).â€

One might be a little more precise here and say that for c = 7910 (later improved to 748),

for all n >= c and m, the statement BB(n) = m is unprovable in ZF(C).

When put this precisely, this raises the following question about BB:

QUESTION. Let n be fixed and suppose that for all m, the statement BB(n) = m is unprovable in ZF(C). Then is it true that for all r, the statement BB(n+1) = r is unprovable in ZF(C)?

From work of Goedel, we know that ZF and ZFC are equivalent for these questions.

Jon Awbrey Says:
Comment #3 July 23rd, 2020 at 6:14 am
One of my favorite fast functions â€¦

ğŸ™ https://oeis.org/A111788

Jon Awbrey Says:
Comment #4 July 23rd, 2020 at 6:25 am
Hereâ€™s another one â€¦

https://oeis.org/A050924

Harvey Friedman Says:
Comment #5 July 23rd, 2020 at 6:41 am
I can now answer my question in the previous post.

THEOREM. Suppose ZF(C) proves BB(n) = m. Then every TM with at most n states that doesnâ€™t halt can be proved in ZF(C) to not halt. Conversely, if every TM with at most n states that doesnâ€™t halt can be proved in ZF(C) to not halt, then for some m, BB(n) = m is provable in ZF(C).

Proof: For the first claim, just use that: ZF(C) sees that a TM with <= n states halts if and only if it halts in <= m steps. For the second claim, assume

*) every TM with at most n states that doesn't halt can be proved in ZF(C) to not halt.

We now show how to determine BB(n) within ZF(C). Let m be the actual value of BB(n). then ZF(C) identifies the TM with at most n states correctly by waiting m steps, We need to see that ZF(C) can actually prove that the other TM's with at most n states do not halt. That is clear by *). QED

COROLLARY. If BB(n) = m is provable in ZF(C) then for some r, BB(n+1) = r is provable in ZF(C).

Jon Awbrey Says:
Comment #6 July 23rd, 2020 at 7:04 am
All my favorite integer sequences, a few of them very fast growing, spring from the â€œlambda pointâ€ where graph theory, logic, and number theory meet, going back to a time when I was playing around with GÃ¶del numbers of graph-theoretic structures and thinking about computational complexity. I posted a couple of links to the OEIS earlier but they must have fallen into the spam trap. Iâ€™ll try this non-linky comment for now and add links later.

wolfgang Says:
Comment #7 July 23rd, 2020 at 7:48 am
The discussion of BB(n) usually just distinguishes halting vs not-halting TMs, but I think it would be interesting to further distinguish the non-halting, e.g. as follows: non-halting which produces a finite pattern of 0s and 1s , non-halting which produces an infinite pattern of low complexity, e.g. 01010101â€¦ and finally non-halting which produces a pattern indistinguishable from random , e.g. calculating pi etc.

And I think it would be interesting to know something about the ratio of such TMs , basically the ratio of â€˜boringâ€™ TMs (stuck in a loop or finishing early) vs the â€˜interestingâ€™ TMs.
i think both TMs which stop after a finite but very large number of steps and TMs which never stop but create complicated patterns are â€˜interestingâ€™ and suspect that the ratio of â€˜interestingâ€™ to â€˜boringâ€™ TMs quickly tends to zero â€¦

Michael Raskin Says:
Comment #8 July 23rd, 2020 at 7:59 am
When you mention Rayoâ€™s construction, you say that it is too shaky; is it known, though, that uniquely-ZF-definable numbers grow faster than BB of rank of largest constructive ordinal provable in ZF?

Also, maybe SKI combinator logic (with number of reductions as runtime) could also be mentioned as a contender for a well-known succinctly-definable model where reasonably short program can already run for a long time?

Stephen L Says:
Comment #9 July 23rd, 2020 at 8:16 am
Mathematically-literate non-computer-scientist here. Nice article! I was able to follow until section 3. This paragraph lost me a bit:

â€œDefine the â€œsuper Busy Beaver function,â€ BB1 (n), exactly the same way as BB (n), except that the Turing machines being maximized over are now equipped with a HALT oracle in some suitable way. (The â€œoriginalâ€ BB function would then be BB0 (n).) Since the arguments in Section 2 relativize, we find that BB1 (n) dominates not only BB (n) itself, but any function computable using a BB oracle.â€

How does having a Halt oracle allow for longer-running programs?

Joshua B Zelinsky Says:
Comment #10 July 23rd, 2020 at 8:43 am
@ Stephen L, #9

â€œHow does having a Halt oracle allow for longer-running programs?â€

It may be easier as an intuition pump to consider a Turing machine which rather than a Halting oracle, has itself a Busy Beaver oracle (that is for your regular Busy Beaver function). If so, for n that are only a little big one can for n states do something like a machine that does something like â€œCount to BB(2^n).â€ Similarly consider a machine that does â€œFind k=BB(n-10), and then count to BB(k).â€ The number of transitions here should intuitively grow much faster than BB(n).

Oleg Eterevsky Says:
Comment #11 July 23rd, 2020 at 8:50 am
Not claiming any theoretical value, but last autumn as a toy project I decided to find some Busy Beavers in Brainfuck (with some limitations): https://github.com/eterevsky/beaver. TLDR: Iâ€™ve ran all the BF programs up to length 18, solved halting problem for all programs up to length 12. The longest-running program that Iâ€™ve found is this one: >+[>++>+++[->]+. It runs for 9213 steps.

Scott Says:
Comment #12 July 23rd, 2020 at 8:58 am
Harvey Friedman #2 and #5: Granting that you surely forget more logic and computability theory in an hour then Iâ€™ve learned in my entire life, isnâ€™t that observation implicitly right there in Proposition 4 in the survey? ğŸ™‚

(Separately, in your corollary at the end, shouldnâ€™t â€œprovableâ€ be â€œunprovableâ€?)

Idle Squirrel Says:
Comment #13 July 23rd, 2020 at 9:21 am
I know I should probably feel bad for tuning into the comment sections before reading the paper to check if someone has already found a way to politicize or culture-weaponize BB (i.e. by suggesting that the term â€œbeaver frontierâ€ is offensive toâ€¦ rural Canadians)

So how about preemptively turning it into an argument about the supremacy of computable numbers instead â€“ a number that cannot be computed should not be said to be bigger than any number that can be â€“ because come on, thatâ€™s cheating.

fred Says:
Comment #14 July 23rd, 2020 at 10:22 am
My first encounter with the Busy Beaver was in 1984, in the Scientific American column â€œComputer Recreationsâ€ by A.K. Dewdney.

The SA official site only has his full article on Mandelbrot:
https://static.scientificamerican.com/sciam/assets/media/inline/blog/File/Dewdney_Mandelbrot.pdf

But you can find many of his columns (including the Beaver one) in the book:
https://www.amazon.com/The-New-Turing-Omnibus-Excursions/dp/0805071660

wolfgang Says:
Comment #15 July 23rd, 2020 at 10:49 am
@Idle Squirrel #13 >> thatâ€™s cheating

But cheating is sometimes quite interesting, e.g. in a â€˜name the bigger numberâ€™ contest I would go with S + 1 , where S is the largest well-defined number ever mentioned by Scott or one of his commenters on this blog.

You may say that this is quite fuzzy, because S might change over time, but I think this loophole can be fixed by considering a Turing machine capable of simulating Scott and his commenters â€¦ including this one.

Scott Says:
Comment #16 July 23rd, 2020 at 11:05 am
wolfgang #15:

in a â€˜name the bigger numberâ€™ contest I would go with S + 1 , where S is the largest well-defined number ever mentioned by Scott or one of his commenters on this blog.
The trouble is, youâ€™re a commenter on this blog, and you just mentioned S+1. So to whatever extent S is well-defined at all, we get Sâ‰¥S+1, an obvious absurdity.

Scott Says:
Comment #17 July 23rd, 2020 at 11:10 am
fred #14: Hey, I also first learned about Busy Beaver from A. K. Dewdneyâ€™s The New Turing Omnibus! I think when I was 16. And I wondered why no one had told me about such huge numbers earlier, and figured Iâ€™d tell my own kids as early as possible when and if I had any. ğŸ˜€

Scott Says:
Comment #18 July 23rd, 2020 at 11:16 am
Idle Squirrel #13: If the Busy Beaver function is considered suspect because of noncomputable supremacy, then we could also consider the Idle Squirrel function, defined as the least number of steps made by any n-state Turing machine on an all-0 input. As IS(n)=1 for all n, my survey article on The Idle Squirrel Frontier would have the advantage of being a lot shorterâ€¦ ğŸ˜€

Scott Says:
Comment #19 July 23rd, 2020 at 11:27 am
Sniffnoy #1:

1) Yes, PA has to be easier than ZFC. I guess Stefan was saying that some particular way to encode PA is even worse than what he got for ZFC, but if so, then it must be a bad encoding.

2) Yeah, I knew from the Wikipedia page that Grahamâ€™s number is a looser upper bound that Graham explained to Martin Gardner; what appeared in Grahamâ€™s paper was a tighter (though still incomprehensibly enormous) bound thatâ€™s harder to explain. That seemed legit though, since even for mathematical reasons one might prefer a simpler, looser bound.

Dangernorm Says:
Comment #20 July 23rd, 2020 at 11:42 am
Is there a stylistic requirement that you use dismissive scare quotes around names that donâ€™t, or that you believe donâ€™t, match the names people use to file their tax paperwork? If someone makes a significant enough contribution that youâ€™d want to reference them at all, surely we can respect their decision to do so under the name Wythagoras, or any other. I assume you wouldnâ€™t do the same for the self-selected name of transgender persons, even if you knew that they hadnâ€™t yet filed formal name change paperwork.

Scott Says:
Comment #21 July 23rd, 2020 at 11:58 am
Michael Raskin #8:

When you mention Rayoâ€™s construction, you say that it is too shaky; is it known, though, that uniquely-ZF-definable numbers grow faster than BB of rank of largest constructive ordinal provable in ZF?
I and others debated some of these issues years ago on this MathOverflow page. I also talked them over with Agustin Rayo, my former colleague at MIT (and a cool guy), who completely agreed that his number is ill-defined given the philosophical commitments that Iâ€™m willing to make.

To make a long story short, my current understanding is that, if
(1) someone has a way of uniquely defining a huge numbers using a ZF predicate, and
(2) Iâ€™d be willing to admit their number as well-defined (e.g., not depending on an intended model of set theory),
then it should be possible to simulate their construction using an ordinal BB function. Or to say it another way: I donâ€™t see set theory as having a magical power to make integers well-defined that otherwise wouldnâ€™t be. If an integer is well-defined, then Iâ€™d like its definition to ultimately be in terms of first-order quantification over the integers (possibly with an ordinal number of quantifiers), in which case that integer will be â€œordinal-BB-simulable.â€ The proof of the constructionâ€™s soundness might depend on highfalutin set theory (for example, large-cardinal axioms to establish the existence of the requisite ordinal), but the construction itself shouldnâ€™t.

But if anyone wants to revisit that debate in this thread, I wonâ€™t stop themâ€¦

Scott Says:
Comment #22 July 23rd, 2020 at 12:10 pm
wolfgang #7:

The discussion of BB(n) usually just distinguishes halting vs not-halting TMs, but I think it would be interesting to further distinguish the non-halting, e.g. as follows: non-halting which produces a finite pattern of 0s and 1s , non-halting which produces an infinite pattern of low complexity, e.g. 01010101â€¦ and finally non-halting which produces a pattern indistinguishable from random , e.g. calculating pi etc.
Yes, I mention that distinction in Section 5.6 of the survey. In practice, when youâ€™re trying to calculate BB numbers, the non-halting machines that youâ€™re worried about are virtually all ones that generate non-repeating patterns. If itâ€™s a repeating pattern, then it tends to be easy to detect that, prove the machine never halts, and discard it.

And I think it would be interesting to know something about the ratio of such TMs , basically the ratio of â€˜boringâ€™ TMs (stuck in a loop or finishing early) vs the â€˜interestingâ€™ TMs.
i think both TMs which stop after a finite but very large number of steps and TMs which never stop but create complicated patterns are â€˜interestingâ€™ and suspect that the ratio of â€˜interestingâ€™ to â€˜boringâ€™ TMs quickly tends to zero â€¦
No, thatâ€™s not the case. See for example the literature on Chaitinâ€™s constant Î©. Once you have a prefix-free encoding scheme (so that the notion of a â€œrandom programâ€ makes sense at all), the proportion of programs displaying basically any nontrivial behavior you want (e.g., generating an infinite non-repeating pattern) is going to be some uncomputable real. So in particular, it will be bounded away from 0.

DangerNorm Says:
Comment #23 July 23rd, 2020 at 12:15 pm
Actually, on the subject of math results posted by Internet users, have you heard of the paper, A lower bound on the length of the shortest superpattern, which credits Anonymous 4chan Poster as the main contributor? I expect that the number of significant results credited by other-than-legal-names will only increase with time.

Scott Says:
Comment #24 July 23rd, 2020 at 12:50 pm
Dangernorm #20:

Is there a stylistic requirement that you use dismissive scare quotes around names that donâ€™t, or that you believe donâ€™t, match the names people use to file their tax paperwork? If someone makes a significant enough contribution that youâ€™d want to reference them at all, surely we can respect their decision to do so under the name Wythagoras, or any other. I assume you wouldnâ€™t do the same for the self-selected name of transgender persons, even if you knew that they hadnâ€™t yet filed formal name change paperwork.
Aha, thank you! This is 2020, so I knew someone would find something to take offense at, even in a survey article about an uncomputable integer sequence. ğŸ˜€

I have nothing but admiration for anyone who discovered that BB(7)>1010^10^10^18,000,000. The trouble is, thereâ€™s not much tradition in academia for publishing original research under pseudonymous handlesâ€”the closest to a counterexample that I could think of was Bourbaki, which of course published under the same name for many decades. Academics can change their names, as many trans people do (or for that matter, spouses who take a new surname when they get married). But academic exchange usually does presuppose some level of consistency in what name a person is known by, so that they can stay accountable for what they said and also so that itâ€™s easy for others to credit their contributions.

In the case at hand, the point is not just that Wythagoras presumably doesnâ€™t use that name to file taxesâ€”rather, itâ€™s that as far as I know, they havenâ€™t used that name for anything besides a few forum posts about large numbers. Indeed, I had wanted to contact Wythagoras to solicit feedback on my survey, but couldnâ€™t find any way to do that. (Wythagoras, if youâ€™re reading this now: big fan of your posts; please get in touch! ğŸ™‚ ) And what if someone had questions about the veracity of the result, which was described rather briefly on the forum and wasnâ€™t peer-reviewed?

Iâ€™m extremely far from the reactionary camp that says â€œif itâ€™s not in a peer-reviewed journal, it doesnâ€™t exist.â€ But I think weâ€™re still negotiating the norms for results that, in some cases, exist only as pseudonymous blog comments with no way to contact the author. And this issue came to the fore with my Busy Beaver surveyâ€”given the centrality, especially recently, of online contributions outside the normal academic process.

DangerNorm Says:
Comment #25 July 23rd, 2020 at 1:05 pm
I see. Iâ€™m not offended, but I am in favor of expanding the area of society in which people may operate without pre-doxing themselvs.

My own sense is that mathematics should be the most open to this, since the reader can scarcely understand what a math paper is even trying to say without joining the author hand-in-hand, step-by-step. There is not the issue one has with, for example, the collection of datasets, whereby one must put trust in the process, even if the alleged data itself is included. The proof could as well have been published by a university, posted on 4chan (as in the paper mentioned above), or recovered from carvings in Antarctic passages in an unknown language, but with clear enough pictures that you can follow the geometric construction; youâ€™re either persuaded by the proof or you arenâ€™t.

As someone more embedded in the academic process than myself, does this match your understanding?

Stephen Jordan Says:
Comment #26 July 23rd, 2020 at 1:33 pm
At a time like this it is very valuable to have a mathematical world into which one can be absorbed. Iâ€™ve chosen the world of tensor rank and algebraic complexity. I canâ€™t quite put my finger on what specific property makes certain mathematical subjects particularly suitable for this but I think Busy Beaver has it.

The hard part is keeping the external world from intruding. I could be bounded in a nutshell and count myself king of infinite space were it not that I have facebook.

Idle Squirrel Says:
Comment #27 July 23rd, 2020 at 1:36 pm
Satoshi Nakamoto comes to mind as having made a rather significant impact with a pseudonymous paper even if not in the classic academic sense.

John Michael Says:
Comment #28 July 23rd, 2020 at 2:11 pm
I found your â€œBigger Numberâ€ essay when I was a kid and was utterly fascinated (and also made deeply confused and curious by the whole concept of uncomputability). Itâ€™s probably still buried near the bottom of the bookmarks on the family computer, heh. Cool to be reading an even more fascinating piece on an overlapping subject all these years later! (And a bit embarrassing to find out Iâ€™m now older than you were when you wrote that article!)

Thanks for being such an inspiring & engaging writer!

Jacob Manaker Says:
Comment #29 July 23rd, 2020 at 2:42 pm
You mention in the paper that the optimal functions for small n seem to be running Collatz-type iterations. This would make sense if they â€œunpackedâ€ into some larger function via a FRACTRAN-type encoding.

Gautham Kamath Says:
Comment #30 July 23rd, 2020 at 3:28 pm
Scott,

In the article the the lowest bound for BB(N) that canâ€™t be proved by ZF is BB(748). Similarly the current lowest bound for BB(N) that can prove Riemann is for N =744 and for Goldbach N=27.

1) If someone manages the lower the ZF bound to N < 744, but lets say that we magically know that N=744 can't be improved upon for Riemann. Would that mean that Riemann is not provable in ZF?

2) Do you think that the above current bounds hint that proving Riemann is more difficult than proving Goldbach?

Scott Says:
Comment #31 July 23rd, 2020 at 3:36 pm
DangerNorm #25:

The proof could as well have been published by a university, posted on 4chan (as in the paper mentioned above), or recovered from carvings in Antarctic passages in an unknown language, but with clear enough pictures that you can follow the geometric construction; youâ€™re either persuaded by the proof or you arenâ€™t.
As someone more embedded in the academic process than myself, does this match your understanding?

The trouble is that it doesnâ€™t. Like, youâ€™ve described the Platonic ideal of math research, and it used to be more like that, and itâ€™s still like that for certain problems and in certain areas of math. But many modern proofs are insanely complicated, and they depend on previous work in messy and incompletely-specified ways, and include many steps like â€œthis is handled using the standard tools.â€ Itâ€™s typically unrealistic that even world experts would be able to follow such a proof without some back-and-forth with the author. (By analogy, even if your code contains all the essential ideas of a modern operating system, itâ€™s probably not going to boot on the first try.)

Sure, after a long refereeing process, a product will hopefully emerge that can be understood with no further input from the author. But the forum posts that weâ€™re talking about are not refereed! (At most theyâ€™re commented on by other forum users.)

These problems are compounded in the specific case of searches for 6- and 7-state Busy Beavers, which involve a mixture of informal reasoning and the results of running simulations. In cases like this, even if code is available for download, rather than struggling to get the code to work, most people are just going to trust the author about what the results were, and are also going to trust the author that the code is doing what itâ€™s supposed to do and that it correctly links up with the informal reasoning.

Eventually, I expect people to find solutions to these problemsâ€”which might involve automated proof-checking, or decoupling peer review from journal publication, or letting authors have pseudonymous handles by which theyâ€™re reachable even decades later, or something else. But right now weâ€™re in a transitional era where the solutions havenâ€™t yet emerged.

Scott Says:
Comment #32 July 23rd, 2020 at 3:40 pm
Stephen Jordan #26:

I could be bounded in a nutshell and count myself king of infinite space were it not that I have facebook.
Iâ€™m tempted to make that my blogâ€™s new tagline! ğŸ˜€

Scott Says:
Comment #33 July 23rd, 2020 at 3:46 pm
Idle Squirrel #27:

Satoshi Nakamoto comes to mind as having made a rather significant impact with a pseudonymous paper even if not in the classic academic sense.
Yes, good, thatâ€™s another big example! Besides Satoshi and Bourbaki, what others are there?

zjin Says:
Comment #34 July 23rd, 2020 at 3:50 pm
I wonder if you would comment on the news of National Quantum Internet. It seemed that the U.S. Department of Energy and the University of Chicago would have a major project to build this.

Scott Says:
Comment #35 July 23rd, 2020 at 3:51 pm
Jacob Manaker #29:

You mention in the paper that the optimal functions for small n seem to be running Collatz-type iterations. This would make sense if they â€œunpackedâ€ into some larger function via a FRACTRAN-type encoding.
While I donâ€™t know FRACTRAN well, the Collatz-like iteration could itself be seen as a sort of unpacking: we start with 0, which then gets successively â€œunpackedâ€ into larger and larger positive integers, but only a finite number of times until the iterative process terminates for a modularity reason. But in addition to that, thereâ€™s some â€œunpackingâ€ in a Turing machine with only 5 states implicitly encoding a relatively complicated Collatz-like iteration.

Scott Says:
Comment #36 July 23rd, 2020 at 3:57 pm
Gautham Kamath #30:

1) If someone manages the lower the ZF bound to N < 744, but lets say that we magically know that N=744 can't be improved upon for Riemann. Would that mean that Riemann is not provable in ZF?
Yes, it would mean that. As I point out in footnote 18, the moment the Riemann hypothesis was proven, the truth of RH would then be proved to be equivalent to the non-halting of a one-state Turing machineâ€”namely, one that just goes into a trivial infinite loop! ğŸ˜€

2) Do you think that the above current bounds hint that proving Riemann is more difficult than proving Goldbach?
No, not necessarily. In practice, the number of states needed to encode a conjecture via a Turing machine need not have any correlation with the difficulty of proving it.

Scott Says:
Comment #37 July 23rd, 2020 at 3:59 pm
zjin #34: Sorry, I havenâ€™t read about it and donâ€™t have a comment right now.

CC Says:
Comment #38 July 23rd, 2020 at 4:18 pm
Thanks for the article. Just wanted to note my appreciation since I had not really read about BB before and you made me do it and it was rewarding.

Gautham Kamath Says:
Comment #39 July 23rd, 2020 at 4:31 pm
Scott #36: Thanks for the explanation!

Unrelated: You gave a talk at my company (Cirrus Logic) last year and it was pretty well received; attendance was significantly higher than the average Professor talks we usually have. So the organizer would like to have future speaker talks about Quantum Computing and wanted suggestions from me. I said that since you were on the theoretical computer science side, that maybe we should now get someone from the engineering/experimental side and suggested he try to reach out to the Google Engineering team since they had the quantum supremacy milestone several months ago.

I know you are friends with some of those guys; perhaps there is a name you can suggest, someone who is a good expositor, just like you? Or some other engineer/experimentalist colleague of yours that has nothing to do with Google?

Scott Says:
Comment #40 July 23rd, 2020 at 4:41 pm
CC #38: Thanks!

Scott Says:
Comment #41 July 23rd, 2020 at 4:43 pm
Gautham #39: If you wanted a QC experimentalist from UT Austin, Shyam Shankar is your man. Though in the Covid/Zoom era, I guess proximity no longer matters, so you could also reach out to Google folks in Santa Barbara, like Ryan Babbush or Sergio Boixo.

Mark Marshall Says:
Comment #42 July 23rd, 2020 at 5:46 pm
Scott #33

At school, I learnt about the â€œStudent T Testâ€, which was published by â€œStudentâ€. He was also the head brewer at Guinness, which tells us all we need to know about statistics.

Filip Says:
Comment #43 July 23rd, 2020 at 5:54 pm
Scott #33: How did you forget G. W. Peck? Itâ€™s the pseudonym of Ronald Graham, ErdÅ‘s, Douglas West, George B. Purdy, Fan Chung, and Daniel Kleitman.

asdf Says:
Comment #44 July 23rd, 2020 at 7:17 pm
The citations to the googolology wiki remind me of the following extremely nerdy passage from HPMOR:

â€œMeself,â€ Hagrid continued, â€œI think we might â€˜ave a Parisian hydra on our â€˜ands. Theyâ€™re no threat to a wizard, yehâ€™ve just got to keep holdinâ€™ â€™em off long enough, and thereâ€™s no way yeh can lose. I mean literally no way yeh can lose so longâ€™s yeh keep fightinâ€™. Trouble is, against a Parisian hydra, most creatures give up long before. Takes a while to cut down all the heads, yeh see.â€

â€œBah,â€ said the foreign boy. â€œIn Durmstrang we learn to fight Buchholz hydra. Unimaginably more tedious to fight! I mean literally, cannot imagine. First-years not believe us when we tell them winning is possible! Instructor must give second order, iterate until they comprehend.â€

I had to look it up: the Googolology wiki describes the Buchholz hydra here.

John Michael Says:
Comment #45 July 23rd, 2020 at 9:03 pm
Question about uncomputability and definability:

So, the value of BB(748) is independent of ZFC. As I understand it, by GÃ¶delâ€™s completeness theorem, this implies that in different models of ZFC BB(748) has different values. Right? Thatâ€™s pretty weird though, since if you had two competing values for BB(748) proven in two different extensions of set theory, if there were enough time and space and negentropy, you could just run them in check to see which is rightâ€¦

Iâ€™m missing some things here but Iâ€™m not sure what they areâ€¦ Does it have something to do with â€œnonstandard modelsâ€ or whatever and weird theories like ZFC+Â¬Con(ZFC)? Scott, why do you consider BB(748) to be mathematically well-defined, even though you donâ€™t consider Radoâ€™s 2nd order number to bet? Donâ€™t both change depending on which model of set theory youâ€™re in?

lazyworm Says:
Comment #46 July 23rd, 2020 at 9:46 pm
About your conjecture BB(20,2) is independent of ZF, one can speculate that BB(10,3) should be independent of ZF â€¦â€¦â€¦â€¦. then there exists an integer k such that BB(2,k) is independent of ZF.

Oscar Cunningham Says:
Comment #47 July 23rd, 2020 at 10:58 pm
I realised an interesting thing while reading this. If we define the theory T_n as PA + â€˜the nth Busy Beaver machine is bâ€™ where b actually is the nth Busy Beaver machine, then T is a sequence of effectively axiomatized consistent theories with arbitrarily high consistency strength! For any other effectively axiomatized consistent theory S thereâ€™s some n_S such that PA+Con(T_n_S) proves Con(S).

So the Busy Beaver numbers give.us a countable ladder on which we can place the various Large Cardinal axioms for comparison to each other. Previously Iâ€™d been assuming that the structure of all possible Large Cardinal axioms was much more complicated than that, and that they order of their consistency strengths would be transfinite, with no hope of having countable cofinality.

Raoul Ohio Says:
Comment #48 July 23rd, 2020 at 11:52 pm
Scott #24,

Pretty sure Student of â€œStudent Tâ€ test fame is not her/his real name.

asdf Says:
Comment #49 July 24th, 2020 at 12:34 am
John Michael #45, !CON(ZFC) asserts that a certain Turing machine (one that searches for a proof that 1=0) eventually halts. Assuming ZFC is actually consistent, in a model of ZFC with standard integers, this TM never halts, so some other TM is the busy beaver (the longest-running halting TM) for that number of states. In an alternate model with nonstandard arithmetic, the 1=0 TM halts after a nonstandard number of steps, but that is larger than any standard integer, so there is no way to actually run the TM for that long to see what happens. In particular, the busy beaver in that model would have a nonstandard running time.

Laurent Claessens Says:
Comment #50 July 24th, 2020 at 12:50 am
If I understand correctly, there is a small typo on page 8 : â€œFor completeness, here are the Busy Beavers and (for n â‰¥ 5)â€ It should be â€œn <= 5".

I've a question about "usefulness" of BB in the following sense : Is there a question in mathematics that
1. can be asked without knowing about BB
2. cannot be answered without using some BB ?

Scott Says:
Comment #51 July 24th, 2020 at 1:45 am
Oscar Cunningham #47: Yes!!!! Thatâ€™s a beautiful observation. Would you mind if I included it in my survey, crediting you?

In some sense, I suppose itâ€™s obvious that we can order all effectively axiomatized theories T along a countable ladderâ€”namely, the ladder of â€œthe number of bits needed to write a program that enumerates all the theorems of Tâ€â€”which is all that the BB ladder really is. The less obvious part is that the steps on this ladder are themselves theories that are ordered by consistency strength: namely, your theories of the form â€œPA + BB(n)=k.â€

Hereâ€™s something that Iâ€™m now wondering. If we consider popular theories like ZF, is their consistency provably equivalent to some statement of the form BB(n)=k? Or is Con(ZF) sandwiched between different such statements? In other words, let xâ‰¥5 be the first integer for which ZF doesnâ€™t prove the value of BB(x), and let yâ‰¤748 be the first integer for which the value of BB(y) implies Con(ZF). Clearly xâ‰¤y. But is there a gap between x and y, and if so, how large is it?

Scott Says:
Comment #52 July 24th, 2020 at 1:58 am
Laurent Claessens #50:

If I understand correctly, there is a small typo on page 8 : â€œFor completeness, here are the Busy Beavers and (for n â‰¥ 5)â€ It should be â€œn <= 5".
No, thatâ€™s not a typo. For nâ‰¥5, no one has established what the Busy Beavers are; we only have candidates.

Iâ€™ve a question about â€œusefulnessâ€ of BB in the following sense : Is there a question in mathematics that
1. can be asked without knowing about BB
2. cannot be answered without using some BB ?
Probably the best example is, â€œHereâ€™s someone who never studied computability or logic. Whatâ€™s a positive integer thatâ€™s vastly bigger than anything they could write down within the lifetime of the observable universe?â€ ğŸ˜€

More seriously, I sometimes see BB show up in papers on Kolmogorov complexity, sophistication, depth, Chaitinâ€™s Î©, and other topics in computability theory. In all these cases, undoubtedly one could â€œroute aroundâ€ BB if one really wanted to, and phrase everything in terms of other ways to make computability quantitative, like K and Î©. But why? The relevant question is just whether BB is a useful concept, and I think it clearly is, at least for anyone who cares about any quantitative question that touches on computability.

Scott Says:
Comment #53 July 24th, 2020 at 2:16 am
John Michael #45: Yes, it has to do with nonstandard models. The situation is this: let G be the actual value of BB(748), whatever it is.

(And if you donâ€™t accept that thereâ€™s such a thing as the â€œactual valueâ€ of BB(748), then get out of the room! ğŸ˜€ For me, if thereâ€™s no objective fact of the matter about whether a given Turing machine halts or runs forever on an all-0 inputâ€”separate from the question of that factâ€™s provabilityâ€”then thereâ€™s no objective fact of the matter about anything, including anything that weâ€™re talking about in this very conversation.)

Still with me? OK then, ZF clearly proves that BB(748)â‰¥G, by just simulating a 748-state Busy Beaver for the requisite number of steps. So every model of ZF â€œknowsâ€ that BB(748)â‰¥G. But some models of ZF â€œincorrectly believeâ€ that BB(748) is strictly greater than G. None of these models â€œbelieveâ€ that BB(748)=H, for any positive integer H strictly greater than Gâ€”if they did, that would easily lead to a contradiction. Their â€œbelief,â€ so to speak, is instead that BB(748) equals a nonstandard integer. Which is the same as saying: these models â€œbelieveâ€ that some particular 748-state TM halts, even though in actual reality that TM runs forever. By GÃ¶del, this false belief can never be disproved within ZF, but itâ€™s false all the same.

In other words: metatheoretically, we can say that these theories, though consistent, are all â€œpathologicalâ€ and arithmetically unsound, in exactly the same way that ZF+Not(Con(ZF)) is â€œpathologicalâ€ and arithmetically unsound.

Oscar Cunningham Says:
Comment #54 July 24th, 2020 at 3:30 am
Scott #51: Of course I donâ€™t mind!

Sniffnoy Says:
Comment #55 July 24th, 2020 at 3:54 am
A bunch more mathematiciansâ€™ pseudonyms here. Some of these are pseudonyms they published mathematics under; some are not.

Also, some other amusing irregularities in academic authorship: F. D. C. Willard and G. Mirkwood ğŸ™‚

Jon Awbrey Says:
Comment #56 July 24th, 2020 at 8:00 am
Dear Scott,

I put a few links related to my previous comments in a blog post â€”

ğŸ™ Riffs and Rotes â€¢ 5

DR Says:
Comment #57 July 24th, 2020 at 8:05 am
An option I have seen and maybe tried myself when referring to internet pseudonyms is to cite â€œuser Xâ€, where X is the username, but with no quotes in the text. So for instance, one can cite user Wythagoras on Googolology, rather than â€œWythagorasâ€. Or, to pick a rather well-known anonymous mathematician, user quid on MathOverflow (no longer active, sadly), rather than â€œquidâ€.

Cecile McKee Says:
Comment #58 July 24th, 2020 at 11:05 am
Iâ€™ve just met your blog (by following a link from Pinkerâ€™s website). I love it! And, from what I can tell of your eclectic interests, I thought you might enjoy this â€œMental Flossâ€ piece: https://www.mentalfloss.com/article/31283/who-wrote-happy-birthday-you-and-whos-collecting-millions-royalties

You: â€œOK, I mused, how many people have even heard of the Linguistics Society of America, compared to the number whoâ€™ve heard of Pinker or read his books?â€

Joshua B Zelinsky Says:
Comment #59 July 24th, 2020 at 12:18 pm
One other thought which may or may not be useful:

On page 16, you ask â€œdo all Busy Beavers halt on all finite inputs?â€ While my guess is that the answer is probably no, it might be possible to prove results of the sort which I believe you once characterized as something like â€œIf pigs flys, then the moon is not made of green cheese.â€ In particular, if we assume that all Busy Beavers do halt on all finite inputs, then subject to that assumption, it looks like conditionally proving that BB(5) = 47,176,870 might become easier. For the remaining 25 5- state Turing machines, it would then suffice for each to find some initial tape configuration where we can prove that machine does not halt. That seems much easier than proving it for the specific case of not halting on the blank tape.

Persona Says:
Comment #60 July 24th, 2020 at 1:54 pm
â€œOne can even define, for example, BBÏ‰ZF (n), where Ï‰ZF is the computable ordinal thatâ€™s the supremum of all the computable ordinals that can be proven to exist in ZF set theory. Or BBÏ‰LC (n), where LC is some large-cardinal theory extending ZF, and Ï‰LC is the computable ordinal that similarly encodes LCâ€™s power.â€

BBÏ‰LC (n) is unambigously defined as long as LC is effectively axiomatized and consistent, right? Could we define BBmax(n) = max_LC BBÏ‰LC (n) with LC running over all effectively axiomatized consistent theories extending ZF?

Scott Says:
Comment #61 July 24th, 2020 at 4:17 pm
Persona #60: For a fixed value of n, why would we expect that maximum to exist?

Persona Says:
Comment #62 July 24th, 2020 at 4:51 pm
#Scott61: Okay, next try. Define some enumeration of all effectively axiomatized extensions of Zf.

Let L(m,n)=max_(number(LC) < m and LC is consistent) BBÏ‰LC (n).

Then L(n)=L(n,n) should be well-defined.

Michael Says:
Comment #63 July 24th, 2020 at 4:51 pm
Scott, an off topic question- I was just reading Jonathan and Jesse Kellermanâ€™s murder mystery Half Moon Bay and at one point the protagonist questions a physics professor named DELIA Moskowitz. Do either you or Dana know the Kellermans?

Job Says:
Comment #64 July 24th, 2020 at 4:58 pm
So BB(n) is basically a race to see which n-state machine encodes the largest number in its run time.

For large enough n we could embed a random number generator, as well as a seed, such that it runs until it sees k consecutive heads, for some large k â€“ just let the seed do the work.

And if we embed an m-state simulator (m less than n), then k could be another BB(m). Thatâ€™s like raising BB(m) to the power of RNG. ğŸ™‚

Not surprised it grows so fast, very interesting stuff.

(BTW the comment preview is having issues with < characters).

asdf Says:
Comment #65 July 24th, 2020 at 5:05 pm
There is an MO thread about math pseudonyms: https://mathoverflow.net/questions/45185/pseudonyms-of-famous-mathematicians

Nick Says:
Comment #66 July 24th, 2020 at 5:20 pm
This is exactly the kind of â€œsimple-but-deepâ€ inquiry that brought me to this blog in the first place, and I imagine that goes for many other readers as well. Letâ€™s all try to keep that common ground in mind in the political discussions that take place on this blog. Thatâ€™s at least as much a reminder to myself as to anyone else!

Questions / comments:

1. Say BB(n) is provable in T, but BB(n + 1) is not. In a qualitative sense, what is it that changes to make that the case? What is the difference between n and n + 1? Why is the cutoff point n + 1 and not n? Itâ€™s like youâ€™re walking down a staircase, and the all of a sudden youâ€™ve taken a step into a bottomless pit. I hope this question makes sense despite being vague.

2. Let CC(n) be the number of machines in T(n) that halt in BB(n) steps. What is known about CC? Sorry if this is addressed in the article and I missed it.

3. Scott: I am far from an expert, but I have read the TeXBook, and your definition of BB on page 2 strikes me as ugly-looking TeX. I donâ€™t have any suggestions for how to make it look better, except to ask â€œWhat would Knuth do?â€ (Knuth, if you are reading this, please advise.)

4. BB(1) through BB(4) is [1, 6, 21, 107]. As I write this, there is exactly one sequence in the Online Encyclopedia of Integer Sequences containing that subsequence, and it is BB [1]. So here is a challenge for anyone who is really bored: devise a natural (i.e. non-artificial) integer sequence containing [1, 6, 21, 107] as a subsequence, preferably as its first four values.

[1] https://oeis.org/search?q=1%2C+6%2C+21%2C+107

Sniffnoy Says:
Comment #67 July 24th, 2020 at 5:26 pm
Job #64: Itâ€™s a (limited) HTML comment box; write &lt; instead of < to get a less-than sign.

Raoul Ohio Says:
Comment #68 July 24th, 2020 at 6:20 pm
Sniffnoy #55,

Your reference includes my guess: E.T. Bell / John Taine. Related topic: I want to suggest to everyone a highly entertaining book â€œThe search for E.T. Bellâ€ by Constance Reid. It is listed online as a novel, but it is a serious biography â€” and it is astounding! For unknown reasons, Bell invented an elaborate history for himself that no one caught on to, including his wife, son, and colleagues at Stanford, which is all the more remarkable because he spend much of his childhood in San Jose.

Scott Says:
Comment #69 July 24th, 2020 at 6:55 pm
Nick #66:

1. One possible answer to your question is that it happens to take n+1 states to design a Turing machine that searches for contradictions in the formal system. A second possible answer is that the phase transition has to happen somewhere, so why not at n+1? ğŸ™‚

2. CC(n) is another quantity that, if you know it, lets you solve the halting problem for all Turing machines with â‰¤n states. CC(n) also has n log2 n â€“ O(n) bits of algorithmic information (ie, no shorter program can output CC(n)). CC(n) is extremely closely related to, although not identical with, Chaitinâ€™s halting probability Î©.

3. Sorry! My LaTeX skills are better than those of the people who use < and > for the angle-brackets of quantum states, and who write words in math mode that are actually products of variables, but theyâ€™re not MUCH better. ğŸ™‚

Scott Says:
Comment #70 July 24th, 2020 at 7:00 pm
Michael #63: No, sorry, Iâ€™ve never heard of those people.

Persona Says:
Comment #71 July 24th, 2020 at 7:46 pm
Regarding my comment #62: I was looking for a trick to avoid what you called the â€œargument over which large-cardinal axioms are allowed when defining a generalized BB functionâ€. Does it work? I mean, if L is well-defined then it grows (for sufficiently large n) at least as fast as any BBÏ‰LC for some fixed LC.

Jacob Says:
Comment #72 July 24th, 2020 at 10:36 pm
Layman here, with a probably naive question about the proof of proposition 4.

How would T proving that M never halts prove Tâ€™s consistency? Wouldnâ€™t proving that M never halts just prove that a proof of 0=1 does not exist? Couldnâ€™t there still exist other inconsistencies in T besides 0=1?

Zeb Says:
Comment #73 July 25th, 2020 at 12:03 am
In your discussion of the platonic reality of the busy beaver numbers BB(n) â€“ and your reasons for rejecting Rayoâ€™s function â€“ you make several references to the â€œstandard integersâ€.

Before saying something you will find controversial, let me say that I agree with your philosophical position that the standard integers are a definite thing that truly exists, in some platonic sense. (J. D. Hamkins seems to have a coherent view of an alternative â€œmultiversalâ€ philosophical stance, where the â€œstandard integersâ€ canâ€™t be nailed down at all and are at best a convenient fiction, which I find slightly terrifying and hard to refute.)

I also agree that the values of the busy beaver function can be probed empirically. The fact that BB(3) = 21 is an empirical fact which we can test by a concrete experiment, so it has a physical reality to it that the truth or falsity of the continuum hypothesis lacks.

Here is the issue: there is no guarantee that the â€œstandard integersâ€ are the same thing as the â€œphysical integersâ€. I think that you are conflating two concepts here which might not be the same at all. Let me explain.

We all should agree that it is an empirical fact that the number 10 is a natural number. Most of us have directly observed children counting to 10, the number 10 behaves in every way like we expect natural numbers to act (i.e. it is either even or odd), we can watch the second-hand of a mechanical watch tick by 10 times in a row, etc. So I think we can all agree that 10 is a â€œphysical integerâ€. However, I would like to convince you that 10 might not be a â€œstandard integerâ€ â€“ I claim that 10 might be non-standardly large! For this, we need to perform a thought experiment.

Suppose first that it is possible to completely describe our universe â€“ or at least, a universe which looks very similar to our universe â€“ as a mathematical structure which satisfies some list of axioms of first order logic. This assumption is surprisingly controversial: the only academic Iâ€™m aware of who seems willing to really defend it is Tegmark, even though the search for a â€œgrand unified theoryâ€ of physics seems to presuppose the existence of such a description. But at least, I think we can agree that this assumption hasnâ€™t been completely ruled out.

Suppose further, that the axioms which our universe satisfies are sufficiently permissive that we can prove that for each natural number n, there exists some universe which looks superficially similar to ours, in which some living creature is able to survive for n years (and in fact is able to simulate n steps of computation of any Turing machine). This is a slightly more controversial assumption, but there are enough people making a serious effort to live forever in the actual, physical universe that we should be able to at least entertain this idea.

Then we can use standard logical constructions (such as the ultraproduct construction, or the Godel completeness theorem) to produce a universe which looks superficially similar to ours, but in which the internal â€œphysical integersâ€ are a nonstandard model of PA. Whatâ€™s more, inside this universe, there will be some living creature which lives for a non-standardly large time n. We can even arrange that this n is so non-standardly big, that in this alternative universe, the Turing machine which searches for contradictions within ZFC eventually finds a contradiction and halts â€“ so it would be an empirical fact in this universe, that BB(748) is some nonstandardly huge integer. This nonstandardly huge integer would pass every empirical test for being a true â€œphysical integerâ€ that you could throw at it â€“ people would be able to count up to it, you could time that amount of time passing on a stopwatch, it would either be even or odd, etc.

The above might seem like a fantastical scenario â€“ but how can we rule it out? Any physical experiment we could devise to ensure that our universe is built out of a mathematical structure in which the internal â€œphysical integersâ€ are the true â€œstandard integersâ€ could be described as an axiom of our universe, and we can always arrange to find nonstandard models which additionally satisfy any finite list of additional axioms (so long as they are consistent with PA). So there is no physical experiment we can perform to definitively rule out the possibility that the physical integers are nonstandard â€“ or even, for that matter, to rule out the possibility that the number 10 = 1+1+1+1+1+1+1+1+1+1 is nonstandard.

Going a step further, Iâ€™d say that while we can agree that empirically we have BB(3) = 21 in our physical universe, it is not clear that BB(3) is 21 in the true â€œstandard integersâ€: perhaps 21 is nonstandardly large, and the 3-state Turing machine which we find empirically halts after 21 steps (when we simulate it via our physical pencils and paper), actually runs forever in the true standard integers, which end somewhere between the standard integer 3 and the nonstandardly large number 10 (no, I canâ€™t list out the precise set of which integers below 10 are the true standard ones â€“ that set is infinite, undefinable, and uncomputable!).

murmur Says:
Comment #74 July 25th, 2020 at 2:51 am
Hi Scott, can you explain why in Proposition 4 there should be a finite state Turing machine that finds all the proofs in T?

Gerald Says:
Comment #75 July 25th, 2020 at 4:13 am
On conjecture 12: PA is equivalent to ZF-INF (axiom of infinity deleted). Commenter #1 however mentioned that ZF was actually easier to do than PA. Does this mean that deleting INF would not help to further simplify the known 748-state construction, in other words that INF comes for free?

Conjectures 11 and 12 suggest that just adding INF may already double the number of required states. I feel that plain ZF (no large cardinals) should not actually be that much stronger than PA. An interesting, much stronger theory would be Second Order Number Theory with Projective Determinacy (Z2+PD). Since the work of Woodin and others in the 1980s there is a growing consensus that Z2+PD is the canonical theory of second order math, i.e. the right theory of V(Ï‰+1), while PA being the canonical theory of first order number theory. So let me suggest

Conjecture 11 a: Second order math (Z2+PD) does not prove the value of BB(25).

Has anyone actually tried to add a supercompact or other sufficiently large cardinals to the 748-state machine in order to get the strength up to PD?

Bunsen Burner Says:
Comment #76 July 25th, 2020 at 8:58 am
How is any of this modified if you move second order logic? Or even higher? Maybe there is an argument here even for infiniary logics? Has anyone considered such a thing?

Joshua B Zelinsky Says:
Comment #77 July 25th, 2020 at 10:05 am
One other question that comes to mind. Is it possible for there to be an n such that there are two genuinely distinct Turing machines with n states which are both Busy Beavers for that n? More rigorously, is there an n, such that there are Turing Machines T1 and T2 each with n states such that T1 and T2 run for different numbers of steps on some input L (both halting but running for different numbers of steps before halting, or one halting on L and the other not halting on L), but that T1 and T2 both run for BB(n) steps on the blank tape? My guess would be no.

Scott Says:
Comment #78 July 25th, 2020 at 10:34 am
Jacob #72: Principle of explosion. If a theory proves any contradiction, then it proves any other contradiction (including 0=1), since false implies false.

Scott Says:
Comment #79 July 25th, 2020 at 10:40 am
murmur #74: The existence of such a Turing machine is what it means for a theory to be â€œcomputableâ€ (another term is â€œeffectively axiomatizableâ€). All the usual theories youâ€™ve heard of, like PA and ZFC, have this property, which is why GÃ¶deâ€™s Theorem applies to them. You can simply write a program that does a breadth-first search over all possible valid derivations from the axioms, incorporating more and more axioms as it goes if there are infinitely many of them. Alternatively, you can have the program iterate over all finite strings, and check each one to see whether it constitutes a valid proof.

Scott Says:
Comment #80 July 25th, 2020 at 10:44 am
Bunsen Burner #76: I confess that Iâ€™ve never really grokked second-order logic. And Iâ€™m deeply suspicious of anything that presupposes a definite truth-value for, e.g., the Axiom of Choice or the Continuum Hypothesis. But maybe someone who understands this stuff would like to enlighten us on how second-order logic would change the discussion of the BB function?

Scott Says:
Comment #81 July 25th, 2020 at 10:50 am
Joshua #77: Donâ€™t we already get such an example when n=1? Consider the 1-state machine that halts on a 0, but keeps moving to the right when it sees 1â€™s. That, and a machine that halts on both 0 and 1, both demonstrate BB(1)=1.

Joshua B Zelinsky Says:
Comment #82 July 25th, 2020 at 10:55 am
Scott, yes, sorry, I actually meant to write n>1 there. Although now Iâ€™m worried someone is going to point out a trivial example for n=2 or n=3.

invisibules Says:
Comment #83 July 25th, 2020 at 12:00 pm
Iâ€™ve just tucked in to the paper (and havenâ€™t yet got to the meaty bits) â€” it occurred to me that props 1, 3 and 4 donâ€™t apply to all BB_L. For example in my computer language, all programs of size evenly divisible by 100 are defined to halt immediately. ğŸ™‚

invisibules Says:
Comment #84 July 25th, 2020 at 12:02 pm
ohâ€¦ but you saw to that in the definition of BL, with that sneaky <=nâ€¦

Nick Says:
Comment #85 July 25th, 2020 at 1:02 pm
Scott #69

Is it obvious that the function CC has these interesting properties? Couldnâ€™t it be that CC(n) = 1 for all n (or for all n > some n_cc)? The survey is careful not to assume uniqueness (â€˜_A_ machine M that achieves the maximum is also called _an_ â€œn-state Busy Beaver.â€â€˜), but does anyone have any idea how common or rare Busy-Beaverness really is?

Scott Says:
Comment #86 July 25th, 2020 at 1:12 pm
Nick #85: Sorry, I thought you meant halt in at most BB(n) steps (or equivalently, halt at all). I have no idea how many machines run for exactly BB(n) steps, except that itâ€™s at least as many as fit into one isomorphism class. Thatâ€™s related to the question Joshua Zelinsky just asked.

Scott Says:
Comment #87 July 25th, 2020 at 1:25 pm
Gerald #75:

Has anyone actually tried to add a supercompact or other sufficiently large cardinals to the 748-state machine in order to get the strength up to PD?
Not that Iâ€™m aware of. Thatâ€™s another worthwhile project!

And I wouldnâ€™t even think of encoding PA by doing ZF-INF â€” Iâ€™d instead look for some â€œnon-logicalâ€ arithmetical statement equivalent to Con(PA), the simpler the better.

Gerald Says:
Comment #88 July 25th, 2020 at 1:50 pm
Bunsen Burner #76: Second order logic tells us essentially nothing new. Once we assume the second oder induction axiom, the theory is complete regarding arithemtical statements. Second order peano axioms are categorical: There is only one model, the set of natural numbers. Second order ZFC (with second order versions of separation and replacement) has only the trivial models V(Îº) with Îº inaccessable.

So, second order PA (PA2) decides every statement (first order or otherwise) about natural numbers, but we often donâ€™t know wich way. GÃ¶delâ€™s completeness theorem does not hold for second order logic, there is no complete logical deduction system. PA2 â€œprovesâ€ the statement T if and only if T is actually true. Likewise ZFC2 â€œprovesâ€ the continuum hypothesis iff CH is actually true. Unless you are a hardcore platonist, the latter depends on the background set theory in use. â€œProvesâ€ of course only means â€œimpliesâ€ in the semantic sense.

While infinitary logics are an important tool in model theory and set theory, they are uninteresting for finitary math. Once we can axiomatize wellordering, i.e. â€œthere exist no n1, n2, n3 â€¦ such that n1 > n2 > n3 > â€¦, the theory completely determines finitary math, the reason being that nonstandard models of the natural numbers are not wellordered.

Nick Says:
Comment #89 July 25th, 2020 at 2:36 pm
Scot #86

I donâ€™t know if you have time to add more stuff, but it might be worth mentioning CC or something like it as an open problem (with JBZâ€™s qualifier â€œgenuinely distinctâ€ to weed out mirror images, etc). If the goal is to get these open problems out there in the hopes that answers will show up, this is one I sure would like to see answered.

BTW, the chess masterâ€™s quotation on page 3 should be capitalized I think: â€œno,â€ -> â€œNo,â€

Scott Says:
Comment #90 July 25th, 2020 at 5:05 pm
Zeb #73: I was with your comment right up to the paragraph that begins â€œThen we can use standard logical constructionsâ€¦â€, where you increasingly say things that I donâ€™t even know how to refute, they make so little sense to me! Itâ€™s easiest to start from the end: that BB(3)=21 is a theorem of Peano Arithmetic. There are no models of arithmetic where BB(3) is a nonstandard integer, or indeed anything other than 21. More broadly, though, what would it even mean to live in a physical universe governed by a nonstandard model of arithmetic? What empirical test could we ever do to tell if we were in such a universe? Would the result of such a test always be â€œoh sure, that will confirm weâ€™re in a nonstandard universe, but it will only do so at a nonstandard timeâ€”i.e., after what you naÃ¯vely regard as the end of eternityâ€? ğŸ™‚

Your comment made me realize something, though, about why I love the BB function so much: because it forces all the vague-talkers and anti-Platonists and anti-Law-of-the-Excluded-Middle people (not saying that youâ€™re one) to put their cards on the table! Itâ€™s like, you agree that itâ€™s a fact that BB(2)=6 and BB(3)=21? Well then, why isnâ€™t there a fact about the value of BB(1000)? At which n does there stop being a fact about the value of BB(n)? But once you agree that thereâ€™s a fact about the value of BB(1000), how are you not a Platonist about the positive integers, just like I am?

maline Says:
Comment #91 July 25th, 2020 at 11:13 pm
Is there really any reason to suspect a connection between the BB numbers and the (generalized) Collatz conjecture? Sure, the Busy Beavers we have found can be expressed as iterating Collatz-like maps, but always with a particular start and end. They never explore more than one orbit, and that one is always one that ends. Is there any reason to relate this to the question of whether there are orbits that do not end?

Scott Says:
Comment #92 July 25th, 2020 at 11:35 pm
maline #91: I concede that itâ€™s a genuine difference that with the Collatz Conjecture, one cares about all orbits, whereas with BB, one only cares about the orbit that starts at 0.

Still, given a Collatz-like iteration rule g, it could already be a hard problem to decide if a specific orbit of g is finite or infinite â€¦ especially to prove that itâ€™s infinite in case it is! And thatâ€™s exactly the sort of problem that might arise in proving that some of the 5- or 6-state holdout machines run forever (although I donâ€™t know for certain that it does). And of course, if we donâ€™t even know gâ€™s behavior on the orbit starting at 0, then weâ€™d seem to have little hope of understanding all the orbits! Thus, I stand by the claim that progress on determining the small values of BB could go hand in hand with progress on generalized Collatz iteration problems.

Zeb Says:
Comment #93 July 25th, 2020 at 11:43 pm
Scott #90: Hereâ€™s the thing, though. In our hypothetical nonstandard universe, there is a nonstandard version of Scott, who says, â€œCome now, here I have a proof in PA that BB(748) is at least [some huge (nonstandard) number]. Here, Iâ€™ll even write it out for you, and go through every step in detail! How can you claim that there might be some other model of arithmetic where BB(748) is smaller than that?â€ â€“ and indeed, there would be such a proof in this universe, which could be written on a nonstandardly large physical piece of paper, with a nonstandardly long length. And I could perform the physical experiment where I feed this proof to my electronic formalized proof checker (which has a nonstandardly large amount of battery life), and it would (after a nonstandardly long time) output the result â€œyes, this looks totally legit!â€ Of course, none of us would be aware that any of these quantities (length of the proof, amount of time that passes, etc.) are actually nonstandard â€“ we would think they are just ordinary, perhaps even somewhat small, numbers.

So somehow that hypothetical version of Scott in that hypothetical universe is wrong, even though his reasoning looks just like yours. This is a very confusing state of affairs for him, but for us, it is easy to explain: his â€œproofâ€ is not a true, platonic proof, since it is nonstandardly long.

The same thing might be happening here, with your claim that BB(3) is 21. The proof you can write down of the claim that BB(3) = 21 probably has at least 10 steps in it, no? If 10 were secretly a nonstandardly large integer all along, then this â€œproofâ€ would just be another nonstandard mirage, and wouldnâ€™t have any implications about what is true or false in the true, platonic, standard model of the integers.

asdf Says:
Comment #94 July 26th, 2020 at 12:37 am
Itâ€™s possible to believe in natural numbers while rejecting PAâ€™s impedicative induction axioms, i.e. recognize known numbers like 0,1,2â€¦ but reject axioms that quantify over â€œnumbersâ€ that havenâ€™t already been proven to exist. You get a system of arithmetic weaker than PA but that can apparently still handle most ordinary math like calculus. See Ed Nelsonâ€™s 200 page book or more accessible informal talk on the subject. What I donâ€™t know is whether Nelsonâ€™s predicative arithmetic proves the existence of the BB numbers. I suspect that it does not.

Scott Says:
Comment #95 July 26th, 2020 at 12:38 am
Zeb #93: Sorry, I still donâ€™t follow you. We know for sure that 10 is not a nonstandard integer! Like, even within the nonstandard models of PA, where there are nonstandard integers, the integer 10 still exists and is still standard, because we can actually construct it as 1+1+1+1+1+1+1+1+1+1. This is not a matter of opinion or interpretation.

Beyond that specific point, you might say I attach vastly less metaphysical importance to nonstandard models of arithmetic than you apparently do! For me, nonstandard models are best understood as formal artifacts of the completeness theorem. So for example, a â€œnonstandard proofâ€ of PAâ€™s inconsistency, within a model of PA+Not(Con(PA)), is not actually a â€œproofâ€ at all. Itâ€™s just a placeholder that represents PAâ€™s inability to rule out such a proof.

By analogy, Andrew Wiles presumably spent years with tokens in his brain for objects like â€œthe smallest counterexample to FLTâ€ and â€œa non-modular elliptic curve.â€ But the fact that he had these tokens, and could even do complicated manipulations of them, doesnâ€™t mean that the tokens ever had referents, any more than my daughterâ€™s tokens for â€œunicornâ€ or â€œmermaidâ€ do. For me, thatâ€™s precisely what a â€œnonstandard integerâ€ is: a token for an integer that doesnâ€™t actually exist.

And while one could of course invent laws of physics that included those tokens as basic entities (just like one could invent worlds with unicorns and mermaids), theyâ€™d be totally unlike the laws of physics that we see. And if you want to claim that our laws might secretly already involve the nonstandard entities, but we could only notice them by doing nonstandard experiments or by waiting nonstandard amounts of time or some such â€¦ well then, youâ€™ll have to take the discussion up with my nonstandard doppelgÃ¤nger, rather than with the standard me who youâ€™re talking to! ğŸ˜€

Scott Says:
Comment #96 July 26th, 2020 at 1:01 am
Persona #62 and #71: Iâ€™m sorry for the delay in replying to youâ€”I wanted a chance to think it over.

Briefly, yes, I think your proposal works! An even simpler version of your proposal would be as follows:

Let Ï‰(n) be the largest computable ordinal thatâ€™s definable by a computer program at most n bits long. Let me stick my neck out and say that Ï‰(n) strikes me as a thing that clearly Platonically exists, despite the inability of any fixed formal system (like PA or ZFC) to determine it beyond the first few values of n. Just like BB(n) itself Platonically exists, even if PA or ZFC canâ€™t calculate it beyond the first values. The one relevant difference, I guess, is that computable ordinals are not objects that you can even define using first-order quantification over the integers: you need the notion of a well-ordering (i.e., no infinite descending sequence). But Iâ€™m fine with that.

Anyway, once we have Ï‰(n), we can then define:

L(n) := BBÏ‰(n)(n).

This will grow faster than any of the functions like BBÏ‰_LC that I considered in Section 3 of my surveyâ€”in effect, by diagonalizing across all those functions.

Indeed, it should have similar growth as your version, which diagonalizes across all computable ordinals that can be proved to exist in consistent, computable extensions of ZF. To see this: my version simulates yours because mine eventually hits all the computable ordinals. But your version also simulates mine, because for every computable ordinal Î±, one could consistently extend ZF with an axiom that says â€œthe following Turing machine, M, computes the order relation of a computable ordinal, which weâ€™ll call Î±.â€

Does anyone else have any comments on this? Any reasons why itâ€™s ill-defined that I overlooked?

If not, then may I include this observation in my article? And should I thank you as â€œcommenter Persona,â€ or by a real name? ğŸ™‚

Zeb Says:
Comment #97 July 26th, 2020 at 1:17 am
Hereâ€™s an experiment you can perform, to help understand what it would â€œfeel likeâ€ to live in a nonstandard universe, where the number 10 was secretly nonstandardly large. Set an alarm to go off in 10 minutes. Then set aside all distractions, put the phone away, stop thinking about any sort of research, and just wait patiently for the alarm to go off.

Done?

Now tell me â€“ how certain are you that an eternity did not just pass, between beginning the experiment and ending it? Can you really track every single step of what happened, from minute to minute? Or was there a sort of vague feeling where the time in between became a long blur, with only a short summary claiming that quite a lot of time passed being stored in your brain?

This is, of course, a joke â€“ doing the above experiment wonâ€™t actually prove anything. Here is a real experiment you can perform to help determine whether you live in a nonstandard universe: take the Turing machine which is supposedly the busy beaver on three states listed in page 8 of your paper, and try simulating it. If it runs forever, then congratulations! You probably live in the true, platonic, standard universe (but thereâ€™s no way to be absolutely sure: nonstandard universes can mimic standard universes for the sake of fooling any particular experiment). If it ever halts, then you might want to seriously consider the possibility that you live in a nonstandard universe, where enormous â€œnumbersâ€ like â€œ21â€ exist.

[Ok, that was also a jokeâ€¦ I think. A better experiment would be to search for a contradiction in PA. If you ever find one, then either PA is truly inconsistent, or you live in one of the nonstandard universes which witness the failure of Con(PA) â€“ and either way, logic becomes very, very difficult for you to make sense of.]

Zeb Says:
Comment #98 July 26th, 2020 at 1:33 am
Scott #95: Oh, I think I understand your philosophical position better now. You seem to be saying that while the true integers really have a Platonic existence, these nonstandard models of arithmetic do not! So the fact that your nonstandard doppelganger is wrong doesnâ€™t bother you, because to you, he doesnâ€™t truly exist to you in any meaningful sense. For the same reason, you see no reason to spend huge amounts of time imagining what life is like from his point of view.

This seems consistent with your position that you donâ€™t feel confident that second order statements such as the continuum hypothesis should have a definite truth value. After all, nonstandard models of arithmetic are uncomputable, so they might coherently be viewed as being just as unreal as well-orderings of the continuum or unicorns.

Scott Says:
Comment #99 July 26th, 2020 at 2:47 am
Zeb #98: Itâ€™s not exactly that the nonstandard models â€œdonâ€™t exist.â€ Given a countable number of steps, you can even explicitly construct a countable nonstandard model of PA; the completeness theorem tells you exactly how.

Itâ€™s more like: the standard model of the integers has such an unambiguous meaning (indeed, one thatâ€™s presupposed in any mathematical discussion, including about nonstandard models), and is obviously so vastly more central to mathematics than any nonstandard models, that the word â€œtrueâ€ should just mean â€œtrue in the standard modelâ€ unless specified otherwise.

If someone insists otherwise, then I say to that person: all your study of logic did for you, is lead you into a tangle that youâ€™ll need more study of logic to get out of. Right now, you know less than someone who never studied logic at all!

To illustrate what I mean, consider the following dialogue:

Alice: â€œUnicorns donâ€™t exist.â€

Bob: â€œNo, you only mean that unicorns donâ€™t exist in our world. They do exist in logically consistent hypothetical worldsâ€”for example, those that you get by taking our world and then adjoining a unicorn to it.â€

Alice: â€œYeah, thatâ€™s what I said, that unicorns donâ€™t exist.â€

Scott Says:
Comment #100 July 26th, 2020 at 2:52 am
Joshua Zelinsky #82: OK, I also have a counterexample to your conjecture for n=2. Namely, the machine M, given by

A 0:1LB 1:H
B 0:1RB 1:1RA,

runs for 6 steps on an all-0 tape, just like the â€œofficialâ€ 2-state Busy Beaver

A 0:1RB 1:1LB
B 0:1LA 1:H.

But when started on a â€œ1â€ square (surrounded by infinite â€œ0â€ squares to either side), M halts in a single step, whereas the â€œofficialâ€ machine takes 4 steps to halt.

Having said that, looking at Pascal Michelâ€™s page, it looks like this non-uniqueness has only been observed for n=1 and n=2â€”apparently the shift-number Busy Beavers for n=3 and n=4 are essentially unique, and so is the candidate for n=5. Thus, your conjecture might hold for n=3 onwards!

Job Says:
Comment #101 July 26th, 2020 at 2:55 am
Iâ€™m wondering whatâ€™s the smallest run time that is not achievable by any n-state machine, for some n.

Since many n-state machines have huge run times, and many never even halt, the space of possible run times for an n-state machine (described by k bits) will have some holes below 2^k, right.

Thatâ€™s like finding out that a 6-state machine canâ€™t run for exactly 234 steps, though there is one that can do 235.

Maybe we could use that to prove a statement false? E.g. X is false because otherwise we can construct a 20-state machine that runs for exactly 2^105 steps, well known to be impossible even though you wouldnâ€™t guess it. Is that plausible?

Itâ€™s the lazy beaver function: Whatâ€™s the least amount of work that no one is already doing?

Scott Says:
Comment #102 July 26th, 2020 at 3:53 am
Job #101: Thatâ€™s a beautiful question! Note that, unlike BB, your LB function is computable (and, as you pointed out, upper-bounded by (4n+1)2n+1).

Clearly LB(1)=2. I just confirmed that LB(2)=7. But LB(3) might already drop below BB(3).

Note that LB increases monotonically for similar reasons to BB: given an n-state machine, we can always replicate its runtime with an (n+1)-state machine, or increase its runtime by 1 step.

Can anyone here say anything else about this function?

Gerald Says:
Comment #103 July 26th, 2020 at 5:28 am
For finite mathematics, i.e the theory of the positive integers, most mathematicians agree with Scott. The philosophy is platonism. We know what valid (standard) natural numbers look like, they can in principle be written down. And prior to any logic and formal theories there is already truth about them out there. Itâ€™s the job of mathematicians to find what is true. Itâ€™s easy to see that at least Î 1 statements like Goldbach should obviously be either true or false â€œin realityâ€. Itâ€™s then also not too hard to extend this truth-realism to all arithmetical statements.

Now, there seems to be quite an amount of confusion going around regarding GÃ¶delâ€™s incompleteness theorems and nonstandard models of arithmetics. People often mistakenly think that these theorems say that there canâ€™t be objective truth in mathematics, that truth-relativism is a fundamental featue of math. This is wrong. The incompleteness theorems only say that no effecitve formal theory can capture all truth that is already there, that every such theory must necessarily be incomplete. GÃ¶del sentences are actually true statemens about positive integers that the theory only fails to prove true. Nonstandard natural numbers are cleverly constructed infinitary objects. They are no natural numbers, they are fake integers that the theory is too weak to detect as being invalid. Nonstandard models exploit the incompletenes of a theory like PA to trick it into accepting these fake numbers as valid numbers.

The situation is very different in infinitary math. We do not know what all the â€œvalidâ€ reals (subsets of integers) are. Many set theorists believe that we have to accept a mathematical multiverse here, that statements about infinitary combinatorics like the continuum hypothesis (CH) do not come with a prior truth value attached. However, this has nothing to do with finite math and the GÃ¶del incompleteness theorems. Unfortunately some popular math articles present the indepence of CH as an example of GÃ¶del incompleteness. This is bad, itâ€™s wrong and further adds to the confusion. Neither CH nor its negation are GÃ¶del sentences. The independence of CH of ZFC is quite a different beast. Also, the reals added in Cohenâ€™s construction are completely valid real numbers, they are in no way fake or â€œnonstandardâ€. The Cohen-Models of set theory are not nonstandard models in any sense.

So, we have platonism for finite math but we probably have to accept a more formalistic foundation of infinitary math at least for now. Of course not everyone agrees with this philosophy. Ultrafinitists for example bring the limitations of the physical world into the picture. They claim that huge numbers like Grahams Number, TREE(3), BB(1000) etc. are still fictions, they are only thoughts. They are defined in an abstact way but in no realistically conceivable universere can they actually be written out. They say if ZFC is inconsistent but the shortes proof of a contradiction has length TREE(3), why should we bother? Itâ€™s hard to argue against this. IMO, if you want anti-platonism for finite math, you have to resort to some ultrafinitistic argument.

Mg Says:
Comment #104 July 26th, 2020 at 8:40 am
Conjecture: for all n>4, BB(n) is an odd perfect number such that there is a zero of the Riemann zeta function with itâ€™s imaginary part floored equal to BB(n) and real part not equal to 1/2, and also BB(n)â€™s big-endian ASCII encoding is a proof of P=NP in Coq (your favourite implementation)

Persona Says:
Comment #105 July 26th, 2020 at 9:44 am
Scott #96: Thanks for your kind reply. Of course, you may include this observation in the article. You may refer to me by Niels S. Lohmann. (Iâ€™ m not identical with any Niels Lohmann you might currently find via Google.)

Scott Says:
Comment #106 July 26th, 2020 at 11:14 am
Job #101: I now have a conjecture about your Lazy Beaver function. I think LB(n) is going to grow like nÎ©(n). In other words, I think that as n gets large, the small running times are going to get densely filled in, and the first unfilled running time will be only polynomially smaller than the total number of n-state machines. Moreover, I think that weaker results in this direction (e.g., at least some exponential growth with n) are probably feasible to prove, by giving explicit constructions that fill in the running timesâ€”likely a patchwork of constructions, with the better ones kicking in only later, but with the small values of n and small running times that the better constructions canâ€™t handle having already been handled by worse constructions.

If I mention this conjecture in my survey, how should I thank you? As â€œcommenter Jobâ€ or by a real name? ğŸ™‚

Scott Says:
Comment #107 July 26th, 2020 at 11:19 am
Mg #104:

Conjecture: for all n>4, BB(n) is an odd perfect number such that there is a zero of the Riemann zeta function with itâ€™s imaginary part floored equal to BB(n) and real part not equal to 1/2, and also BB(n)â€™s big-endian ASCII encoding is a proof of P=NP in Coq (your favourite implementation)
You see, thatâ€™s exactly why I think itâ€™s so important for us to nail down BB(5)â€”to refute conjectures like that one. ğŸ˜€

Scott Says:
Comment #108 July 26th, 2020 at 11:25 am
Gerald #103: Thatâ€™s beautifully said, better than I couldâ€™ve said it. I give it a standing ovation.

But I do disagree with you on one point. I think I have a clear enough conception of what a â€œvalidâ€ real (i.e., an infinite binary sequence) is. Itâ€™s only at sets of reals (not coincidentally, the subject of CH) that I lose a clear conception.

And related to that, I think itâ€™s crucial to understand that Cohenâ€™s construction takes place entirely in the world of countable models of ZF, not the world of â€œthe real reals.â€ So, yes, a Cohen real is a real real, but itâ€™s being added to a set of reals thatâ€™s actually only countable (except, a model of ZF â€œmistakenly thoughtâ€ that it was uncountable)! So it shouldnâ€™t be thought of as enlarging the universe of reals in any Platonic sense.

maline Says:
Comment #109 July 26th, 2020 at 1:40 pm
Scott #108: A calculable real, or an algorithmically generated binary sequence, are clearly â€œvalidâ€ â€“ but there are only countably many of those. Personally I am not at all comfortable with the concept of a â€œgeneralâ€ binary sequence as a valid object. If there is no way at all of generating or describing such an entity, then in what sense does it â€œexistâ€?

On a similar note, do you find Godelâ€™s model L, or similarly constructible models, to be less â€œtrueâ€ than von Neumannâ€™s V? Why?

Scott Says:
Comment #110 July 26th, 2020 at 2:23 pm
maline #109: So let me state my position more carefully. Iâ€™m not aware of any question of the form, â€œdoes there exist a real number / infinite binary sequence with property X?,â€ that I wouldnâ€™t accept as having a Platonically correct answer. (Can anyone suggest a candidate?) I am, of course, aware of multiple such questions about sets of reals.

Filip Says:
Comment #111 July 26th, 2020 at 3:52 pm
A fresh 2 hour Richard Karp interview hosted by Lex Fridman:
https://youtu.be/KllCrlfLuzs

Itâ€™s pretty advanced and technical ğŸ™‚

DangerNorm Says:
Comment #112 July 26th, 2020 at 4:02 pm
It occurs to me that Iâ€™ve been thinking about the meaning of S(n) a bit wrong. Iâ€™d been thinking of it as the longest that an n-state Turing machine can run and still halt, but of course this is not true: a machine that performs some computation whose runtime depends on its input, like factoring an integer, could be made to run arbitrarily long by setting it to work on a tape containing a large enough number. Hence why â€œstarting on an empty tapeâ€ is part of the definition.

However, this requires that you simply assume that the number is encoded on the tape when you start counting. If you require that the machine also encode the value of the number, then BB(n+m) puts an upper bound on the largest number that can be encoded, where n is the number of states of the machine that implements the algorithm, and m is a machine that writes out the input before simulating the n-machine.

This line of thought led me to another function that one might wonder at the properties and values of: the largest natural number that can be encoded on a Turing tape with an n-state Turing machine, such that all smaller natural numbers can also be encoded with an at-most n-state Turing machine, using the same encoding.

Zeb Says:
Comment #113 July 26th, 2020 at 4:18 pm
Gerald #103:
> They are no natural numbers, they are fake integers that the theory is too weak to detect as being invalid. Nonstandard models exploit the incompletenes of a theory like PA to trick it into accepting these fake numbers as valid numbers.

Iâ€™m not claiming that the true natural numbers donâ€™t exist and arenâ€™t meaningful, or that nonstandard naturals are true numbers. Iâ€™m just pointing out that if a model of nonstandard arithmetic looks sufficiently like the true numbers to fool PA, then it can also be used to construct a fake physics (which we might be living in right now).

I see no physical experiment we can perform which will definitively rule out the possibility that we are living in a universe based on one of the nonstandard models of arithmetic, and that a nonstandard amount of time has already passed within our lifetimes â€“ do you have one in mind?

I understand that second order axioms can pick out one single true, platonic set of integers (and that currently, the same canâ€™t be said to be true of set theory). But physics doesnâ€™t magically give us access to second-order properties of the integers, unless the physical Church-Turing thesis is dramatically incorrect.

Any refutation of this possibility must be philosophical rather than logical or physical. You can argue that there are philosophical reasons to believe that we do not live in an uncomputable universe which merely believes itself to be computable with respect to its internal model of the integers â€“ and honestly, I find such a philosophical argument fairly compelling. But there are no *logical* reasons that this couldnâ€™t be occurring, given our observations, just as there is no logical way we can be completely sure that we donâ€™t live in a simulation, or that we arenâ€™t characters in a play about the hubris of pure logic.

(My view on ultrafinitism is that it is laughably naive: just because you can count to a number and grasp it in your hands, you are convinced that that number is â€œsafeâ€? Nothing is absolutely safe.)

Scott #99: The difference between this situation and the unicorn example that you give, is that there are actually physical experiments we can perform to rule out the existence of unicorns. For instance, we can send teams of people around the world to search for unicorns, or ask our neighbors if they have ever seen a unicorn, or break into secret government labs where we believe that unicorns might be being created.

DangerNorm Says:
Comment #114 July 26th, 2020 at 4:51 pm
Actually, I suppose that function would be very slow growing, compared to BB(n), since uniquely specifying every natural number up to some n requires at least log(2)(n) bits, and there just arenâ€™t that many bits available in the specification of Turing machines, as described right on page 1.

Scott Says:
Comment #115 July 26th, 2020 at 5:48 pm
Job #101 (following up on your #106): Let T(n) be the set of n-state Turing machines. Then I believe I can now prove the following about your Lazy Beaver function:

|T(n)|/cn â‰¤ LB(n) â‰¤ |T(n)|,

for some constant c. The lower bound is via a construction that combines the â€œintrospective encodingâ€ of Ben-Amram and Petersen 2002 (see my survey for more), which should let you build an n-state machine that runs for any desired number of steps up to some |T(n)|/cn, but accurate only to within plus or minus n or so, with a more specialized gadget that uses O(log n) states to run however many additional steps between 0 and n you need. I can post more details later if anyone is sufficiently interested.

Of course one can ask many other questions about the spectrum of runtimes: for example, how much less than BB(n) is the second-longest runtime?

Scott Says:
Comment #116 July 26th, 2020 at 6:05 pm
Zeb #113: Physical experiments can only rule out unicorns in the same ways they rule out nonstandard integers in physics. What if the unicorns, being magical, are invisible and make no sound, like Carl Saganâ€™s invisible dragon in the garage? So yes, with unicorns and nonstandard integers alike, the argument is ultimately â€œphilosophical.â€ Itâ€™s something like: not only have we seen no evidence that this exists or influences physical reality, but its existence would do violence to our present understanding of physical reality, and no empirical problem has been presented that postulating this new entity would solve.

RANDY Says:
Comment #117 July 26th, 2020 at 6:06 pm
I have at multiple times in the past had an experience that seemed like I had just completed a supertask with order type Ï‰*, with no subjective connection between any portion of the Ï‰*-ordered part & the things that must have happened before. It seems this was because my brain conflated two parts of a monotonous experienceâ€”it has always been something like raking leaves or pacing that is repetitive but not static. (This seems related to Zeb #97â€™s joke experiements.) Experience continuing on after an Ï‰-ordered supertask does seem more impossible, but that could just be a failure of imagination on my part. So I do not find physical time being nonstandard to be obviously impossible.

But I am not sure if 10 being nonstandardly large makes sense, because we can explicitly enumerate the finite number ofâ€¦oh. Okay, I think I see Zebâ€™s point. We can (maybe) imagine a larger nonstandard world, but what would the standard world look like from a nonstandard world? It seems it would have to look like it just stopped after some finite point, if it were imaginable at all.

maline Says:
Comment #118 July 26th, 2020 at 6:15 pm
Scott #110: Well, Iâ€™m convinced by your argument that questions about Turing machine behavior are â€œultimately finitaryâ€, which implies that querying a HALT oracle is a well-defined â€œprocedureâ€. Therefore Iâ€™ll accept as â€œvalidâ€ any binary sequence that can be constructed using such an oracle, and so on up the ordinal hierarchy of oracles. Combining a countable set of well-defined oracles into a single oracle, with the index of desired sub-oracle given as part of the query, also seems fairly innocuous. How far does that get us? I think until, but not including, \epsilon_0. That limit step might not work, because you will need an infinitely long address to describe which sub-sub-â€¦-oracle you want to query!

As long as there is some such limit step that we treat as potentially ill-defined, we have a countable set of â€œvalid realsâ€. Diagonalization is not an issue because we cannot enumerate the set without using the unacceptable oracle.

BTW, does anyone know whether this set has a name? Does it correspond to anything a set theorist would find familiar?

Anyway, how does this leave me with regard to statements of the form you mentioned? Are there any familiar cases where a specific real number or a binary sequence can be shown to exist, but may not equal any of my â€œvalidâ€, definable ones? If so, I probably will have to modify my setup to include them. But if not, then it may be that Scott and I are in agreement: Perhaps all â€œinterestingâ€ statements of the form â€œthere exists a real number/ infinite binary sequence with property Xâ€ do have Platonic truth values, but only because they turn out to be discussing constructible entities!

Scott Says:
Comment #119 July 26th, 2020 at 6:58 pm
maline #118: Right, at some point weâ€™re going to bump up against Lowenheim-Skolem. Any theory of the reals will have a model with â€œsecretlyâ€ only countably many reals. So even when we talk about, e.g., a â€œrandom realâ€ with no definable regularities of any kind, in some sense we have no way to tell whether itâ€™s secretly just a special example of such a real, constructed as part of a countable model.

All the same, I â€¦ feel like I have a clear enough conception of what it would mean to flip a fair coin a countable number of times. And Iâ€™m very open to hearing counterexamples, but so far I havenâ€™t heard a compelling problem that would come from extending my Platonism to the whole uncountable collection of such countable sequences of coin flips. I have no similarly clear conception of what it would mean to pick a random set of reals (where do you start?), or of whether itâ€™s â€œbetterâ€ for CH or AC or Projective Determinacy to hold or not hold.

maline Says:
Comment #120 July 26th, 2020 at 8:00 pm
Scott #119: Well, if our universe (spacetime universe, not universe of discourse!) is actually infinite in extent, then there really are infinitely many independent qubits. There probably are even infinitely many literal coins. So I guess arbitrary infinite sequences might be a real thing whether we like them or notâ€¦

BTW, is there a sensible formalism to deal with infinitely many qubits? How could you give amplitudes to uncountably many sequences, while keeping the total probability finite?

I also take back my point about â€œinfinitely long addressesâ€. The sub-oracles are indexed by ordinals, each of which can be written naturally and finitely in the Cantor notation. So now Iâ€™m confused as to where would be a good place to stop allowing oracles.

Scott Says:
Comment #121 July 26th, 2020 at 8:29 pm
maline #120:

BTW, is there a sensible formalism to deal with infinitely many qubits? How could you give amplitudes to uncountably many sequences, while keeping the total probability finite?
The question is an ironic one, since the physicists (especially in the context of QFT) were dealing with all the complications of infinite numbers of qubits for generations before quantum information came along, and started worrying about only finite numbers! Anyway, the short answer to your question is yes. You do much the same thing as in classical probability over continuous spaces, and assign probabilities not to individual points but to intervals (and calculate the probabilities by integrating a density). Things are nicer when the Hilbert space has a countable basis, but itâ€™s possible to do QM even in Hilbert spaces of uncountable dimension. But itâ€™s complicated! And I, for one, am infinitely grateful (finitely grateful?) for the finitude of my Hilbert spaces.

Job Says:
Comment #122 July 26th, 2020 at 9:18 pm
If I mention this conjecture in my survey, how should I thank you? As â€œcommenter Jobâ€ or by a real name?

Thanks, but no need. Itâ€™s easier for me to learn when using an alias. Like, a commenter is good enough if you need to blame someone. ğŸ™‚

I believe I can now prove the following about your Lazy Beaver function:
|T(n)|/c^n â‰¤ LB(n) â‰¤ |T(n)|,
for some constant c.

Are there any bounds on c?

Also, LB(n) is strictly less than |T(n)| right?
E.g. if NH(n) is the number of non-halting n-state machines, then LB(n) is no more than |T(n)|-NH(n)? Plus lots of machines will have the same run time.

Since |T(n)| â€“ LB(n) is both computable and an upper bound on the number of candidate machines for BB(n), i wonder how large LB(n) is even allowed to be.

E.g. if you could always narrow down BB(n) to n^c machines, for some fixed c, would anything break?

Scott Says:
Comment #123 July 26th, 2020 at 9:47 pm
Everyone: Now that I have my own Busy Beaver search code up and running, I can start answering some of the empirical questions without relying on others! Here are two nuggets for tonight:

â€“ LB(3)=22. In other words, 3-state Turing machines fill out the entire spectrum of runtimes from 1 up to BB(3)=21, so the third Lazy Beaver number is just BB(3)+1=22, similar to LB(1)=BB(1)+1 and LB(2)=BB(2)+1. We know this pattern must break down by LB(6)<<BB(6) if not earlier.

â€“ The 3-state Busy Beaver is indeed unique up to trivial isomorphisms.

asdf Says:
Comment #124 July 26th, 2020 at 11:20 pm
If we believe the Church-Turing thesis then we canâ€™t verify the existence of long random bit strings (say from coin tosses or quantum experiments). But we go around believing in their existence. Standard and nonstandard integers seem about the same way. Similarly, GR lets us predict stuff about the interior of black holes, but by definition (if we believe GR) there is no way to observationally verify the predictions. It usually doesnâ€™t bother us too much. I donâ€™t see how physics can even confirm the existence of ordinary computable numbers like A(100), let alone busy beaver numbers. The number of informational bits in the universe is much smaller than the log of that number. So itâ€™s all philosophy again.

STEM Caveman Says:
Comment #125 July 26th, 2020 at 11:33 pm
Busy Beaver for halting time (rather than output size) is an obfuscation. There is nothing interesting you can say about specific values of BB(n) that is not an obfuscation of a clearer and sharper statement about halting of an n-state Turing machine. The reason is that knowing BB(n) is equivalent to solving the halting problem for all n-state TMâ€™s, but any specific statement about BB for specific n comes through individual machines, e.g., lower bounds, or that 27 states is enough to encode a search for Goldbach counterexamples and 748 encompasses ZF.

> â€œnonstandard integerâ€ is: a token for an integer that doesnâ€™t actually exist.

Nonstandard integer is an an anti-concept introduced to avoid speaking explicitly about syntax.

Scott Says:
Comment #126 July 26th, 2020 at 11:43 pm
STEM Caveman #125: Wait â€¦ youâ€™re saying that, whenever we look at the maximum of a finite set, there must be a particular element in the set that achieves the maximum? Thank you! How did I write a whole article on the BB function while missing such a profound insight? ğŸ˜€

Scott Says:
Comment #127 July 26th, 2020 at 11:53 pm
asdf #124: Iâ€™m not even sure what it means to â€œconfirm the existenceâ€ of an integer. Do you agree that 1080 exists? Would we need to count up to it, maybe with fingers and toes, to be sure it did? Or is it enough that astronomers assure us that there are >1080 atoms in the observable universe? What if next week the astronomers revise their models and say there are actually >108000 atomsâ€”will the latter number then pop into existence? Who would want to use language in such a tortured way? Why not simply say that, in whatever sense 10 and 20 â€œexist,â€ 108000 and A(100) and BB(100) clearly also â€œexistâ€? And even though we canâ€™t well count to them, we can (wonderfully) still study them and prove things about themâ€”just like we donâ€™t need to fly to the Sun and touch it in order to learn about its composition?

Scott Says:
Comment #128 July 27th, 2020 at 12:01 am
Nick #66, #85, #89: Iâ€™m doing final revisions to the article now, and decided to add a short section on the question of uniqueness of Busy Beavers. Should I thank you as â€œcommenter Nickâ€ or by a real name?

STEM Caveman Says:
Comment #129 July 27th, 2020 at 12:05 am
@Scott 126

Not to deny you the fun of â€œsneering the messengerâ€, but:

Apart from the communication and comprehension overhead caused by talking in terms of BB (and the fun of â€œsneering the messengerâ€), this is a finitary version of issues about â€œpredicativityâ€ that arise when defining something as a max or min of some set of values. Itâ€™s not as trivial as you imply since membership in the set is not computable. Generally predicative definitions are easier to understand and work with whereas impredicative are chosen for their Aesthetic or Metaphysical advantages.

maline Says:
Comment #130 July 27th, 2020 at 12:53 am
Scott #121: Can you give me a keyword or reference on working with infinitely many qubits?

In standard presentations of QFT the issue is avoided by working in Fock space: there are only countably many field modes and the total number of excitations must be finite. My question is about what happens if you drop the second condition and allow infinitely many particles, as is appropriate for an infinite and homogenous universe.

Sniffnoy Says:
Comment #131 July 27th, 2020 at 1:05 am
Scott #119:

Hm, so you donâ€™t have a problem with â„˜(N), but you do have a problem with â„˜(â„˜(N))â€¦ what about â„˜(Ï‰_1)? Since the question you asked was â€œwhere do you startâ€â€¦ ğŸ™‚

Luke G Says:
Comment #132 July 27th, 2020 at 1:26 am
maline #118

On the question of how far you can get up the hierarchy of ordinals, you can get considerably farther than epsilon_0 with â€œwell definedâ€ operations by introducing the Veblen function, which is basically a way of systematizing diagonalization.

Using the Veblen function, you can get up to the Feferman-Schutte ordinal with only â€œwell definedâ€ operations. More precisely speaking, this ordinal is often considered the limit of â€œpredicativeâ€ mathematics. â€œPredicativeâ€ basically means you can only build on what youâ€™ve already established: sets can only be defined in terms of sets that you already know exist. To get above the Feferman-Schutte ordinal, you need to use impredicative principles, such as assuming larger ordinals already exist even though you havenâ€™t yet given a construction for them. (Disclaimer: there is some disagreement on exactly what â€œpredicativeâ€ means and hence the corresponding ordinal.)

Jonathan Weinstein Says:
Comment #133 July 27th, 2020 at 1:51 am
Thanks, I enjoyed very much the BB survey. I also scanned through your older article on big numbers. There are some (perhaps difficult) questions that would come up if we made the rules in the â€œbigger numberâ€ game that you can call BB, but not any other functions besides those on a calculator (which has exponents, but not factorials, letâ€™s say.) Should I spend my limited time nesting BBs? Or putting a tower of exponents inside a BB? The following seems like a reasonable conjecture at a recursive answer: If it costs time d to write BB(), then the optimal solution given total time t is to write the optimal solution for time t-d, then put BB() around it.

Scott Says:
Comment #134 July 27th, 2020 at 2:05 am
Jonathan #133: If youâ€™re allowed only BB and calculator functions, then definitely, without a doubt, just nest BB as many times as you can (except possibly in the last second). I guess stuff like â€œBB(BB(â€¦BB(100))) nested BB(100) timesâ€ is disallowed under your rules?

Jon Says:
Comment #135 July 27th, 2020 at 2:59 am
Scott #119: re: sequences of coin flips, you may like https://arxiv.org/abs/math/0610705.

Sniffnoy Says:
Comment #136 July 27th, 2020 at 3:27 am
maine #118, Luke G #132:

Expanding on Luke Gâ€™s final comment, hereâ€™s an article by Nik Weaver arguing that the idea that Î“_0 is the first impredicative ordinal is unfounded, that it is in fact entirely predicative, and that the first impredicative ordinal is in fact much larger (larger than the small Veblen ordinal, and even â€” he claims but doesnâ€™t explicitly argue â€” larger than the large Veblen ordinal). Note: Anything involving predicativity is very much not my area and I canâ€™t really meaningfully discuss this myself, other than to say that it seems convincing to me.

Also maine, just in case youâ€™re not familiar with all these big countable ordinals, this series of blog posts by John Baez (1, 2, 3) is I think a good introduction. ğŸ™‚ (Although part 1 will probably be things you already know.)

Persona Says:
Comment #137 July 27th, 2020 at 8:39 am
What about the Busy Beaver function of a Turing machine with oracle access to L?

Scott Says:
Comment #138 July 27th, 2020 at 9:00 am
Persona #137: For which L?

Persona Says:
Comment #139 July 27th, 2020 at 9:10 am
Scott #138: L as defined in Scott #96.

Scott Says:
Comment #140 July 27th, 2020 at 9:33 am
Persona #139: Oh sure, you can always take BB with an oracle for whatever fast-growing function you previously defined (this is the BB version of whatâ€™s called the â€œTuring jumpâ€ in computability theory). Iâ€™m curious about whether you can go beyond diagonalizing across all the computable ordinal notations in some more impressive way than that.

STEM Caveman Says:
Comment #141 July 27th, 2020 at 11:27 am
@Scott

> Given a countable number of steps, you can even explicitly construct a countable nonstandard model of PA; the completeness theorem tells you exactly how.

What? There are such strong limitations on how explicit it can be that, like Alice and Bob talking about unicorns, it is pretty much everything we could mean if we said â€œthere is no such thing as an explicit nonstandard modelâ€.

You are very much correct about learning so much logic that even more logic is needed to escape the trap. But this can also be applied to the BB function â€”- a grand theory of hypothetical numbers-according-to-a-formalism, that lack the materiality of what we think of as integers but (lo-GIC!!!!) get talked about as though they were, out of force of verbal habit.

Curtis Bright Says:
Comment #142 July 27th, 2020 at 11:45 am
Sniffjoy #131:
If Iâ€™m understanding Scott correctly then he does have a problem with â„˜(N). Because if this set platonically exists then presumably there would be a definite answer to if it has a subset whose cardinality is strictly between the cardinalities of N and â„˜(N). (You also need to believe in the existence of â„˜(N)Ã—â„˜(N) to formalize this.)

Scott Says:
Comment #143 July 27th, 2020 at 12:09 pm
STEM Caveman #141: Sorry, by â€œconstructâ€ I just meant â€œfollow the iterative process from the proof of the Completeness Theorem.â€ I didnâ€™t mean that youâ€™d end up with computable addition or multiplication operations (by Tennenbaumâ€™s theorem, you wonâ€™t).

Also, the surest sign that your study of logic has led you into a trap that you need more logic to get out of, is if you start to doubt the â€œmaterialityâ€ of (by which we simply mean, objectivity of truths about) positive integers, whether weâ€™re talking about 10 or about BB(10). ğŸ™‚

Scott Says:
Comment #144 July 27th, 2020 at 12:22 pm
Sniffnoy #131 and Curtis Bright #142: Since â€œhave a problem withâ€ is a bit vague, let me make this discussion concrete as follows. Can either of you propose a candidate for a mathematical statement with an indefinite truth-valueâ€”like I think is very plausibly the case for CHâ€”but whose â€œlogical complexityâ€ is lower than CHâ€™s?

Somewhere between quantification over reals and quantification over sets of reals, we seem to lose Platonicity, and it would be fascinating to zoom in on where (although, if I try to look, I fear getting lost in a thicket of arcane ordinal notations).

Vaarsuvius Says:
Comment #145 July 27th, 2020 at 12:33 pm
Being entirely a novice in this area, my questions may or may not be laughable to the extreme. Nevertheless:

1. The Busy Beaver number BB(n) seems to be a definition based on a qualifier i.e. â€œthe grains of sand in the largest beach on Earthâ€. Of course, beaches are real places whose average volume of sand can be counted, just as the Busy Beaver is a condition that can be tested for by evaluating the possibility space of all possible Turing machines with n states. Doesnâ€™t that make BB(n) as â€œrealâ€ a number as n, given that nâ€™s definition in PA is just the number that satisfies the condition (n-1)+1 (speaking in extremely loose terms, since addition is defined recursively and each new number defined in terms of the previous)?

2. Can we design a program to check for BB numbers in anywhere near reasonable time, even if you allow quantum computers?

Nick Says:
Comment #146 July 27th, 2020 at 12:50 pm
Wowee, a mention in section 5.7! Iâ€™ll claim comment #66 as â€œNick Drozdâ€.

The definition of â€œessentially differentâ€ in section 5.7 is not what I would have expected. Take a machine M and construct a new machine Mâ€™ by replacing every Left shift with a Right and vice versa. Mâ€™ will be just like M except that it runs in the opposite direction, so we wouldnâ€™t want to count M and Mâ€™ separately for BB purposes. Maybe there are some other transformations like this. Iâ€™m sure thereâ€™s a more succinct way of describing this idea. Importantly, it is tedious but straightforward (primitive recursive) to verify that two machines are similar in this sense.

In contrast, your definition says that machines M and Mâ€² are different â€œif there exists an input configuration on which they run for different numbers of stepsâ€. Certainly this is a striking way to define similarity, and it underscores the point from section 1.1 about â€œa goal completely orthogonal to any normal programmerâ€™s goalâ€. But it doesnâ€™t seem like a straightforward property to test for. Is it even decidable in general? If so, is it primitive recursive?

Is it possible for two machines that exhibit different behavior to be runtime-equivalent? That is, do there exist two machines that run for exactly the same number of steps on all inputs but that leave different tape contents for some input?

Gerald Says:
Comment #147 July 27th, 2020 at 12:58 pm
Scott #119: â€œAll the same, I â€¦ feel like I have a clear enough conception of what it would mean to flip a fair coin a countable number of times. And Iâ€™m very open to hearing counterexamples, but so far I havenâ€™t heard a compelling problem that would come from extending my Platonism to the whole uncountable collection of such countable sequences of coin flips. I have no similarly clear conception of what it would mean to pick a random set of reals (where do you start?), or of whether itâ€™s â€œbetterâ€ for CH or AC or Projective Determinacy to hold or not hold.â€

This is a strong statement, Iâ€™m not that confident. Should there be a nonconstructible real? Should 0# exist? Should Projective Determinacy hold? Must god know the one correct answer to these questions?

If you say you have a clear conception about arbitrary (!) Ï‰-sequences of coin flips (reals) does this intuition include quantifying over reals? Should every second-order arithmetic statement (allowing quantifiers over reals) have a definite â€œrightâ€ truth value?

While CH is a third-order statement involving quantifiers over arbitrary sets of reals, PD talks only about definable sets of reals. It is an infinite scheme of statements in second-order number theory, similar to induction in PA. Should it have a definite truth value? We know it fails in L but follows from large cardinals.

The reason why Iâ€™m a platonist about first-order arithmetics is that I know the quantifiers should only range over numbers I can at least in principle write down. If Goldbach is wrong we can in principle write down an actual counterexample. This does not apply to the reals, they are uncountable, I donâ€™t have a name for most of them. There are many forcing constructions that add a single or only few new interesting reals with special properties. I canâ€™t see in general wether these new reals should exist or not, they all seem legit. I donâ€™t see when the set of reals is ever complete, it seems to never be. Making the universe broader adds new reals. Making the universe higher also adds new reals of a different kind. For the positive integers every model of even the weakest theory already contains all of them.

Btw. the countable ground model is only a technicality in some approaches to forcing. One can start with an arbitrary transitive model (even V itself) and then construct V[G] via the boolean-valued-model/ultrafilter construction.

Scott Says:
Comment #148 July 27th, 2020 at 1:16 pm
Vaarsuvius #145:

1. Yes, Iâ€™d say that the BB numbers are as sharply defined as any other numbers in math. People have even determined the first 4 of them!

2. There is no algorithm, classical or quantum, to calculate BB(n) given n, in any finite amount of time. Thatâ€™s the whole point of being uncomputable. The first 4 values were determined via a combination of automated tools and ad hoc, hand analysis of particular machines to prove that they didnâ€™t halt.

Nick Says:
Comment #149 July 27th, 2020 at 1:19 pm
Hereâ€™s a historical question about Radoâ€™s paper â€œOn Non-Computable Functionsâ€. Reading over it just now, I notice that the abstract, the introduction, and the closing summary do not make any mention of Turing machines or the Busy Beaver function. Instead, they discuss the general point that non-computable functions can be defined using â€œextremely primitiveâ€ means, like â€œthe principle that a finite, non-empty set of non-negative integers has a largest elementâ€, and without using any diagonal arguments. The rest of the paper describes the Busy Beaver game in detail.

So the question is: did Rado sandwich the Busy Beaver game in those general observations at the behest of an editor? Iâ€™m imagining a scenario where Rado just writes about BB, and an editor says, thatâ€™s frivolous, recreational, etc, and then Rado (or maybe even the editor) adds the stuff about â€œexceptionally well-definedâ€ functions as a way of justifying BBâ€™s importance. But thatâ€™s just my feeling. Does anyone know for sure?

STEM Caveman Says:
Comment #150 July 27th, 2020 at 1:50 pm
> PA is equivalent to ZF-INF (axiom of infinity deleted)

Negated, not deleted. ie, ZF + Not(INF).

Sniffnoy Says:
Comment #151 July 27th, 2020 at 2:48 pm
Hm, so having now read the part about beeping busy beavers â€” is there any way to extend this construction to make a function that grows like BB_k for any given k? Like is there a way to get an approximation of BB_2 via â€œbeep-beeping busy beaversâ€? ğŸ™‚

dm Says:
Comment #152 July 27th, 2020 at 3:00 pm
Your survey is surprisingly accessible to a biologist with limited math sophistication. I (apparently) followed things well-enough that the question of uniqueness came to mind just in time to be addressed by your conjecture.
1) What does the distribution of halting run-times look like for TMâ€™s below the champion, for say n=5? Iâ€™m guessing that the current champion is well separated from the pack.
2) What is the smallest state number needed to implement the Collatz map on consecutive numbers? Presumably, BB(n) for that number must be greater than 2^68.
Thanks!

Scott Says:
Comment #153 July 27th, 2020 at 4:15 pm
Sniffnoy #151:

is there any way to extend this construction to make a function that grows like BB_k for any given k? Like is there a way to get an approximation of BB_2 via â€œbeep-beeping busy beaversâ€?
Hey, I wondered about that too! My guess was that itâ€™s just a happy accident that thereâ€™s a definition for BBB thatâ€™s essentially as simple as the definition of BB itself, and that as k gets large, the definitions for BBk will necessarily get more and more kludgy and complicated. But Iâ€™d love to be proven wrong about that!

Scott Says:
Comment #154 July 27th, 2020 at 4:29 pm
Gerald #147: Well, as a quantum mechanicist in my day job, Iâ€™m more than happy to take a fair coin-flip as a basic primitive of my conceptual universeâ€”and having flipped a coin 10 or 20 times, Iâ€™m more than happy to add an ellipsis and imagine the same process continuing forever. ğŸ˜€ Whereas thereâ€™s nothing Iâ€™m similarly happy with that would need to be done separately to each point on a continuum.

Having said that, Iâ€™m entirely open to being convinced that thereâ€™s some question about the existence or nonexistence of an infinite binary sequence, that Iâ€™d admit as probably not having a Platonic answer. I just havenâ€™t seen it yet.

Regarding 0# and nonconstructible reals, canâ€™t we even explicitly calculate a few of their digitsâ€”just like we can explicitly calculate the first few values of the BB function? Canâ€™t we imagine whatever we did continuing to infinity? If so, then my feeling is unequivocally: yes, these objects exist. Which is entirely compatible with the fact that set theories that we might want to use might be unable to talk about the objectsâ€”because if they could then theyâ€™d yield a contradiction, and indeed the objects were basically constructed by diagonalizing against those set theories.

I still donâ€™t understand the various determinacy axioms well enough to have feelings one way or the other, although perhaps Iâ€™ve already committed myself to a view with what I said above.

Scott Says:
Comment #155 July 27th, 2020 at 8:12 pm
dm #152:

1) There are probably many, many interesting things to figure out about the distribution of runtimes, and I hope my survey plays some part in inspiring people to do that! But, yes, the current n=5 and n=6 champions are well separated from the pack, although there are other 5- and 6-state machines with â€œqualitatively similarâ€ behavior (e.g., running for about half as long as the champion in the 5-state case, or 10 to some slightly less huge number in the 6-state case).

2) Collatz is not exactly a question about the halting of a Turing machine on a specific inputâ€”itâ€™s a question about whether a Turing machine halts on all inputs. So Collatz and BB arenâ€™t directly comparable. Still, Pascal Michel has indeed studied the smallest Turing machines that implement the Collatz map on arbitrary inputsâ€”see for example this paper.

Curtis Bright Says:
Comment #156 July 27th, 2020 at 10:16 pm
If I understand your view correctly, Scott:

&bullet; Statements in first-order arithmetic: have definite truth-values.

&bullet; Statements in second-order arithmetic: you suspect they have definite truth-values, but you are open to counterexamples.

&bullet; Statements in third-order arithmetic: may not have definite truth-values (e.g., the continuum hypthothesis).

I found a paper that lists a number of statements equivalent to a â€œsecond-order version of the continuum hypothesisâ€ (equivalent to what Gerald mentioned, that all reals are constructible). So you would have to believe that either the propositions in Thm 1.1 are true or the propositions in Thm 1.2 are true, and maybe you doâ€”Iâ€™m not sure if these count as plausible â€œindefinite truth-valueâ€ statements or not.

Bruce Smith Says:
Comment #157 July 28th, 2020 at 1:19 am
Regarding the â€œopen problemâ€ of â€œimproving Proposition 1â€ (BB(n+1) > BB(n)), I think I can improve it in two distinct ways â€” one that I suspect might be obvious (though I think your paper could usefully mention it), and one that your paper implies is new.

Theorem X1 (suspected obvious): BB(n+2) â‰¥ BB(n)(1 + 2/n).

Theorem X2 (new): BB(n+1) â‰¥ BB(n) + 2.

Here are their proofs, encoded by rot13 for those of you whoâ€™d like to puzzle them out first:

Proof of X1:

Yrg O or n znkvzny-ehagvzr znpuvar va G(a), naq yrg C or gur fgngr bs O juvpu vf zbfg bsgra gur pheerag fgngr nf nal fgrc bs O fgnegf ehaavat. (C znl be znl abg or gur vavgvny fgngr.)

Abgr gung C vf gur pheerag fgngr ng gur fgneg bs ng yrnfg OO(a)/a fgrcf.

Jr jvyy znxr n arj znpuvar Oâ€™ ol zbqvslvat O, nqqvat gjb arj fgngrf naq vapernfvat vgf ehagvzr.

Gb qb guvf, fgneg jvgu O, gura ercynpr vgf fgngr C jvgu n guerr-fgngr frdhrapr, A1 A2 C, jurer gur frdhrapr A1 A2 â€œqbrf abguvatâ€ naq C npgf nf orsber. (Vs C jnf gur vavgvny fgngr, A1 vf abj gur vavgvny fgngr; bgurejvfr gur vavgvny fgngr vf hapunatrq.)

Zber cerpvfryl, rirel rkvfgvat â€œgenafvgvba gb Câ€ (fbzr fhofrg bs gur 2a genafvgvba ehyrf va O) vf ercynprq ol na bgurejvfr-vqragvpny genafvgvba gb A1. Qrsvar A1 gb abg nygre gur gncr (v.r. jevgr gur fnzr ovg vgâ€™f ernqvat), zbir yrsg, naq tb gb A2. Qrsvar A2 gb abg nygre gur gncr, zbir evtug, naq tb gb C. Qrsvar C nf orsber (rkprcg gung vs bar bs vgf rkvfgvat genafvgvbaf jrag gb C, vg abj tbrf gb A1 qhr gb gur zbqvsvpngvba nyernql qrfpevorq).

Pbzcnevat n eha bs O naq n eha bs Oâ€™, rirel fgrc fgnegvat abg ng C orunirf nf orsber. Rirel fgrc fgnegvat ng C orpbzrf n frevrf bs guerr fgrcf fgnegvat ng A1, A2, naq C, erfcrpgviryl. Gur A1 fgrc zbirf yrsg, gur A2 fgrc zbirf evtug (arvgure bar nygref gur gncr), gura gur C fgrc fgnegf va na vqragvpny fgngr nf vg qvq va O, fb vg orunirf vqragvpnyyl.

Fvapr jr pubfr C gb eha ng yrnfg OO(a)/a gvzrf, rnpu bs A1 naq A2 nyfb ehaf gung znal gvzrf. Gur bgure fgngrf eha gur fnzr ahzore bs gvzrf nf orsber. DRQ.

(Vs genafvgvbaf jrer nyybjrq gb â€œzbir arvgure Y abe Eâ€, jr pbhyq nqq bayl bar arj fgngr, naq cebir OO(a+1) â‰¥ OO(a)(1 + 1/a).)

Proof of X2:

Qrsvar O nf orsber, ohg yrg C or gur fgngr juvpu vf pheerag jura vg unygf (va vgf fvatyr eha ba gur mreb gncr), naq (nsgre gung unyg) yrg gur gncr pbagnva o1 ng gur pheerag cbfvgvba, naq o2 gb vgf vzzrqvngr yrsg. Abgr gung guvf vzcyvrf Câ€™f ehyr sbe o1 vf UNYG. (Gurer znl or bgure UNYG vafgehpgvbaf va gur znpuvar, va C be va bgure fgngrf, ohg gurl arire eha.)

Jr jvyy sbez n arj znpuvar Oâ€™ sebz O ol erivfvat C naq nqqvat bar arj fgngr A.

Yrnir Câ€™f ehyr sbe pbzcyrzrag(o1) hapunatrq, ohg ercynpr Câ€™f UNYG ehyr sbe o1 jvgu â€œjevgr pbzcyrzrag(o2), zbir Yrsg, tb gb Aâ€.

Qrsvar A fb gung jura vg ernqf o2, vg zbirf evtug, ohg jura vg ernqf pbzcyrzrag(o2), vg unygf.

Pbzcnevat gur eha bs Oâ€™ gb gur eha bs O, gurl orunir vqragvpnyyl hagvy gur ynfg fgrc bs O, jura O unygf ohg Oâ€™ qbrfaâ€™g. Gur gncr ng gur fgneg bs gung fgrc (va rvgure znpuvar) ybbxf ybpnyyl yvxr [o2] [C o1]. (Gung vf, vg unf gjb fhpprffvir pryyf nf fubja, jvgu gur frpbaq bar pheerag naq gur fgngr orvat C, naq nyy bgure pryyf pbhyq or nalguvat.)

Va Oâ€™, gur arkg fgngr ybbxf yvxr [A o2] [pbzcyrzrag(o2)], naq gur fgngr nsgre gung ybbxf yvxr [o2] [A pbzcyrzrag(o2)], naq nsgre gung gur znpuvar unygf. DRQ.

[end]

Joshua B Zelinsky Says:
Comment #158 July 28th, 2020 at 7:33 am
It might also be of interest to examine the graph structure of the Busy Beaver functions. Often when introducing Turing machine to students, we represent it as a directed graph with vertices for states and directed edges for transitions. We can for a given Turing machine ask then about the graph properties its graph has.

In that context, do Busy Beaver machines always have strongly connected graphs (in the sense that given any two distinct vertices A and B, there is always a directed path from A to b). This is true for the 2 state, 3 state, and 4 state machines, as well as the candidates for 5 and 6. This is *not* the case for Wythagorasâ€™s 7 state candidate.

Hereâ€™s a rough intuition for why we should expect Busy Beaver machines to be strongly connected: Assume that a given Turing machine is not strongly connected. Then we can partition the states into two non-empty subsets X and Y, such that after we have enter a state in Y we never return to any states X. Note that at some point all transitions stay in Y. Letâ€™s say X has x total states and Y has y total states. Then the X component is only used for at most BB(x) states, and has written at most BB(x) tape symbols.

Define a T(k,n) to be the largest number of states a Turing machine can run for followed by halting when run on some input of length no larger than k. (Note that T(0,n) = BB(n)). Now, assume that the m state Busy Beaver has a partition given as above, with m= x+y

Then BB(x+y) would be bounded above by BB(x) + T(BB(x),y). This seems unlikely to be true for large m.

A slightly weaker but also plausible guess is that there is some constant k such that any Busy Beaver machine has a strongly directed graph with at most k exceptional states/vertices And again, a similar argument would apply to the above.

If this is true in general, this might also in some sense give implicitly a statement about the limits of introspection; if I understand the introspection technique, it should often lead to Turing machines which are not strongly connected.

Joshua B Zelinsky Says:
Comment #159 July 28th, 2020 at 9:01 am
Bruce @ #157,

The first of your two statements I think has been proved before (someone proved that result in an earlier comment thread here).

Your second proofâ€™s trick of using the actual situation on the tape at the halt is clever.

Vasek Says:
Comment #160 July 28th, 2020 at 9:22 am
I wonder, can some of the conjectures from the survey make a good project to PolyTCS (https://polytcs.wordpress.com/)? I feel that an understandable problem that anyone can start playing with could have its place there.

Scott Says:
Comment #161 July 28th, 2020 at 1:43 pm
Curtis Bright #156: Yes, thatâ€™s precisely my view.

I looked at the paper you linked and found it quite interesting, but Iâ€™d strongly prefer a candidate whose statement didnâ€™t involve Î£12 definability or whateverâ€”one that was just directly about the underlying concepts, like CH is.

Scott Says:
Comment #162 July 28th, 2020 at 1:57 pm
Bruce Smith #157: Nice!! May I include a link to your comment in the paper and acknowledge you? (Although maybe I should de-rot13 it first.)

Scott Says:
Comment #163 July 28th, 2020 at 2:01 pm
Joshua #158: Goddammit, yet another great question from you! Actually, the same question had rattled around in my mind when I looked at the current BB(7) champion and thought â€œthat canâ€™t possibly be optimal,â€ but I never formulated the conjecture explicitly.

Alas, Iâ€™m already way over SIGACT Newsâ€™s page limit, and am not sure if I should risk the editorsâ€™ ire by exceeding BB(3) pagesâ€¦ ğŸ™‚

maline Says:
Comment #164 July 28th, 2020 at 2:49 pm
Luke G #132: Let me try to be explicit about what I am assuming. Letâ€™s say that I accept the following:

1) The questions of whether a given Turing machine will halt on a given input, and of what the output will be if it does halt, are â€œwell-definedâ€ (meaning they have â€œabsolutely trueâ€ answers).

2) A â€œwell-definedâ€ oracle is one that provides values of a â€œwell-definedâ€ function from the naturals to the naturals.

3) The behavior of a Turing machine that is able to query a â€œwell-definedâ€ oracle is itself â€œwell-definedâ€.

Now consider the ordinal hierarchy of oracles, where oracle 1 answers the Halting problem, oracle n+1 answers the Halting problem for machines with access to oracle n, and the oracle for a limit ordinal allows access to all of the previous oracles. Up until what ordinal should I consider these oracles to be â€œwell-definedâ€?

It seems clear that the only obstacle will be if, at a particular limit ordinal, the capability of accessing all previous oracles cannot be expressed as a â€œwell-definedâ€ function from the naturals to the naturals â€“ that is, the command to query a particular sub-oracle cannot be expressed as a natural number in a â€œwell-definedâ€ way. My initial thought was that this might happen at epsilon_0, because any ordinal smaller than epsilon_0 is naturally indexed by tuples of a fixed length: the coefficients in the Cantor normal form.

But this of course was silly of me; there are infinitely many other indexing schemes that can work for much larger ordinals. Indeed, if I understand correctly, any ordinal smaller than the Church-Kleene ordinal can be put in bijection with the natural numbers in a recursive way. Such a bijection would provide exactly the indexing we need to â€œwell-defineâ€ our limit ordinal oracle. But we can do even more: why should we be limited to recursive schemes, when we have this incredible tower of oracles at our command? All we should require is that, for each particular limit ordinal, there must be an indexing scheme that can be decoded by a Turing machine equipped with some specific earlier oracle. So how far up can we actually go?

Bruce Smith Says:
Comment #165 July 28th, 2020 at 2:56 pm
Scott #162: Please do! And if you can locate the original of Theorem X1 (see Joshua #159), I think thatâ€™s highly worth mentioning too, since the paper has very few limits on BB(n + c) in terms of BB(n) for any small constants c.

Bruce Smith Says:
Comment #166 July 28th, 2020 at 3:07 pm
Scott #163 and Joshua #158: I do think that argument for a â€œlikely-almost-connected directed graphâ€ is pretty interesting, and worth mentioning (I think you could do it in a single paragraph).

More generally, so are intuitive arguments modeling the likely evolution of a long-running machine as a â€œpseudorandom walk through tape states which slowly lengthenâ€. For example, that would â€œexplainâ€ the runtime being quadratic in the number of ones left or the number of cells reached (due to statistics of random walks of tape position). (Iâ€™m sure such ideas are not original. I even think I read them already in related old columns by Martin Gardner.)

Bruce Smith Says:
Comment #167 July 28th, 2020 at 3:32 pm
Thinking more, I think I was wrong about the random walk statistics prediction. It would only hold for the part of the runtime spent while the tape state had already reached maximal length, which would be a small fraction of the runtime. Your earlier explanation of the observed quadratic relation, implying a more systematic scanning of the tape, seems better.

I suppose people have examined machine-state sequences to see how random they look. (BTW, Iâ€™m sure Wolfram must have written up some experiments related to this in A New Kind of Science.)

Luke G Says:
Comment #168 July 28th, 2020 at 9:44 pm
maline #164

This is getting outside my familiarity (so I hope someone here can correct me if Iâ€™m wrong!), but my understanding is that adding BB oracles doesnâ€™t actually give you access to ordinals above the Church-Kleene ordinal. In particular, the Church-Kleene ordinal is the both limit of recursive ordinals, and the limit of hyperarithmetical ordinals.

Bruce Smith Says:
Comment #169 July 28th, 2020 at 11:53 pm
I think I can improve Proposition 1 further!

Theorem X3: BB(n+1) â‰¥ BB(n) + 3.

Proof:

Let B be a maximal-runtime machine in T(n), and let P be the state which is current when it halts (in its single run on the zero tape), and (after that halt) let the tape contain b1 at the current position, and b2 to its immediate left. Note that this implies Pâ€™s rule for b1 is HALT. (There may be other HALT instructions in the machine, in P or in other states, but they never run.)

We will form a new machine Bâ€™ from B by revising P and adding one new state N.

Leave Pâ€™s rule for not(b1) unchanged, but replace Pâ€™s HALT rule for b1 with â€œwrite b1, move Left, go to Nâ€.

Define N with these rules:
b2: write not(b2), move Right, go to P;
not(b2): HALT.

Comparing the run of Bâ€™ to the run of B, they behave identically until the last step of B, when B halts but Bâ€™ doesnâ€™t. The tape at the start of that step (in either machine) looks locally like [b2] [P b1]. (That is, it has two successive cells as shown, with the second one current and the state being P; all other cells could be anything.)

Bâ€™ will perform three extra steps compared to B, with these states and actions:

(from above) [b2] [P b1] (will halt in B but not in Bâ€™)
new state 1: [N b2] [b1]
new state 2: [not(b2)] [P b1]
new state 3: [N not(b2)] [b1] (will halt in Bâ€™). QED.

==

Can we do better by using more knowledge about the tape? (We know its entire state when B halts.)

For most tape states, yes; but it might be all 0 (except for [P b1] if b1 is not 0), and in that case I canâ€™t think of any way to use the rest of it with so few new instructions.

We also donâ€™t know enough to safely have N jump to whatever state last jumped to P â€” our revised instruction in P, by writing the correct bit onto the tape, could make P safe to rerun, or make that next-older state safe to rerun, but (in general) not both at once.

BjÃ¸rn Kjos-Hanssen Says:
Comment #170 July 29th, 2020 at 2:56 am
Solovay in the 1970s showed that a function is computable from every sufficiently fast growing function (given as an oracle) iff it is hyperarithmetic. So the other functions all have more to them than just their speed, so to speak.

P.S. Google seems to have a hard time learning that I want this blog and not the Israeli TV show Shtisel!

STEM Caveman Says:
Comment #171 July 29th, 2020 at 4:16 am
@Scott 143,

Itâ€™s not just the lack of computable model. The completeness theorem is a strong form of the Axiom of Choice, and something like that must be true for the countable case, e.g. equivalence to countable Dependent Choice (combinatorially, Koenigâ€™s tree lemma or the like). This is wildly nonconstructive. The completeness theorem in such cases is a proof that you will never reach a contradiction if you pretend to talk about â€œactual objectsâ€ rather than syntax, and I guess one can get used to the talk to the point that it feels real, but the proof does not produce new nonstandard objects at the same ontological level as the old, standard ones.

So it looks to me like your minimal meta-mathematical assumptions are Dependent Choice and the existence of a definite answer to the halting of any individual TM, regardless of our ability to find that answer, coupled to some basic set-theoretic setup for talking about discrete objects. This is indeed pretty minimal by modern standards, and covers virtually all of concrete mathematics for TCS, but I donâ€™t think it is all that coherent when examined. Basically the trick is to avoid issues by leaving certain things vague or undefined. When you stop doing that the issues re-emerge and at a more finite level.

For example, your article takes it as a given that the runtimes of n-state machines that halt are meaningful (â€œobjectiveâ€ as you wrote), since they can eventually be computed. But they rapidly exhaust what is physically computable and what we actually (physically) have are upper and lower bounds produced within some theory. The length of the halting proofs, a.k.a upper bounds, has its own Busy Beaver-like growth as a function of n. Things become theory dependent and the supposed objectivity is that there is some consistency between the different theories that can be effectively used. Thatâ€™s no doubt true for very small n but there quickly appears a need for more and more sophisticated theory and more choices of theory so you get a miniaturized form of all the Goedelian complications it seemed one was avoiding by talking about things that halt in finite time.

gentzen Says:
Comment #172 July 29th, 2020 at 5:50 am
maline #164, Luke G #168

Not claiming to be an expert either, but you are basically correct that the hyperarithmetic sets are closely related to the construction outlined by maline. However, I never heard of hyperarithmetical ordinals. But it is a good question, what comes after the recursive ordinals, in terms still related to computability.

(And now I had a short parenthetical remark putting â€œpossibly relevant computability conceptsâ€ into context, but it got so long that I decided it earned its own paragraph:) If we denote recursive/computable by Î”_0^0, then Î”_1^0 would be next, which denotes limit computability, described in footnote 62 as â€œtrial and error procedure â€¦ by following a guessing procedure which may make a finite number of initial errors before it eventually converges to the correct answerâ€ (attributed to Putnam). Then comes Î”_2^0, Î”_3^0, â€¦, but that is a bit boring and not very enlightening conceptually, except maybe for the fact that we get Î” instead of Î£ or Î , because we need function classes. And then we come to Î”_0^1, which denotes arithmetic computability/definability. And next comes Î”_1^1, which denotes hyperarithmetic computability/definability. This is again an interesting concept, which has at least three totally different characterizations. Next would be Î”_2^1, I donâ€™t know a name for that, but it is the last one still covered by Shoenfieldâ€™s absoluteness theorem.

What else could/should I say about hyperarithmetic sets? (I tried to explain why I find such concepts important, but then I drifted too much into closure of function classes under composition, Weihrauch complexity (because there a subtle differences in ways to compose functions), and I was too unsure whether closure under composition is trivial or not:) Classes like Î”_0^0, Î”_1^0, Î”_0^1, and Î”_1^1 interpreted as function classes are closed under composition of functions. It is less obvious to me whether this is still true for classes like Î”_2^0, Î”_3^0 Î”_2^1, or Î”_3^1. At least for the individual classes in the logarithmic time hierarchy (AC^1), I guess it is not true, but maybe I am making a trivial mistake here. (So I better stop here, and do my homework first.)

gentzen Says:
Comment #173 July 29th, 2020 at 6:12 am
Gerald #75:

An interesting, much stronger theory would be Second Order Number Theory with Projective Determinacy (Z2+PD). Since the work of Woodin and others in the 1980s there is a growing consensus that Z2+PD is the canonical theory of second order math, i.e. the right theory of V(Ï‰+1), while PA being the canonical theory of first order number theory.

Scott #144:

â€¦, let me make this discussion concrete as follows. Can either of you propose a candidate for a mathematical statement with an indefinite truth-valueâ€”like I think is very plausibly the case for CHâ€”but whose â€œlogical complexityâ€ is lower than CHâ€™s?

Somewhere between quantification over reals and quantification over sets of reals, we seem to lose Platonicity, and it would be fascinating to zoom in on where

In response to Scottâ€™s question, I looked up the following FOM post by Dmytro Taranovsky about Z2+PD. I had not noticed that Gerald had already mentioned Z2+PD before. The relevant point here is that ZFC is compatible with Z2+PD but does not prove it, yet there is a growing concensus that Z2+PD â€œthe canonical theory of second order mathâ€ or â€œthe basic theory of real numbers and projective setsâ€.

Joshua B Zelinsky Says:
Comment #174 July 29th, 2020 at 7:35 am
@Bruce Smith #169,

That looks valid.

â€œFor most tape states, yes; but it might be all 0 (except for [P b1] if b1 is not 0), and in that case I canâ€™t think of any way to use the rest of it with so few new instructions.â€

So it seems like one reasonable subgoal should be to prove that for n>1, the Busy Beaver machine on n states never leaves a blank tape when it is finished. Iâ€™m not sure how to prove that, but maybe worth noting that if it did, then a whole bunch of moves at the end are going to be writing 0s; that should maybe restrict what the system can do, with something akin to a pumping lemma or similar sort of pigeon hole argument, since if on the last time it mostly moves in one direction from where it has a lot of 1s, it needs to be able to replace them all with 0s, and then somehow know enough to stop when it gets to the end of the line.

Scott Says:
Comment #175 July 29th, 2020 at 11:05 am
BjÃ¸rn Kjos-Hanssen #170:

Solovay in the 1970s showed that a function is computable from every sufficiently fast growing function (given as an oracle) iff it is hyperarithmetic. So the other functions all have more to them than just their speed, so to speak.
Thatâ€™s extremely interestingâ€”do you have a reference for that?? Andy Drucker just raised the question to me, in an email a few weeks ago, of characterizing the functions that are computable given an oracle for any sufficiently fast-growing function.

Scott Says:
Comment #176 July 29th, 2020 at 11:30 am
STEM Caveman #171: Your comment led me to some interesting reading on Dependent Choice, Countable Choice, KÃ¶nigâ€™s Lemma, Weak KÃ¶nigâ€™s Lemma, and the relation of all these axioms to each other and to the Completeness Theorem. Briefly, I have no problem at all with Dependent Choiceâ€”as I said before, Iâ€™m totally fine with the notion of flipping a coin a countable number of times. (Flipping it an uncountable number of times is a different matter.) All the same, if I had to give up Dependent Choice or even KÃ¶nigâ€™s Lemma, I could still do math and TCS with hardly any problem.

By contrast, I could not do math and TCS if I had to give up the idea that a Turing machine either halts or runs forever, and that one which it does is an iron fact of Platonic reality. Itâ€™s the denial of that idea that strikes me as crazy and barely even comprehensible. Iâ€™m inclined to turn the tables and ask: whatâ€™s a specific example of a Turing machine whose halting you consider to be indeterminate? (As opposed to not yet known, which surely you agree is different?) Or: you agree, donâ€™t you, that BB(1)=1, BB(2)=6, BB(3)=21, BB(4)=107, BB(5)â‰¥47,176,870, BB(6)>1036,534, BB(7)>1010^10^10^18,705,353, that these are all facts are surely as 4+3=7 is a fact? Great then, whatâ€™s the first value of n for which you think that BB(n) is indeterminate?

Gargantua Says:
Comment #177 July 29th, 2020 at 12:32 pm
Let Ï‰(n) be the supremum of all the computable ordinals whose order relations are computable by n-state Turing machines. Could we consider Turing machines equipped with an oracle for all BB_{Ï‰(n)} and define their Busy Beaver function?

BjÃ¸rn Kjos-Hanssen Says:
Comment #178 July 29th, 2020 at 12:45 pm
Scott #175:

Theorem. The following are equivalent
(i) f is hyperarithmetic
(ii) â€œf is computable from all sufficiently fast-growing functionsâ€ in the sense that there is a function g such that all h dominating g compute f.

Proof sketch:
(ii) implies (i): Apply Solovayâ€™s result in â€œHyperarithmetically encodable setsâ€, TAMS 1978 [link: https://www.ams.org/journals/tran/1978-239-00/S0002-9947-1978-0491103-7/S0002-9947-1978-0491103-7.pdf ]
to the range of g.
(i) implies (ii): Consider a characterization of hyperarithmetic sets in terms of iterated halting problems over the computable ordinals. [ ]

Iâ€™m not sure if this has been spelled out in a paper yet actually. A version for partial functions seems to be in Stephan and Yu, â€œA reducibility related to being hyperimmune-freeâ€, APAL 2014.

Scott Says:
Comment #179 July 29th, 2020 at 12:50 pm
Gargantua #177: The problem is that, in order to define BBÎ±(n), where Î± is an ordinal, you need a notation for the ordinalâ€”a way for the Turing machine to tell the oracle which level in the ordinal hierarchy itâ€™s talking about. Whenever we have a computable ordinal, defined via a Turing machine, that machine provides a ready-made notation, but if we try to go beyond the Church-Kleene ordinal, weâ€™re in danger of writing nonsense if weâ€™re not extremely careful about notations. (I know just enough about the subject to tell you that, and am trying to learn more!)

Gargantua Says:
Comment #180 July 29th, 2020 at 1:18 pm
Scott #178: But you accepted these BB_{Ï‰(n)} to be well-defined for n âˆˆ N. If we do so, then theyâ€™re just a family of integer sequences and we could define oracle TMs with access to them. These machines donâ€™t have to know how these sequences are defined.

Scott Says:
Comment #181 July 29th, 2020 at 3:07 pm
Gargantua #180: Oh, sorryâ€”I misread your earlier comment! I thought you wanted to define a single BB function based on the supremum of computable ordinals (which is called the Church-Kleene ordinal), rather than diagonalizing across all the different computable ordinals. Yes, sure, you can do the latter, and then look at Busy Beaver with an oracle for the result, and so on as often as you like.

Let me be clear: thereâ€™s no question of ever defining a â€œfastest-growing sequence.â€ You can always find a faster-growing one. But precisely for that reason, the standards are higher. A new sequence is only interesting if, in some sense, it blows all the previous sequences out of the water, rather than merely exceeding them in ways that we previously understood.

Bruce Smith Says:
Comment #182 July 29th, 2020 at 3:47 pm
Joshua Zelinsky #174:

â€œâ€¦ prove that for n > 1, the Busy Beaver machine on n states never leaves a blank tape when it is finished.â€

(Or even a tape which is blank everywhere except in the current cell.)

That is an interesting idea â€” but do you have any intuitive reason to guess it might be true? (Or at least, â€œtrue for some non-accidental reasonâ€, which I guess means â€œprovableâ€?)

I didnâ€™t fully understand your proposal for a proof outline, so maybe there is an idea implicit in that which I donâ€™t yet understand.

I do see that, when the machine is k steps from halting and in state X, there is a fraction of at least 1/2^k tape states which would cause it to halt (since it will examine at most k cells before halting).

Maybe thatâ€™s unclear â€” what I mean is, for every k (between 0 and the runtime), there is a machine state X and a â€œlocal tape stateâ€ K (a set of at most k adjacent relative cell positions, plus a k-tuple of bits to fill them), such that itâ€™s a true statement about the machineâ€™s time evolution that â€œif itâ€™s ever in machine state X and local tape state K, then it will halt in exactly k stepsâ€.

Are you suggesting that in a BB machine, for small k and the associated X and K, itâ€™s unlikely the number of ones in K is small? And beyond that, itâ€™s also unlikely that the halting process (for those last k steps) erases all those ones in K? (Where by â€œunlikelyâ€ I mean â€œif we restrict consideration to this kind of machine, we somehow force the machine to lose the competition to be a BB machineâ€.)

I donâ€™t yet see a heuristic reason to suspect that, let alone a potential proof outline. For example, I donâ€™t see why â€œzeroing a special pattern of ones at the same time as we recognize itâ€ is somehow a bad strategy for a BB machine to use.

On the other hand, the general idea of considering this sort of analysis does seem attractive.

A related more general question: is there any known consequence of a machine M â€œbeing a BB machineâ€ which would be â€œlocally observableâ€ in the sense that certain short patterns of behavior (if they occur during the run that starts from the empty tape) can be ruled out? (Since any machine that ever does that could not be one that wins the runtime competition to be a BB machine.) (Not counting â€œentering a short infinite loopâ€ â€” I mean something compatible with halting, but not compatible with having maximum runtime for its number of states.)

Gargantua Says:
Comment #183 July 29th, 2020 at 4:05 pm
I think Iâ€™ m having trouble understanding certain steps in the construction of the BB-hierarchy, starting already with B_Ï‰. How exactly is it defined?

Gargantua Says:
Comment #184 July 29th, 2020 at 4:12 pm
Typo in previous comment: B_Ï‰ should be BB_Ï‰. By the way: How can we type subscripts here?

Bruce Smith Says:
Comment #185 July 29th, 2020 at 5:16 pm
I asked just above, more or less: is there any short behavior sequence in a machine M which rules out its being a BB machine, but doesnâ€™t rule out its halting?

Here is one example: there are instruction sequences which can be â€œpessimizedâ€ (revised to do the same thing in the same number of states but more time), which therefore canâ€™t be present in a BB machine. (I guess this might be well known â€” I apologize if Iâ€™m repeating what most of you already know.)

(By an â€œinstruction sequenceâ€ I really mean a subgraph of the graph of machine states with some designated â€œentry statesâ€, so that the rest of the machine can only jump to those entry states rather than to arbitrary states within this subgraph. A â€œpessimizationâ€ is then a revision of the rules of the states in the subgraph, so that machine histories are unchanged except for what happens within periods spent within this subgraph. Those periods might take longer, but otherwise they must leave the machine in the same state (after exit from the subgraph) as without the revision.)

Here is an example that seems natural: suppose you wanted to zero out and skip over the next two tape cells, and then go to state P. To do this, you write an instruction sequence with one entry point X1 and one internal state X2, coded as:

ORIG:

X1: 0: 0 R X2, 1: 0 R X2
X2: 0: 0 R P, 1: 0 R P

But you could â€œpessimizeâ€ this by instead writing:

PESS1:

X1: 0: 0 R X2, 1: 0 R X2
X2: 0: 0 R P, 1: 0 L X1

This is a â€œweak pessimizationâ€, since it only takes longer for some tape states, and takes the same time for others. But you *alternatively* could have pessimized it as:

PESS2:

X1: 0: 0 R X2, 1: 0 R X2
X2: 0: 1 L X1, 1: 0 R P

This is also a weak pessimization, but its weakness occurs on a disjoint set of tape states â€” every tape state is slowed down by one or the other of these pessimizations.

Therefore, no BB machine can contain ORIG!

Proof: any machine M (running on the blank tape) either never runs those instructions, or it runs them on tape states (I mean just the local part of the tape, seen by these two instructions) of the form x1, and/or it runs them on tape states of the form x0; in each case there is a way to either remove machine states (with unchanged runtime) or to strongly pessimize the machine. Therefore M is not a BB machine. QED.

==

To connect this to a prior topic â€” if you could rule out certain instruction sequences as coming just before halting in a BB machine, you might prove the remaining possibilities either leave the tape non-blank or have states that can be reused by a new state added to modify halting behavior (in the same way, in my proof of Theorem X3, the new state N reuses state P, but going farther back in time).

(However, Iâ€™m pretty skeptical this scheme could work well and fully enough to actually improve Theorem X3. I spent awhile considering cases about the prior one or two instructions before P, in that theoremâ€™s proof, and found some cases that seemingly leave no room for this kind of thing to work. On the other hand, I hadnâ€™t yet realized â€œpessimizable code can be ruled outâ€, so I might have missed something. But on the third hand, in the general case where every state might be an entry point, I donâ€™t know if pessimizable code even exists.)

Bruce Smith Says:
Comment #186 July 29th, 2020 at 5:55 pm
FYI, this sequence is pessimizable even if both its states can be entry points:

X1: move right, go to X2
X2: write 0, move right, go to P

In the pessimized forms, X2 might go to X1, which is safe even if X1 didnâ€™t just run.

Bruce Smith Says:
Comment #187 July 29th, 2020 at 10:01 pm
Joshua Zelinsky #174: I should clarify my comment that sparked our discussion about whether a BB machine can terminate with an almost-blank tape, since Iâ€™m realizing it might have been unclear in a way that makes that question seem more significant for improving Theorem X3 than it is. (Though of course, that question also has intrinsic interest.)

Whatâ€™s true: for a reasonably large fraction of tape states S, if you knew machine M halted (on the initial blank tape) with tape state S, you could use a slight variant of the proof of Theorem X2 to improve that theoremâ€™s conclusion, or even (for a smaller fraction of tape states) Theorem X3â€™s conclusion.

What might be true, but Iâ€™m not sure: just for improving Theorem X2, *every* tape state S is like that, except the ones which are blank everywhere except perhaps under the halting step with state P. (I just mentioned the blank tape as an easy-to-describe counterexample to the idea that you could *always* do this â€” not intending to imply that it was the only counterexample. Also, though I mentioned it after Theorem X3, my own feeling of its significance may have been partly left over from my earlier attempts to improve Theorem X2. And I then forgot some of this by the time I read your idea about proving the post-halting tape was non-blank, since I got interested in that idea for its own sake.)

It might be literally true that those states are the only exceptions relevant to this way of improving Theorem X2 (and it would only take a few minutes to check), but since X2 is already superceded by X3, this no longer seems important. For X3, itâ€™s much harder to improve it this way â€” Iâ€™m nearly certain there are almost-but-not-quite-so-blank tape states which would fail to improve it in any way I know.

(This potential improvement, relative to the proof of X2, is just to get state N to be a one-state loop which stops a little farther along, optimizing all details of which side of P it starts on, which bit value it stops at, which direction it goes, and what P writes under itself before going to N. For an arbitrary non-blank tape state, the worst case is probably a 1 on one side of P and blank everywhere else. In that case N can first run when itâ€™s over that 1, then skip it and the P cell and stop on the 0 (on the other side of the P cell), thus equalling but not exceeding the performance of X3. Even if you proved the tape had lots of ones elsewhere, if it happened to surround the P cell like 01P01, those farther away ones would not help improve this result.)

gentzen Says:
Comment #188 July 30th, 2020 at 6:26 am
gentzen #172: â€¦ I tried to do some homework now â€¦

Classes like Î”_0^0, Î”_1^0, Î”_0^1, and Î”_1^1 interpreted as function classes are closed under composition of functions. It is less obvious to me whether this is still true for classes like Î”_2^0, Î”_3^0, Î”_2^1, or Î”_3^1. At least for the individual classes in the logarithmic time hierarchy (AC^0), I guess it is not true, but maybe I am making a trivial mistake here. (So I better stop here, and do my homework first.)

The non-closure under composition of functions seems to be an artifact of the logarithmic time hierarchy (AC^0, an individual class in this hierarchy would be circuits of depth k, minus some details). Using computations in the limits with oracles, it it pretty obvious (to me now) that Î”_2^0 and Î”_3^0 are closed under composition of functions. And if I had some basic understanding of Î”_2^1 and Î”_3^1 (like I have for Î”_1^1), then it would probably be pretty obvious to me that they are closed under composition of functions.

gentzen Says:
Comment #189 July 30th, 2020 at 7:39 am
STEM Caveman #171:

Itâ€™s not just the lack of computable model. The completeness theorem is a strong form of the Axiom of Choice, and something like that must be true for the countable case, e.g. equivalence to countable Dependent Choice (combinatorially, Koenigâ€™s tree lemma or the like). This is wildly nonconstructive.

The way the completeness theorem is presented is wildly nonconstructive, because text-books are eager to present it for arbitrary sets of axioms. Maybe some are modest enough to restrict themselves to arbitrary countable sets of axioms, but you will have a hard time finding a text-book which limits itself to computable sets of axioms.

For non-standard models of PA, we do have a computable sets of axioms, and therefore we can construct reasonably concrete models in that case. (However, your exchange with Scott makes it clear to me that you wonâ€™t even be happy with those models.)

The concrete models consists of symbol strings as objects, functions (like S(x), x+y, and x*y for PA) are represented as (extremely simple) computable functions from strings to strings, predicates (like x &lt y in case of PA) are represented as limit computable yes/no functions (of strings), and the equality relation (between to symbol string that represent the objects) is a limit computable yes/no function (of strings) too.

The completeness theorem in such cases is a proof that you will never reach a contradiction if you pretend to talk about â€œactual objectsâ€ rather than syntax, and I guess one can get used to the talk to the point that it feels real, but the proof does not produce new nonstandard objects at the same ontological level as the old, standard ones.

Well, for a long time, I was unsure too whether the completeness theorem would fall into that category. My main reasons against believing this were that Kurt GÃ¶del and Paul Bernays are much more careful with respect to ontological commitments than typical text-books, and since both of them believed that the model existence theorem implied â€œreal existenceâ€, then it is probably just the fault of modern text-books that I failed to see how it could imply it.

In the linear algebra course in the first semester (a very long time ago), the professor proved that every vector space has a (Hammel) basis. Just like you wrote above, I accepted that this proof meant that I would never be able to prove otherwise by finding a contradiction, but I still thought that this was a sort of modeling error. The existence implied by that theorem was simply not the type of existence I expected from mathematical idealization of reality. I am more than willing to accept existence of some abstract objects that can never be constructed exactly, but only approximated better and better, such that the abstract objects act as an idealization of the actual objects. (And that is why limit computability is OK for me, because it approximates some idealized answer better and better in a suitable sense.)

So it looks to me like your minimal meta-mathematical assumptions are Dependent Choice and the existence of a definite answer to the halting of any individual TM, regardless of our ability to find that answer, coupled to some basic set-theoretic setup for talking about discrete objects.

The concrete model I mentioned above does depend on a definite answer to the halting of any individual TM, and even more depends on the definite last revision of an answer that may be revised a finite number of times by a TM which may run forever. However, neither Dependent Choice nor basic set-theoretic setup are needed.

Scott #176:

Your comment led me to some interesting reading on Dependent Choice, Countable Choice, KÃ¶nigâ€™s Lemma, Weak KÃ¶nigâ€™s Lemma, and the relation of all these axioms to each other and to the Completeness Theorem.

The interesting thing is that using Weak KÃ¶nigâ€™s Lemma is both weaker and stronger than the concrete construction above, in a certain sense. It is weaker, because it is just a â€œde dictoâ€ construction (there is no explicitly defined object associated with that construction), which has weaker ontological implications than accepting limit computation. The concrete constructon above is a â€œde reâ€ construction, because it defined an explicit well defined object, therefore it is stronger. But it is also weaker, because it does not need to work with arbitrary sets, but just with computable sets.

Joshua B Zelinsky Says:
Comment #190 July 30th, 2020 at 8:27 am
@Bruce Smith #187,

It does seem to me that the all blank tape is the only barrier for the following reason: If you have a string of 0s followed by a 1, from where you halted (without loss of generality say it looks like 01â€¦.1[] ) where [] is the tape location one is currently in, and there are k 1s, then one can use the new state to loop moving the tape until one hits the 0 and halt at 0. Does that not work?

Eric Cordian Says:
Comment #191 July 30th, 2020 at 10:00 am
Hi Scott,

As a supporter of Israel, but not necessarily of the policies of the Netanyahu government, I was wondering what you thought of Seth Roganâ€™s recent comments that Israelâ€™s existence â€œDoesnâ€™t make sense,â€ and that as a Jewish person, he was â€œfed a huge amount of lies about Israelâ€ his entire life.

https://www.rt.com/usa/496485-seth-rogen-israel-lies/

While Iâ€™d like to see peace in the region, and justice for the Palestinians, I think the existence of Israel serves a useful purpose. If Jewish people ever again find themselves in a situation where the country they live in is planning on murdering them, and no other country will give them a visa, they can go to Israel where they will be welcomed, and helped to build new lives.

While being displaced from ones home country is obviously sub-optimal, I think we can all agree it is vastly superior to being murdered.

Scott Says:
Comment #192 July 30th, 2020 at 11:25 am
Eric Cordian #191: I agree with what you wrote. Iâ€™m not, to put it extremely mildly, a fan of the Netanyahu government, or how they torched the peace process while making Israel every day less of a liberal democracy and more of a settler-theocracy. But just like my contempt for Trump doesnâ€™t mean I want to see the US destroyed and its current inhabitants murdered or exiled, so my contempt for Netanyahu doesnâ€™t mean I want the same for Israel. And while I loved Seth Rogenâ€™s North Korea movie, I think heâ€™s incredibly naÃ¯ve if he doesnâ€™t realize that thatâ€™s what â€œthe end of Israelâ€ would mean. Israel is no longer a theoretical proposal for â€œwhat to do with the Jews,â€ but an actual country with millions of inhabitants, many of whom happen to be my friends and family, and with universities and high-tech industry and all the rest. â€œEndingâ€ Israel now is about as feasible as â€œendingâ€ Canada or Singapore or the US, and as horrifying if you think through its implicationsâ€”especially given the stated intentions of some of Israelâ€™s neighbors to complete the Holocaust if and when they become able.

And now, letâ€™s please get back to Busy Beaver! ğŸ™‚

maline Says:
Comment #193 July 30th, 2020 at 2:38 pm
Luke G #168, Gentzen #172, and Scott #179: The hyperarithmetical sets are indeed â€œwell-definedâ€ according to my assumptions in #164. Their recursive construction is almost identical to what I want, except for a detail I mentioned: in that construction, the ordinal notations must be computable. There must be a way for a Turing machine to â€œfindâ€ the correct sub-oracle to query, starting with a natural-number â€œaddressâ€ and without supernatural help.

I want to loosen this, and allow oracles to be used in the ordinal notation system. For a given limit ordinal a, the corresponding oracle should be considered well-defined if it first interprets the address number using some fixed oracle machine of order <a, and then sends the query to the correct oracle, which may be of any order <a.

According to [https://en.wikipedia.org/wiki/Hyperarithmetical_theory/Relativized hyperarithmeticity and hyperdegrees], if I understand correctly, the supremum of the ordinals that can be given notations using an oracle X is written \omega_1^X and is larger than the Church-Kleene ordinal. Now suppose we define a function on the ordinals
f(a)=sup{\omega_1^X(b)}
where the supremum is over ordinals ba, there is some ordinal ba, meaning that oracle X(b) can be used to interpret a notation for a. So it seems to me that the cutoff for my suggested â€œwell-definednessâ€ will be at the lowest fixed point of this function f.

Can anyone tell me whether this makes sense, whether there is actually a countable fixed point, and whether that cutoff ordinal has a name? Scott, do you agree that up to this point, all of the questions are still â€œessentially finitaryâ€ and so have Platonic answers?

maline Says:
Comment #194 July 30th, 2020 at 2:48 pm
Where it says â€œbaâ€ in my previous comment it should be â€œb smaller than aâ€.

Can someone give me a reference for how Latex, links, and so on work here?

Toby Ord Says:
Comment #195 July 30th, 2020 at 4:00 pm
Hi Scott,

I am so excited to see this paper. When I left Melbourne in 2003, I was working on questions concerning generalised busy beaver functions for models of computation that go beyond Turing machines. I packed it all away in a shoebox, taped it shut, and took it to Oxford with me. 17 years later, I believe it is still unopened, somewhere in my attic. Iâ€™ve now read your paper and this thread, and think I have a few things to add from back in the day, as well as a few observations inspired by this recent progress.

First, The paper mentions rates of growth between computable rates and busy beaver rates. Iâ€™d noticed this too. My thinking was to use the ideas behind the answers to Postâ€™s problem (of finding an r.e. set that is neither recursive nor equivalent to the halting set). Since the Turing degrees were eventually shown to be dense between these levels, there should be corresponding quasi busy beaver functions that are also dense between computable functions and BB().

Since there are also non-comparable r.e. Turing degrees, there should also be rapid growing functions that grow faster than all computable functions, that canâ€™t be used to computer BB() and where neither one can be used to compute the other. The only way this can happen is if neither surpasses the other, but they keep swapping the lead. I canâ€™t remember if I proved any of this more formally, or if it was all still conjecture that the analogies play out from Turing degrees to quasi busy beaver functions. If the analogies do hold, then you could also look through the properties of the Turing degrees (which are, frankly, a mess) and port things over to the quasi busy beaver functions.

I also noticed the idea of using oracles to get faster rates of growth all the way up the arithmetical hierarchy, and of using oracles defined in terms of other busy beaver functions to provide an interesting alternative to halting sets. But I didnâ€™t know enough about how computable ordinals actually work to think of things like the BB_{\omega(n)}(n) of comment #96.

Toby Ord Says:
Comment #196 July 30th, 2020 at 4:10 pm
It seems like we could do with some nice notation for a function growing not just faster but uncomputably faster than another. It is a bit tricky to pin down, as BB(n)^2 grows faster BB(n) by an uncomputably growing amount (and ratio), but we wouldnâ€™t want it to count for these purposes. I think the key is that we should let the monotonic function by which the slower function is allowed to be boosted be relative to an oracle for that function.

Turing degrees use notation of =, etc with subscripted T. How about using the curly versions of these (~, &pr;, &sc; etc) with the subscript capital T? Then, e.g.

f &sc;_T g could mean: For all functions w(.) computable by an o-machine with oracle g, there is still a natural number k such that for all n>k, f(n) > w(g(n)).

Toby Ord Says:
Comment #197 July 30th, 2020 at 4:14 pm
Looking at the spectrum of run times is a great idea.

Conjecture: the ratio between the BB and the second place beaver grows incomputably quickly.

Indeed, we can almost show that something like this is true in wide generality, since any way of reliably bounding (even probabilistically) the true value of BB(n) given initial segments of the spectrum would lead to a contradiction. This doesnâ€™t quite resolve the second-place-beaver conjecture, but might be a useful start and should produce many similar results.

Scott Says:
Comment #198 July 30th, 2020 at 5:43 pm
Toby Ord #195-197: Thanks so much for the kind words and comments!

Yes, of course my functions with intermediate growth between computable and BB were inspired by the intermediate r.e. Turing degrees, although itâ€™s not obvious to me how to derive either of those results from the other.

You write:

there should also be rapid growing functions that grow faster than all computable functions, that canâ€™t be used to computer BB() and where neither one can be used to compute the other. The only way this can happen is if neither surpasses the other, but they keep swapping the lead.
I had the same thought, but one needs to be careful, since we could also get what you asked for by (e.g.) taking my function g with intermediate growth, and then twiddling its oddness or evenness to encode two different Turing-incomparable languages. What you really meant, I assume, is two growth rates h1 and h2 such that no upper bound on h1 is computable given h2 and vice versa. I agree that this ought to be possible as well, via an h1 and h2 that swap the lead infinitely many times. But in such a pathological case, Iâ€™d be tempted to just define h1 and h2 to have â€œthe same growth rateâ€ and be done with it! ğŸ™‚

BB(n)2 is small potatoes. For me, â€œnoticeably faster than BBâ€ means, at the least, faster than anything computable given a BB oracle. ğŸ™‚

I confess that I donâ€™t have an intuition about whether BB(n) should be a computable function of the second-longest runtimeâ€”although notice that BB(n-1) is upper-bounded by, and hence easy to compute given, the second-longest n-state runtime. Would you mind if I put this question in the paper and acknowledged you?

Scott Says:
Comment #199 July 30th, 2020 at 6:03 pm
Just to have something to link to, Iâ€™m now going to de-rot13 the first part of Bruce Smithâ€™s comment #157:

Theorem X1 (suspected obvious): BB(n+2) â‰¥ BB(n)(1 + 2/n).

Proof: Let B be a maximal-runtime machine in T(n), and let P be the state of B which is most often the current state as any step of B starts running. (P may or may not be the initial state.)

Note that P is the current state at the start of at least BB(n)/n steps.

We will make a new machine Bâ€™ by modifying B, adding two new states and increasing its runtime.

To do this, start with B, then replace its state P with a three-state sequence, N1 N2 P, where the sequence N1 N2 â€œdoes nothingâ€ and P acts as before. (If P was the initial state, N1 is now the initial state; otherwise the initial state is unchanged.)

More precisely, every existing â€œtransition to Pâ€ (some subset of the 2n transition rules in B) is replaced by an otherwise-identical transition to N1. Define N1 to not alter the tape (i.e. write the same bit itâ€™s reading), move left, and go to N2. Define N2 to not alter the tape, move right, and go to P. Define P as before (except that if one of its existing transitions went to P, it now goes to N1 due to the modification already described).

Comparing a run of B and a run of Bâ€™, every step starting not at P behaves as before. Every step starting at P becomes a series of three steps starting at N1, N2, and P, respectively. The N1 step moves left, the N2 step moves right (neither one alters the tape), then the P step starts in an identical state as it did in B, so it behaves identically.

Since we chose P to run at least BB(n)/n times, each of N1 and N2 also runs that many times. The other states run the same number of times as before. QED.

(If transitions were allowed to â€œmove neither L nor Râ€, we could add only one new state, and prove BB(n+1) â‰¥ BB(n)(1 + 1/n).)

Scottâ€™s note: the independent earlier discovery of this little result is here.

Raoul Ohio Says:
Comment #200 July 30th, 2020 at 6:04 pm
I donâ€™t know much about BB but I am a great grand student of Radoâ€™s.

Sniffnoy Says:
Comment #201 July 30th, 2020 at 7:02 pm
maline #194:

Itâ€™s just (limited) HTML. So if you want to write â€œb<aâ€, you would write â€œb&lt;aâ€. Similarly links are done the HTML way, etc. I donâ€™t believe thereâ€™s any way to embed LaTeX.

Unfortunately, the <sup> and <sub> tags donâ€™t appear to work at the momentâ€¦ or rather, they only work for Scott! Superscripts and subscripts are reserved to him alone. ğŸ˜› (But heâ€™s trying to fix that, I think?)

gentzen Says:
Comment #202 July 30th, 2020 at 7:41 pm
maline #193:

But we can do even more: why should we be limited to recursive schemes, when we have this incredible tower of oracles at our command? All we should require is that, for each particular limit ordinal, there must be an indexing scheme that can be decoded by a Turing machine equipped with some specific earlier oracle. So how far up can we actually go?

I believe that Luke G tried to argue that you still cannot go beyond the Church-Kleene ordinal (=hyperarithmetical), when he wrote (the same claim also occurs on wikipedia):

In particular, the Church-Kleene ordinal is the both limit of recursive ordinals, and the limit of hyperarithmetical ordinals.

So all the oracles up to that point wonâ€™t help you to define bigger ordinals. On the other hand, nothing seems to prevent you to restart the game at that point, and use the fixed oracle for the hyperarithmetical sets.

I want to loosen this, and allow oracles to be used in the ordinal notation system.

OK, but how do you want to get suitable well-defined oracles beyond the hyperarithmetic sets, given the barrier indicated by Luke G? And where do you want to go? Î”_2^1? Î”_0^2?

Scott Says:
Comment #203 July 30th, 2020 at 7:47 pm
Sniffnoy #201: I donâ€™t know how to fix it!

Would someone like $200 to be temporary webmaster of this blog and fix all the technical stuff thatâ€™s wrong with it?

Bruce Smith Says:
Comment #204 July 30th, 2020 at 11:12 pm
Joshua Zelinsky #190:

That works, but how well it works depends on how many 1s there are. So in the likely-worst case of 01[]01, or the worse-in-a-different way 01[]0â€¦ (0s forever), you can start on the 1 or 0 in the 1[]0 part, and move either way as you loop, and place any bit inside [] before you start, and code it to skip over either 1s or 0s as it loops â€” but you still canâ€™t make it last more than 3 steps before it has stopped by hitting the other kind of bit than it started on.

(In fact, the only way to make it last even that long is to place 1 inside [], and start on 1 in 1[]0, and move right, and skip over 1s. Then it skips over two 1s and stops on the 0 in 1[]0.)

Bruce Smith Says:
Comment #205 July 30th, 2020 at 11:16 pm
Toby Ord #197, and fyi Scott #198:

If you just see an initial segment of the spectrum, you wonâ€™t thereby have any idea â€œhow initial it isâ€, so even if it contains the second place beaver, you wonâ€™t know that.

OTOH if someone promises you m (a runtime) is the second place beaver for n, you can dovetail over all n-machines until you find the next one that halts after time m, and then you know BB(n).

(Iâ€™m not sure how or whether either of these observations impact your conjecture. I guess by â€œuncomputably quicklyâ€ you mean given n, not given m.)

Scott Says:
Comment #206 July 31st, 2020 at 12:05 am
Bruce #205: Duhhhh, yes, thanks! ğŸ˜€

I guess the precise version of the question should be: does there exist a total, computable function f(n,m), such that whenever m is the second-place running-time for n-state Turing machines, we have f(n,m) = BB(n)? Here the requirement of totality means that the program for f still needs to halt, even if someone lied and set m:=BB(n) (for example).

Bruce Smith Says:
Comment #207 July 31st, 2020 at 1:06 am
Scott #206: Thatâ€™s an interesting question, but Iâ€™m not sure itâ€™s equivalent to how I interpreted Toby Ordâ€™s conjecture. I think what I thought he meant was: there is no computable g(n) such that BB(n) / runtime(second place beaver for n) is bounded above by g(n). (Leaving â€œinfinitely oftenâ€ vs â€œeventually alwaysâ€ unspecified, for now.)

I see that if his conjecture was false, you could use g to implement the f in yours. (It would only search up to runtimes of m times g(n).) But Iâ€™m missing the other direction of implication, if itâ€™s there. (I guess Iâ€™m too tired just now to properly look for it.) Well, I do see that yours is intended to be more general, anyway â€” it replaces â€œratioâ€ with â€œcomputable relationâ€, I guess. In that case I am more skeptical of yours than his, though perhaps only slightly more. (But I guess you did not conjecture â€œf doesnâ€™t existâ€ yourself, just stated that form of the question.)

Bruce Smith Says:
Comment #208 July 31st, 2020 at 1:20 am
Actually I want to go on record as conjecturing the second place beaver will not be *too* much different in runtime than the winner. Since the winner has pessimized itself as much as possible, all some variant has to do is use the same algorithm but not be quite so clever at being slow. For example, if there is any â€œsetup phaseâ€ at the beginning, never repeated, then the winner will artificially do it as slowly as possible in some small number of states, and the variant only has to â€œdo it straightforwardly insteadâ€ (in same number of states) to have a runtime differing only by a tiny additive constant.

So for any of these conjectures to hold, a BB program has to be necessarily so convoluted that there is never any distinguishable â€œsetup codeâ€, or really any other code with an â€œabstractly specifiable purposeâ€ which runs in a short time, many times during the entire history. Otherwise the variant can just accomplish that same purpose slightly faster and violate these conjectures. And this strikes me as unlikely to always hold, even if it often holds.

Bruce Smith Says:
Comment #209 July 31st, 2020 at 1:28 am
Addendum: Iâ€™m forgetting again the â€œalwaysâ€ vs â€œsometimesâ€ issue. My view is compatible with â€œsometimes the 2nd place is pretty close, other times itâ€™s far away, depending on the particular winner for this nâ€. And my interpretation of Tobyâ€™s conjecture is also compatible with that. (I better stop posting for tonight!)

Filip Says:
Comment #210 July 31st, 2020 at 1:33 am
Scott #203: I can do it, are sub/sup tags and LaTeX needed or is there anything more?

Bruce Smith Says:
Comment #211 July 31st, 2020 at 2:38 am
Addendum again: ah, now I see why your f version with â€œcomputable relationâ€ is equivalent to Tobyâ€™s g version with â€œcomputable ratioâ€ â€” those are the same thing! (You can compute any relation and then express it as a ratio.) I think this means I misinterpreted his conjecture in the first place as more like a â€œslowly growing ratioâ€.

Toby Ord Says:
Comment #212 July 31st, 2020 at 3:09 am
Bruce #205, Thatâ€™s a nice idea. Indeed it generalises quite far. If you give me the third highest run time for n states (and I really trust you that it is!), I can still calculate BB(n). I could also do it from the median runtime, or the first centile runtime (though there are technical challenges if the quantiles donâ€™t divide cleanly into the total number of halting machines).

Your way of clarifying my point with a ratio is at least roughly what I was trying to say, and Iâ€™m interested to see that you are suggesting it is not only false, but that the ratio between BB(n)/runtime(second place beaver for n) might converge to 1. Thatâ€™s impressively bold.

Toby Ord Says:
Comment #213 July 31st, 2020 at 6:34 am
Scott #198,

You are right that those clarifications are needed for the idea of two incomparable rates of quasi busy beaver growth. I wouldnâ€™t be so quick to call them â€˜the same growth rateâ€™ though, as (i) it will probably lead to an inconsistency, since relations of the form â€˜incomparable withâ€™ are not transitive but â€˜equal toâ€™ are, and (ii) it would anyway deprive you of an interesting object of study.

What else can we say about this relationship between the rates of growth? They must swap the lead infinitely many times, of course, and it must be extremely difficult to pin down where that occurs. If a Turing machine could determine any infinite sequence of natural numbers where we knew the first one was in the lead (no matter how sparse) then it could modify the first function to always be in the lead. So such sets are not recursive (and not recursive given these functions as oracles), but if they canâ€™t be too hard to compute, as a Turing machine with the actual busy beaver function could compute both quasi beaver growth functions and compare them. In contrast, if the incomparability occurs for two rates of growth that are strictly beyond the busy beaver function, then determining such a sequence of points where one is ahead must be even harder.

Or, if we take make a new function from the max of each of the two of these at each n, then we presumably get a function whose growth rate is related to the join of these two Turing degrees.

Is it possible to â€˜split upâ€™ the power of growth of BB into a finite set of functions, none of which grows as fast, but if you had any upper bounds to all of the functions in the set, you could compute BB?

There are no-doubt deeper questions one can ask too. I think there is something here, even if it ends up just recasting results results about Turing degrees.

Joshua B Zelinsky Says:
Comment #214 July 31st, 2020 at 7:06 am
@ Bruce Smith #204,

Ah yes, you are correct. So one would need a stronger claim there then just that little piece of tape.

Also, minor regarding the second place BB function. Since youâ€™ve constructed machines with n+1 states which halt with more than BB(n) +1steps, it follows that the second place machine on n will take at at least BB(n-1) steps.

Zirui Wang Says:
Comment #215 July 31st, 2020 at 10:05 am
You may want to add a link to the 27-state TM for Goldbach. I think I have a fair chance deciding whether such a small thing halts.

Toby Ord Says:
Comment #216 July 31st, 2020 at 10:37 am
A thought on the robustness of the BB function as an oracle for the halting problem.

The main robustness focus (quite rightly) is on the intriguing property that any upper bound to the BB function also works as an oracle for the halting problem. So it has a kind of upwards robustness to noise that isnâ€™t remotely possessed by an oracle like the set of natural numbers for the codes for machines that halt (or equivalently) the real number whose binary expansion that is. This robustness is not surprising for the study of fast growing functions, but is surprising for oracles or the plausibility of finding and using oracles in the physical world.

But also there is also a robustness stemming from the vast amount of redundant information, from the fact that any BB(n) lets you calculate all BB(i), where i less than n. One way to think about that is that if you were lucky enough to find a BB oracle, then even if all the numbers got increased by some arbitrary amount and then arbitrarily many of them got set to zero in an arbitrary pattern, so long as any infinite set of them were left, you would still have an oracle for the halting problem (as you can backfill each gap). So the infinite amount of bits of information they store is encoded in every infinite subsequence. i.e. for all n: K(BB(n-1)|BB(n)) = 0, for all infinite sequences: K(sequence) = infinity and K(sequence1|sequence2) = 0

This is pretty obvious mathematically, but an interesting property none-the-less, and might lead somewhereâ€¦

maline Says:
Comment #217 July 31st, 2020 at 10:43 am
Gentzen #202: Oh, I didnâ€™t understand that point. You are saying that \omega_1^X [the supremum of the ordinals that can be given a notation using an oracle X] will always just be the Church-Kleene ordinal, unless X itself uses a non-calculable ordinal?

That seems very surprising to me: It means that hyperarithmetic oracles are completely unhelpful in defining ordinal notations! Any ordinal that can be indexed using such an oracle has a calculable notation as well. What is the intuition for this?

Anyway, if this is true then the Church-Kleene ordinal is indeed the cutoff for my scheme. Helping myself to a Church-Kleene level oracle would be obviously illegal. So my claim is that the hyperarithmetic sets, and corresponding reals, are those that â€œundeniablyâ€ have Platonic existence.

And now for Scottâ€™s challenge: is there any particular real (or set of integers) that is known not to be hyperarithmetic, but that we â€œwant to believeâ€ should truly exist?

Jon Awbrey Says:
Comment #218 July 31st, 2020 at 1:56 pm
Dear Scott,

This discussion inspired me to go back and look at some of the work I did in the late 80s when I was trying to understand Cookâ€™s Theorem.  One of the programs I wrote to explore the integration of sequential learning and propositional reasoning had a propositional calculus module based on C.S. Peirceâ€™s logical graphs, so I used that syntax to write out the clauses for finite approximations to turing machines, taking the 4-state parity machine from one of Wilfâ€™s books as an object example.  It was 1989 and all I had was a 289 PC with 600K heap, but I did manage to emulate a parity machine capable of 1 bit of computation.  Hereâ€™s a link to an exposition of that.

ğŸ™ Differential Analytic Turing Automata â€¢ Overview

It may be quicker to skip to Part 2 and refer to Part 1 only as needed.

Iâ€™ll try doing BB(2) when I next get a chance. I always learned a lot just from looking at the propositional form.

Bruce Smith Says:
Comment #219 July 31st, 2020 at 2:55 pm
Toby #212: â€œâ€¦ you are suggesting it is not only false, but that the ratio between BB(n)/runtime(second place beaver for n) might converge to 1. Thatâ€™s impressively bold.â€

Either bold or reckless! But all I meant to suggest, at the strongest, was that it might converge to a constant, or at least not grow without bound. And at the weakest, all Iâ€™m really comfortable conjecturing is that it might become small (ie lower than some constant) infinitely often â€” that is, it might not *uniformly* grow without bound (if Iâ€™m using that term correctly).

(For better or worse, I also conjecture that weâ€™ll never know the answer!)

Gerald Says:
Comment #220 July 31st, 2020 at 3:02 pm
STEM Caveman #171.
I think that the proof of the completeness theorem for a countable set of nonlogical symbols goes through in ZF, no choice at all needed (more generally if you can wellorder the set of nonlogical symbols). Typically a term-model is constructed and at some point we need to â€˜completeâ€™ a set of formulas by transfinite induction. However, given a wellorder on the set of nonlogical symbols, everthing can be canonically wellordered here, so no choice at all should be needed.
Or am I missing something?

So, if you start with an uncountable set of nonlogical symbols that you cannot wellorder, you get into trouble later. Thatâ€™s not too surprising, but the source of the problem is not the completeness theorem. Of course not needing AC alone does not mean that the proof is constructive.

Bruce Smith Says:
Comment #221 July 31st, 2020 at 3:20 pm
Joshua #214,

â€œâ€¦ one would need a stronger claim there [than just about] that little piece of tape.â€ I think itâ€™s more accurate to say, a stronger claim *about* that little piece of tape. But any claim that would help, strikes me as implausible. (And so does *proving* the original conjecture about â€œthe halted tape will never be blankâ€, as opposed to just its truth.)

BTW, I noticed that all the BB machines (or candidates) listed in the paper never encounter a 0 without immediately turning it into a 1! Whereas when they hit a 1, about half the states turn it into each of 1 or 0. So those machines do seem to â€œfavor 1â€, and itâ€™s indeed *plausible* that all BBs leave lots of 1s at the end. All Iâ€™m saying I donâ€™t yet find plausible is that we could *prove* that! But itâ€™s a truly interesting question, and Iâ€™d still like to hear any ideas about it, even if they have no bearing on improving Theorem X3 or anything of that kind.

â€œâ€¦ it follows that the second place machine on n will take at least BB(n-1) steps.â€

Actually that was already known just from Proposition 1 (as I assume Scott implied at the end of #198). Thatâ€™s because any M in T(n) is also trivially in T(n+1) just by adding an unreachable state at the end, so the runtime spectrum for n is contained in that for n+1. (I think some other comment, or the paper, made that last statement explicitly.)

But itâ€™s true that Theorems X2/X3 improve this, by forcing each point in that spectrum to expand upwards slightly more. (So incidentally, their proofs (and Proposition 1â€™s), taken all together, also work to force LB (Lazy Beaver) to grow in the same manner, since those proofs used transforms that work on all machines â€” they never assumed the starting machine was a BB machine.)

So I think your comment could have been â€œbecause of Theorem X3, the second place n-machine will take at least BB(n-1) + 2 stepsâ€.

Bruce Smith Says:
Comment #222 July 31st, 2020 at 3:35 pm
Scott, I presume this is well known: Busy Beavers canâ€™t be very compressible, since otherwise a smaller machine could hardcode and unpack their description and then simulate it â€” a contradiction.

I have a few comments/questions about its implications:

â€“ am I right that Lemma 15â€™s error term severely limits how sharp this statement can be?

â€“ I think this provides yet another proof of Proposition 4, since proving â€œM is a BB machineâ€ is proving â€œMâ€™s description has high K-complexityâ€, but any proof of a constant having arbitrarily high K-complexity is not possible (with the threshold depending only on the proof theory and the universal language used to define K-complexity).

â€“ (A technical point, more or less implicit in your paper: K(BB(n)) must equal K(M) (up to a constant) for M the first busy beaver of n states, since given either one we can get the other. Therefore K(BB(n)) lower bounds the K-complexity of any busy beaver of n states (up to a constant). Is this correct as stated here?)

â€“ I am wondering whether/how this relates to your paperâ€™s Theorems 18 and 19, and especially to your speculation that K(BB(n+1)|BB(n)) might be as low as a constant. If so, â€œnaively integrating that over nâ€, weâ€™d get K(BB(n)) â‰¤ O(n), which *looks* lower than the apparent lower bound on K(BB(n)) which weâ€™re discussing â€” but Iâ€™m not sure it really is, due to this Lemma 15 error term issue. So I canâ€™t tell whether thatâ€™s a contradiction, or not, or even whether Iâ€™m just misunderstanding how relative K-complexity works, regarding â€œintegrating itâ€ like that.

Bruce Smith Says:
Comment #223 July 31st, 2020 at 3:53 pm
Scott, about Conjecture 21 (BBs above n = 2 are essentially unique):

â€“ I think someone earlier in this comment thread pointed out the left-right symmetry in BBs. I think you ought to mention this in your definition of â€œessentially uniqueâ€, since otherwise some of your claims about it seem wrong as stated (since to get the same behavior you must sometimes flip the input). One way is to relativize everything to whatever direction the instruction in A0 moves in (or if itâ€™s Halt, then the instruction in A1).

â€“ If the current candidate for n=7 is the winner, then Conjecture 21 is false (easy exercise). Personally, Iâ€™d consider this evidence against that candidate, rather than against the conjecture! However, my feeling about the conjecture is that itâ€™s at best â€œtrue by accidentâ€. I can easily imagine it being false, not by accident, if there are two ways of coding the same algorithm, which differ only in exactly how they mess up when encountering an unexpected primordial 1. Or even if there is only one way to code that algorithm, but it has an unused single instruction (like in the n=7 candidate). (By â€œinstructionâ€ I mean that thing that each state has two of.) Conjecturing this never happens is sort of like conjecturing â€œthe number of instructions needed is always evenâ€! Not exactly, since it implies thereâ€™s a state which is only jumped to when the bit under it is known, which does seem â€œobjectively inefficientâ€. But sometimes there is unavoidable inefficiency, and maybe even the BB oracle canâ€™t avoid that every time.

Scott Says:
Comment #224 July 31st, 2020 at 4:19 pm
Filip #210: Thatâ€™s great, thanks so much! Iâ€™d say the main things to handle right now are:

(1) Superscripts and subscripts in comments
(2) LaTeX in comments
(3) Some German spam that apparently some people are seeing on this blog (?)
(4) Updating my PHP

(Can anyone suggest anything else that needs to be fixed about this blog at a technical level?)

Anyway, if youâ€™re still interested, then please get in touch with me by email!

Scott Says:
Comment #225 July 31st, 2020 at 4:31 pm
Toby Ord #213: You might be right that my idea of declaring two growth rates â€œthe sameâ€ if theyâ€™re incomparable canâ€™t withstand scrutiny.

My issue is this: in 20+ years in theoretical computer science, Iâ€™ve seen more than my share of weird growth rates, but Iâ€™ve never once seen two that were incomparable (i.e., infinitely often overtaking each other), unless they were specifically constructed that way. So it feels to me like it should be possible to identify a subset of growth rates thatâ€™s totally ordered and that â€œincludes all the ones that matter.â€

Does anyone here have a candidate for such a subset, or have a reason why this canâ€™t be done?

Now, regarding â€œsplitting upâ€ the growth of BB into (say) two functions f and g, such that
(i) neither f nor g is upper-bounded by a computable function,
(ii) an oracle for f or g alone doesnâ€™t let you compute BB, and
(iii) an oracle for any upper bound on f, PLUS any upper bound on g, DOES let you compute BB
â€”my feeling is that yes, this could be done, via a construction similar to the one in my survey. Weâ€™ll just have to alternate: for some regions of n, f will stay constant while g will experience BB-like growth, and for the other regions of n it will be the reverse. And weâ€™ll choose the nâ€™s at which the alternations happen increasingly far apart from each other, in such a way as to kill off each possible reduction from f to g or vice versa. Would you like to work out the details or should I? ğŸ™‚

Scott Says:
Comment #226 July 31st, 2020 at 4:36 pm
Zirui Wang #215: The claimed 27-state machine is here (and I did put the link in the article). Unfortunately, the documentation for why its non-halting is equivalent to Goldbach is pretty spare, and I donâ€™t know if more detailed documentation exists.

Scott Says:
Comment #227 July 31st, 2020 at 5:29 pm
Incidentally, Toby Ord and others: given the magnitude of my uncertainty, I decided in the end not to formulate a conjecture or even a concrete question about the relation between BB(n) and the second-place running time, but just to ask about it in general.

Scott Says:
Comment #228 July 31st, 2020 at 5:34 pm
Bruce Smith #222: Yes, I did know that Busy Beavers arenâ€™t very compressible, although already I hadnâ€™t explicitly thought about it that wayâ€”Iâ€™d just thought about it as

K(nth Busy Beaver) = nlog2n Â± O(n).

More importantly, Iâ€™d completely missed the implication of that fact that you noticed, that

K(BB(n+1) | BB(n)) = Î©(log(n))

(at least, on average over nâ€™s), since yes, one can integrate over conditional Kolmogorov complexities.

I updated the paper accordingly and acknowledged you. Thanks again!!

Scott Says:
Comment #229 July 31st, 2020 at 5:37 pm
Bruce Smith #223: The definition of two Turing machines M and Mâ€™ being â€œessentially differentâ€ that I went with was the one suggested by Joshua Zelinskyâ€”namely, that thereâ€™s some input for which M and Mâ€™ run for different numbers of steps. This definition can justly be criticized for conflating machines that are genuinely different (but happen to have the same runtimes), but it certainly handles the left/right symmetry issue.

Nick Says:
Comment #230 July 31st, 2020 at 8:02 pm
Filip #210, Scott #224:

The German spam links are visible at the bottom of the blog if and only if Javascript is NOT enabled. This has been the case for every browser Iâ€™ve tried, including on my phone and in Emacs.

Bruce Smith Says:
Comment #231 July 31st, 2020 at 10:33 pm
Scott #228: My pleasure, and thanks for the opportunity!

Scott #229: Consider this example: suppose my input is 001111[1]100 where the [1] marks the initial tape position. Then a machine coded to â€œmove right until 0, then haltâ€ runs for 3 steps on this input. But a machine coded to â€œmove left until 0, then haltâ€ runs for 6 steps on this same input. So the definition says those machines are essentially different, as far as I understand it. By the same token it would surely classify a typical BB machine and its mirror image as different.

Scott Says:
Comment #232 July 31st, 2020 at 10:59 pm
Bruce Smith #231: Aha, yet another good catch! I guess I was implicitly thinking that, if you invert a Turing machine about the left/right axis, you should also invert its input along with it. But youâ€™re right that that needs to be said explicitly, and once you do, Joshua Zâ€™s criterion becomes more unwieldy.

Bruce Smith Says:
Comment #233 July 31st, 2020 at 11:08 pm
Scott #232: I was assuming you should fix the definition, but youâ€™re right that that makes it unwieldy. Maybe itâ€™s simpler to just fix the conjecture â€” â€œessentially unique except for exchanging left and rightâ€, or however you want to describe it.

Toby Ord Says:
Comment #234 August 1st, 2020 at 2:46 am
Scott #225, Yes, I think you are right that these incomparable growth rates are somewhat pathological. But Iâ€™m not yet sure how common they are once we depart from the slender subspace of functions that we normally talk about. e.g. it was hard to construct the first algorithmically random real, but most have that property. Something similar might be true here. e.g. perhaps most pairs of random monotonic functions from natural numbers to natural numbers have incomparable growth. It feels to me that they actually do, though there is a problem in picking the measure.

But I donâ€™t think there is generally as much incomparability here as there is in the Turing degrees. e.g. obviously for random sets of natural numbers X and Y, machines with oracles X and Y are almost always incomparable in their abilities. Whereas I doubt they would almost always have busy beaver functions BB_X and BB_Y that canâ€™t be upper bounded a computable transform of the other one. (Note that the details of X canâ€™t in general be extracted from all upper bounds to BB_X.)

I think you are right that to construct a CC and DD that together give BB, they need to alternate the lead for longer and longer times. On my sketch where they alternate according to the digits of Omega, I think CC on its own can give you BB because the islands of 0s and 1s donâ€™t grow in size fast enough â€” so you could make a transform of CC where you look ahead exponentially far, then copy those results back to the early values to get an upper bound of BB that bridges across the damaged parts of CC. I think something like this can be made to get around CCâ€™s damage. My guess is that you should therefore make the islands where each has the lead grow in size uncomputably quickly, to avoid these tricks.

Note that this â€˜computable transformâ€™ of CC that I applied uses a trick a bit more powerful than my previous definition. It is computable given an oracle for CC, even though I donâ€™t think there is a computable function w, such that w(CC(n)) upper bounds BB(n). But this more powerful computable transform now seems to me to be the right way to think about it.

Toby Ord Says:
Comment #235 August 1st, 2020 at 2:57 am
Scott #225, As to finding a core totally ordered sequence of fast growing functions, I think that is where you already are, powering along the arithmetical hierarchy of busy beaver functions. I doubt you will find incomparable pairs without working towards it, even if in some sense it is common.

I am highly unlikely to be able to get round to actually proving any of these things. But hope I can be of some assistance in pointing out promising approaches, or new areas to explore.

Toby Ord Says:
Comment #236 August 1st, 2020 at 3:35 am
Here is another new direction: places where BB growth can occur and what follows from that.

For example, it is well known that one can solve the halting problem via a long enough initial segment of Chaitinâ€™s Omega. Suppose you are asking about whether a program of length n halts and have the first n bits of Omega. Just simulate all programs in dovetail, adding 2^-i to a running total when one of size i halts. Keep going until this total exceeds your initial segment of Omega. Then all programs of length n or less that will ever halt have halted, so you can check if the one you are interested in has halted. But this is a slow process, with time complexity of order BB(n) and a space complexity of order Ones(n).

Are there interesting things that come up when uncomputably fast rates of growth enter into computational complexity? We know that a machine at level omega in the arithmetical hierarchy can decide all the truths of first order arithmetic. What are the time complexities for different algorithms that do so? Are there sensible algorithms that use, say, BB_omega(n) time? I also find it fun that more powerful hypermachines can be more *inefficient* than any Turing machine (and so inefficient, that if you could measure it, you could solve the halting problem). If you think bubble sort is bad, wait until you see how inefficiently a machine epsilon_0 levels up the arithmetical hierarchy can sort a list!

Another example comes from when I was trying to test the limits of probabilistic approaches to solving the halting problem. Suppose you have a Turing machine with access to a fair coin. It turns out you can do *slightly* better than you may have thought. You can design a process where for all input TMs, it halts with probability 1, correctly identifies a halting TM with probability > 1/2 and a non-halting one with probability = 1/2. What you do is start by flipping a coin until you get Tails. Then simulate the input TM for as many steps as you had Heads. If it halts, return True. If it hasnâ€™t halted yet, flip a coin and return True/False at random.

For all input TMs, this gets it right with probability = 0.5 if they donâ€™t halt, and probability > 0.5 + 1/2^BB(n) if they do. What is cute here is that you canâ€™t do something like this that would *computably* bound the probability away from 0.5, or you could work out how many trials to do so that the law of large numbers moves this probability close to 1 and effectively solves the halting problem. But here it is so barely above 0.5 that we canâ€™t effectively apply the law of large numbers.

These examples are all perhaps more humorous than useful, but there is probably some more serious stuff too.

Zirui Wang Says:
Comment #237 August 1st, 2020 at 4:26 am
Scott #226: The pseudocode further down at the link is quite clear that the machine puts an even number of 1s on the tape to see if there is a cut, where both sides are prime.

Scott Says:
Comment #238 August 1st, 2020 at 7:47 am
Zirui #237: Good, so would you say youâ€™re satisfied that the machine indeed does that? (If one understood what to look for, it might not be hard to check by running the machine, although I havenâ€™t tried it yet.)

Scott Says:
Comment #239 August 1st, 2020 at 8:48 am
This is a test of LaTeX in comments; Iâ€™m just going to enclose it between two dollar signs:

ğ‘¥2+ğ‘¦2=ğ‘§2
Scott Says:
Comment #240 August 1st, 2020 at 9:00 am
Everyone: Filip says he enabled rich HTML in the comments for people other than me, so feel free to try that out as well!

Filip Says:
Comment #241 August 1st, 2020 at 9:12 am
Scott #240 and all:

<sup> and <sub> are the ones that are enabled for everyone â€“ but WordPress doesnâ€™t like nested tags so something like 2^2^x might not work well.

Anyway ğ¿ğ´ğ‘‡ğ¸ğ‘‹ is there as well. There are two formats:

inline \( latex goes here \)
block $$ latex goes here $$

Nick #230:
Iâ€™ve also deleted the German spam links, can you confirm that?

Toby Ord Says:
Comment #242 August 1st, 2020 at 10:43 am
Test of html: BBÏ‰ &sc;T BB1 &sc;T BB &sc;T A ~T exp

Toby Ord Says:
Comment #243 August 1st, 2020 at 10:45 am
Test HTML seems to have mostly worked, but my curly greater than signs (ampersand sc semicolon) worked in the preview, but not in the posted comment. In contrast, my omega (ampersand omega semicolon) worked fine.

Luke Says:
Comment #244 August 1st, 2020 at 11:18 am
Hi, love your blog, first time questioner hereâ€¦

â€œThe question is interesting because it speaks to a very old debate: namely, whether Godelian
independence from strong formal systems is a property only of â€œabsurdly complicatedâ€ statements
in arithmetic, such as those that talk directly about the formal systems themselves, or whether
independence rears its head even for â€œnaturalâ€ statements. Of course, expressibility by a small Turing machine is not quite the same as â€œnaturalness,â€ but it has the great advantage of being definite.â€

Indeed, it is this self referential nature of GIC that many people attempt to separate from â€œnaturalâ€ statements. This impulse makes sense to me and I can understand why people do it. It is this same intuition of mine that tells me that what the â€œsmallest unprovable BB(n) under axiomatic system Aâ€ question is doing is just a rephrasing if the liarâ€™s paradox.

For any finite axiomatic system A, at some point you will exhaust the theorems A can prove (since theorems are just combinations of the finite axioms). Then you have a choice, you can say â€œwell thatâ€™s all A can tell usâ€ or you can say â€œletâ€™s keep going!â€. If you choose the latter, the only place left to go is self-reference. In metaphor, youâ€™ve already said everything you can about the area inside the fence A props up, now all you have left is talking about A itself.

Is this intuition of mine that we are just rephrasing the liarâ€™s paradox reasonable? I donâ€™t mean to say itâ€™s uninteresting! Clearly Iâ€™m interested enough to ask this question. But thereâ€™s a nagging part of me that says trying to frame this kind of question as a counterexample to the claims of â€œunnaturalnessâ€ of GÃ¶del Incompletenesss misleading.

Scott Says:
Comment #245 August 1st, 2020 at 12:56 pm
Toby airs #242: Can you try ampersand gt semicolon for â€œgreater thanâ€?

Scott Says:
Comment #246 August 1st, 2020 at 12:58 pm
Filip #241: Thanks so much again for all your help!!

Are special symbols, bold, and italic enabled for everyone as well?

Scott Says:
Comment #247 August 1st, 2020 at 1:15 pm
Luke #244: Glad you like this blog! I do fundamentally disagree with one thing you said. Namely, just about any reasonable formal system has an infinite number of theorems, because there are infinitely many ways to combine even a finite list of axioms (and in cases like induction schema, there are infinitely many axioms as well). So itâ€™s not obvious that there couldnâ€™t be a single system to exhaust all (and only) the true theoremsâ€”that really is something that we needed a GÃ¶del to discover. In the case of the incompleteness theorem, the source of the difficulty is, yes, the ability of computable formal systems to talk about themselves. On the other hand, in the case of (e.g.) the independence of the Axiom of Choice and the Continuum Hypothesis from ZF set theory, the source of the difficulty is different, and has nothing to do with self-reference.

As I mentioned in the survey, we do know examples of arithmetical theorems that one might independently care aboutâ€”they have nothing obviously to do with self-referenceâ€”that are provably independent of Peano Arithmetic (typically they require ordinal induction). Harvey Friedman has been working for a long time to produce similar examples even for ZF set theory. I think itâ€™s fair to say that, 90 years after GÃ¶delâ€™s discovery, the extent to which thatâ€™s possible remains an open problem. Our own project, to find small Turing machines whose behavior is probably independent of ZF, was very much motivated by that problem.

Bruce Smith Says:
Comment #248 August 1st, 2020 at 6:05 pm
I think I can pessimize the n=7 candidate by filling in the N/A slot and modifying another instruction.

Note that the only way to get to state C is via the 1RC instruction in state B. That means that whenever we enter C, the cell to our left is 1 (as just set by that rule).

The cell under C (I mean, the current tape position as now seen by state C) might be 0 or 1. I will assume we know it is sometimes each of those values â€” I didnâ€™t verify this, but I assume whoever designed this candidate inspected the history enough to verify it, or they would have written another N/A into one of those slots in State C. (In any case, anyone with a simulator can probably check this within the first few dozen steps of history.)

That means we can replace the contents of either slot in State C, and know that our replacement will run at least once. For this pessimization, Iâ€™ll replace C1, since its contents (1RB) conveniently also exist in A0.

The goal (as we enter C and are about to run C1) is to delay for two steps (without net tape motion or unfixable net tape change) and then execute 1RB. We do this by replacing C1â€™s contents with 0LA, which changes the tape to 0 (to be fixed later), moves left, and then runs A1 (remember we happen to know the cell to our left was 1). We program A1 to not change the tape, move right (over the 0 we wrote a moment ago), but stay in A. This now runs A0, which does the 1RB we were supposed to do two steps before, from the correct tape position. That also fixes the temporary tape change, as promised (not by undoing the change, but by ensuring that cellâ€™s prior state no longer matters).

To summarize: we replace C1â€™s 1RB with 0LA, and program A1 with 1RA. The result is that every time the old machine enters C1 (at least once, but probably lots of times), the new one takes two more steps but ends up doing the same thing.

(BTW, I didnâ€™t simulate this, so donâ€™t trust it without verifying!)

==

I noticed this because I was wondering whether I could prove that no BB contains an N/A (more formally, an instruction that never runs when starting on the blank tape), or failing that, at least prove *some* upper limit on how many N/As it might contain. But the only general theorems I can come up with are:

â€“ either all N/As are in 0-slots, or all N/As are in 1-slots. (This has two proofs: given an exception, either merge the two involved states into one, or use them to pessimize the machine in some simple way I donâ€™t fully recall right now.)

â€“ for large enough n, we canâ€™t have any constant fraction of slots containing N/As, or this machineâ€™s description would be too compressible to be a BB (at least I *think* this is true â€” Iâ€™m not confident enough about all the necessary ingredients of this argument, to be positive).

I got several partial results which give various conditions under which we can pessimize machines with at least several N/As, but no evidence yet that at least one of these conditions always holds. I donâ€™t feel like spelling all these out right now, but if anyone wants to take this up, they can all be rediscovered quickly by using the ideas in my other proofs, plus the fact that for every additional N/A, there is one more state reachable by only one input transition (and thus with a neighbor cell of known content when it runs). (However, Iâ€™m not optimistic that pursuing all this would quickly lead to a better result â€” all I can say is I didnâ€™t rule that out.)

==

Another question is whether there is any theorem of the form BB(n + c) â‰¥ BB(n) + k, for very small c > 2 but not implied by the theorems already known. In principle this seems possible â€” for example, doesnâ€™t BB(n + c) â‰¥ BB(n) + BB( c) seem intuitively likely? But I have not thought of any new way to prove anything with smaller c than in Theorem 16!

The problem is that if you insert code into an existing BB, even after it halted, it starts on a potentially dense pseudorandom tape which it canâ€™t just ignore, but it also canâ€™t be sure it isnâ€™t blank. So there is no safe way to either treat it as a usable initial state, or erase it.

But there are lots of other proof strategies to think about. What would seem especially interesting is anything which treated the time evolution function of a BB machine in a higher-level way, e.g. showing that to be the BB winner it has to have certain statistical properties in the kind of tape states it uses and/or leaves behind (and thereby a certain number of ones, or better yet, of one-zero changes as you scan the tape). Then you could insert new code which â€œdid nothingâ€ in a slightly slower way than now, or add new code after halting which moved back and forth over some number of existing runs of common bits, perhaps modifying them as it went.

==

Even those schemes would not get at the â€œintuitive real reasonâ€ why BB grows so fast â€” that it keeps finding â€œnew ideasâ€ (which it now has enough capacity to make use of) at each new value of n (or at least, at infinitely many new values, and probably at a reasonably dense set of them). I have no idea how to address that formally, let alone prove it.

Bruce Smith Says:
Comment #249 August 1st, 2020 at 6:07 pm
(The reason the prior comment uses an extra blank space in BB( c) is to avoid a new bug which causes a copyright symbol to appear in BB(c), at least in the preview.)

Sniffnoy Says:
Comment #250 August 1st, 2020 at 7:34 pm
Toby Ord #243:

Yeah Iâ€™ve had the same problem before with a number of symbols. Scott/Filip, can we get it so that arbitrary HTML entities are allowed, at least for printable characters?

Bruce Smith Says:
Comment #251 August 1st, 2020 at 8:12 pm
This is about â€œinverting the BB functionâ€, and an implication of that for K(n | BB(n)).

Using the same ideas as in the discussion of Lazy Beaver (in the paper and/or in other comments), we can prove the following exists and is computable:

R(m) = the smallest n such that there is a machine in T(n) with runtime exactly m.

(Feel free to suggest a better name than R.)

R is sort of like a specialized â€œcomplexity measureâ€ for numbers thought of as runtimes. (Note that when R(m) = n, there are trivially also machines of all sizes larger than n with runtime exactly m.)

The entire function R straightforwardly encodes every â€œspectrum of runtimes of the set T(n)â€, and the LB function too.

And specifically, if m happens to be BB(n), then R(m) = n and R(m+1) = n + 1 = R(m+2) = R(m+3). (Thus LB(n+1) â‰¥ LB(n) + 3, as I mentioned earlier â€” but I didnâ€™t reread the paper to see whether something even stronger than that about LBâ€™s increase is known.)

(Of course, R(m+1) = R(m) + 1 is not a *sufficient* condition for m = BB(n) â€” otherwise BB would be computable!)

==

Anyway, we can use this to â€œinvert BBâ€ â€” by which I mean, define a computable function f so that, given m, if m = some BB(n) then f(m) = n, and otherwise fâ€™s value at m doesnâ€™t matter.

To define f using R, just set f(m) = R(m) for all m. (Most of those values donâ€™t matter, but they donâ€™t hurt!)

(So we might as well just say â€œR inverts BBâ€, even though R has an independent meaning aside from that in which all its values are useful.)

==

So, can we use this for anything? Yes â€” it means K(n | BB(n)) = O(1) (which is as close to 0 as any K(a|b) theorem can get, I think).

Unfortunately this doesnâ€™t justify removing the WLOG comment at the start of Theorem 20 (in the latest version of bb.pdf â€” Theorem 18 in the prior version), since the same argument doesnâ€™t work with BB_L, since itâ€™s possible BB_L(n) = BB_L(n+1). Maybe there is some similar function like R, and argument about K(n | BB_L(n)), in that case â€” I donâ€™t know.

Bruce Smith Says:
Comment #252 August 1st, 2020 at 8:20 pm
(In my #248, by â€œa theorem of the form BB(n + c) â‰¥ BB(n) + kâ€, I really meant â€œa theorem of the form BB(n + c) â‰¥ BB(n) + k(n)â€, for some specific small constant c and function k.)

Nick Says:
Comment #253 August 1st, 2020 at 9:41 pm
Filip #241

I canâ€™t find any spam links or posts, so it looks like everything is fixed.

Bruce Smith Says:
Comment #254 August 2nd, 2020 at 1:04 am
What happens if we restrict attention to reversible Turing machines?

More specifically, to â€œmicroscopically reversibleâ€ ones, by which I mean, looking at just the involved subset of the machine on any step, there is exactly one prior state compatible with any given next state (conditional on that â€œnext stateâ€ actually arising from at least one prior state).

I think this is equivalent to the following conditions on the rules:

â€“ for every state, all transitions into it come from the same direction (L or R), and at most one writes either bit (0 or 1) as it transitions (that way you can examine the neighborhood of the new tape position after the transition, to see how to uniquely run it backwards);

â€“ every state, considering both its rules together, either complements or leaves alone its underlying bit (never one of each behavior in its two rules). (That way, knowing that state was the prior one and the tape position it departed from, you can reconstruct the prior value of the bit in that tape cell.)

I donâ€™t know whether you can express the time-reversed Turing machine as another Turing machine of this same form, though certainly its rule is very local (so at least itâ€™s a cellular automaton). (The reversed version has to examine the proper neighbor, change the state based on that bit and the old state, then change that underlying bit based on that new state. Maybe this can be done by using a few new states for each old one? I donâ€™t know.)

None of the BBs or candidates in the paper are reversible (as easily seen just from which bits they write). Can a reversible machine still exhibit the same general variety of behavior as a regular one? Can it usefully compute anything? (It certainly has a large enough supply of â€œfree energy (zero) bitsâ€, and potentially a place to â€œdump garbageâ€ (the zeros on one side of its work area), but it might be hard to traverse the work area and old garbage and notice exactly when it got to the end of that so it could dump new garbage. Maybe itâ€™s easier to just keep moving into fresh tape (always in one direction, in the long run) and leave garbage behind.)

Filip Dimitrovski Says:
Comment #255 August 2nd, 2020 at 1:41 am
Scott #246, Toby Ord #243:

Thereâ€™s lists (<ul>, <li>), subscript/superscript (<sub>, <sup>),
links (<a>), bold (<b>, <strong>), italic (<i>, <em>), strike, blockquote, code for everyone.
â€” â€”

The HTML entities / ampersand characters supported are mostly in the ASCII range. Toby wants â€œscâ€ which is in Unicode â€” but I canâ€™t find a list of all of them.

I did find https://dev.w3.org/html5/html-author/charref but it also has all the spacing / non-printable entities. If I have time I will compile a list that should be sufficient.

In my opinion itâ€™s easier to just use ğµğµğœ”â‰»ğ‘‡ğµğµ1â€¦ by typing
\( BB_{\omega} \succ_{T} \)
I wrote a script that will render the TeX in the comment preview, but it may not always catch up with your typing. The typesetting library used is MathJax.

â€” â€”

Bruce Smith #249:

I looked at the code and the (c) thing dates back 5 years, so itâ€™s not new. I did fix it now ğŸ˜Š Anyway, I hope I donâ€™t interrupt your Busy Beaver discussion, we should test the comment stuff in another thread.

Toby Ord Says:
Comment #256 August 2nd, 2020 at 3:07 am
Great, Iâ€™ll give this another test. First testing ampersand gt semicolon: >

The main thing that had me going with ampersand sc semicolon was that it worked in the preview, but not the comment. However, if the following LaTeX works, then I agree we donâ€™t need all the HTML entitites: ğµğµğœ”â‰»ğ‘‡ğµğµ1
Test of HTML sup and sub: BB1(10) > 1010

Scott should definitely document the fact that you start and end with double dollar signs for blocks or backslashed parentheses for inline in the Comment Policy section (the former is a bit ambiguous at the moment â€“ I though the pair was one at the start and one at the end).

Sniffnoy Says:
Comment #257 August 2nd, 2020 at 4:01 am
Filip Dimitrovski #255:

Thereâ€™s a list of all named ones here, if only named ones are going to be allowed. (Which covers all cases Iâ€™ve actually wanted, really, but Iâ€™m surprised you canâ€™t just likeâ€¦ turn on numeric ones as well, maybe with an exception for ones that could potentially mess things upâ€¦ I mean itâ€™s not like I couldnâ€™t just copy and paste in a character I wanted, if I really wanted!)

Sniffnoy Says:
Comment #258 August 2nd, 2020 at 4:05 am
Filip #255:

Actually, I guess better than Wikipedia is the official list here; it seems to include some that arenâ€™t on Wikipedia at the moment.

Sami Says:
Comment #259 August 2nd, 2020 at 7:11 am
I wonder if any of the exotic behavior of busy beaver is visible by looking at the computable function bb_m (n), defined as the supremum of runtimes of n state machines, that stop after at most m steps. This impatient beaver stays computable no matter how large n gets, but could any interesting properties emerge here, or are the nonprovability and other stuff completely hidden in the m goes to infinity limit?

Jon Awbrey Says:
Comment #260 August 2nd, 2020 at 7:13 am
The following site is very useful for unicode â€”

https://www.sql-und-xml.de/unicode-database/

There is also a converter app â€”

https://www.sql-und-xml.de/unicode-database/online-tools/

Zeb Says:
Comment #261 August 2nd, 2020 at 7:15 am
What can we say about the Kolmogorov complexity of the final state of the tape after the nth busy beaver halts, as a function of n? At first I thought that it should be very high, but clearly it is bounded by the number of bits needed to describe the nth busy beaver (plus an additive constant). Are there any better upper bounds we can put on it?

Scott Says:
Comment #262 August 2nd, 2020 at 10:38 am
Sami #259: Itâ€™s an excellent question, closely related to the Lazy Beaver discussion above. By definition, the spectrum of runtimes doesnâ€™t tell you anything until you get up to LB(n), which is relatively close to |T(n)|. After that â€¦ well, it clearly doesnâ€™t let you compute BB, since otherwise BB would be computable, but it would be fascinating to know whether any of the properties of the BB machines cast â€œshadowsâ€ lower down in the runtime spectrum.

Toby Ord Says:
Comment #263 August 2nd, 2020 at 3:44 pm
I want to take a step back and think more generally about the relationships between (1) fast rates of growth, (2) computability, and (3) ordinals. BB relates (1) and (2). The recursive ordinals that mark the levels of the arithmetical hierarchy combine (2) and (3). Computable approaches to fast growing functions tend to involve ordinals, combining (1) and (3). Our current approaches to the best known variant of BB (by jumping a transfinite distance up the arithmetical hierarchy) involves all three.

Letâ€™s start by considering the sequence of binary operators: addition, multiplication, exponentiation, tetration, etc. Each is defined recursively by repeated applications of the one before. Together they form the infinite hierarchy of the hyperoperations. The best notation I know of for combining x with y using the zth hyperoperation is x[z]y (so 10^6 = 10[3]6). Each of these hyperoperations is computable, and indeed is primitive recursive. But the function A(n) = n[n]n (the most elegant definition for the Ackermann function) diagonalises out of the hyperoperations, growing faster than any of them, and indeed so fast that it grows faster than any primitive recursive function. (The hyperoperations thus nicely stratify the primitive recursive rates of growth.)

So far so familiar. Indeed, this is sometimes seen as a prelude to the main event of the busy beaver function. But it is interesting to pause to wonder how much busy-beaverology can be applied here too. As we did with the busy beaver function, we can add A(n) as a new primitive function for primitive recursion and expand its power. For example, we can now get to new functions that grow faster than A(n).

Q1: does this lead to an interesting transfinite hierarchy of computability extending primitive recursion? (i.e. add the limit of the new fast growing functions in as second additional primitive, then keep going)
Q2: does it suffice to add *any* function that grows faster than A(n) as the primitive, like you can with the busy beaver function?

If the answers are both â€˜yesâ€™, the analogy would be particularly tight. Even if not, it could still be interesting.

Toby Ord Says:
Comment #264 August 2nd, 2020 at 3:45 pm
The relationship between fast growing functions and the ordinals has always seemed very tight to me. For example letâ€™s build a fast growing function. Suppose you start with the constant zero function (f(n) = 0), then add one to get the constant one function (f(n) = 1), then add one to get the constant two function, etc. Then diagonalise out by taking the nth of these functions applied to n (which gives f(n) = n), giving you something that grows faster than all of them. Then you can go on to f(n) = n+1, then f(n) = n+2â€¦ to f(n) = n+n and so on, in a way that mirrors the standard introductions to the ordinals. And indeed to some extent you could use these to represent the ordinals. I think a lot of people (e.g. high school students) would find this a more concrete and less mysterious way of working with a related pattern.

Suppose we keep going with successor functions (adding one to the right hand side) and limit functions (diagonalising out of a parameterised family of earlier functions). We go through f(n) = n*n, then f(n) = n^n, and onwards. This is just the beginnings of the hyperoperations and we can go smoothly up through them, continuing with f(n) = n[4]n, etc. and diagonalising out to f(n) = n[n]n = A(n).

Why donâ€™t the standard introductions to the ordinals proceed this way? You see Ï‰, then Ï‰+Ï‰ then Ï‰*Ï‰, then Ï‰^Ï‰, and then it is treated as if something strange and interesting happens, where you use fix points to get to Îµ0, which is also represented as Ï‰^Ï‰^Ï‰â€¦ It is shown to have the fascinating fixpoint property of being the first Î± such that Î±=Ï‰^Î±, and then this property is used to reach ever higher ordinals. But as far as I can tell, there is nothing special causing a barrier at the point of exponentiation that warrants a new approach. We could just move on to Ï‰[4]Ï‰ (which would equal what is usually denoted Îµ0). And we could use the hyperoperations to power on further with Ï‰[5]Ï‰, Ï‰[6]Ï‰, and diagonalise out to Ï‰[Ï‰]Ï‰, then use other ideas from the theory of fast growing functions to proceed further.

Note that even the special fixpoint property of Î±=Ï‰^Î± doesnâ€™t seem special to me, as it is just the third ordinal to have such a property. Ï‰*Ï‰ was the first to have Î±=Ï‰+Î±, and Ï‰^Ï‰ was the first to have Î±=Ï‰*Î±. So it seems like we could have started to use the fixpoint approach earlier if we wanted and that we didnâ€™t have to start using it at Îµ0. So the standard approach has a strange Frankensteinian combination of the hyperoperation approach switching to the fixpoint approach at an arbitrary point (between the familiar exponentiation and the unfamiliar tetration).

As far as I can tell, the reason we donâ€™t just continue with hyperoperations (at least for some considerable distance) is that they were only systematised in 1914, whereas the standard approach was already systematised by Veblen in 1908. I find even this puzzling, as the hyperoperations are an extremely obvious thing. What fraction of bright high school children came up with raising n to the nth power n times when daydreaming in class? 50%? Or noticed the pattern of multiplication as repeated addition, and exponentiation as repeated multiplication and wondered if this keeps going? Only a particularly bright high school student could properly formalise this, but it is still pretty easy in the scheme of things.

Q3. Is there any problem with the approach of defining ordinals by hyperoperations and then the Ackermann function that I may not have spotted?
Q4. Which ordinals (in standard notations) do Ï‰[4]Ï‰, Ï‰[5]Ï‰, Ï‰[6]Ï‰, and Ï‰[Ï‰]Ï‰ correspond to?
Q5. It might be supposed that a remaining special property of Îµ0 is that Cantor normal form works for all ordinals up to that point, but not beyond. Would a version of Cantor normal form that uses [4] instead of [3] work beyond that point? If so this remains an arbitrary stopping point once hyperoperations are considered first-class citizens.

Toby Ord Says:
Comment #265 August 2nd, 2020 at 3:46 pm
Finally, letâ€™s look at the relationship between all three of rates of growth, computability, and ordinals.

As far as I understand, the recursive ordinals are usually represented with programs/machines that compute unusual orderings of the naturals, where the order-type of the ordering gives you the ordinal it is representing. e.g. imagine a program that rates any even number as â€˜less thanâ€™ any odd number, and uses the usual orderings within each category. This would represent Ï‰+Ï‰.

Q6: Could one instead have a system of recursive ordinals directly based on rates of growth? e.g. using the extension of Cantor normal form via the hyperoperators (that I mentioned in the previous comment) for ordinals up to Ï‰[Ï‰]Ï‰, then continuing with related schemes with even faster rates of growth for higher ordinals. So instead of an ordinal being represented by a program taking two natural numbers and comparing them, it is a program that takes a single natural number and applies a monotonic fast growing function to it.

If so, you could have a very elegant computability hierarchy based entirely in fast rates of growth. You use an oracle for the busy beaver function of a given machine type as the jump operator (instead of using the halting problem), to get above an infinite sequence of such jumps you diagonalise out of the sequence of fast rates of growth (instead of dovetailing the oracle sets). So, e.g., the level Î”Ï‰ is a machine with an oracle for BB_n(n). And for each ordinal in this hierarchy, they get represented with fast growing computable functions. (Indeed, if we wanted to be extreme purists we wouldnâ€™t even need to use ordinal notation in their subscripts, but could just use the fast growing functions themselves. e.g. using n instead of Ï‰, n[4]n instead of Îµ0â€¦). Then all of the Busy Beavers weâ€™ve talked about including BB, BB1 and even (BBw(n)(n)) could be done with fast growing functions all the way down. No halting functions, no orderings, perhaps even no ordinals (at least not considered in their usual sense as infinite numbers).

If all of that works (or can be made to work) I think this is pretty cool. And I think it is the kind of thing Scott is going for in taking the busy beaver function seriously, and in being as finitistic as possible. It may seem a bit more complicated now, because we usually just help ourselves to the traditional versions of these concepts and have built our own here, but I think it is actually at least as simple and may be more pure this way.

And it may also open up some new possibilities in terms of getting ever faster rates of growth. For example, what if we use uncomputable rates of growth to specify the ordinals we need in order to reach ever higher levels of uncomputability and thus faster rates of growth?

Q7: Since ordinals usually correspond to rates of growth, is there an ordinal corresponding to the BB rate of growth?
Q8: If so, could we then specify that level of the arithmetical hierarchy, going beyond where weâ€™ve been able to get to so far, and then take BB function for machines with that oracle?

Bruce Smith Says:
Comment #266 August 2nd, 2020 at 4:15 pm
Can BB( ) *sometimes* be computably upper-bounded?

That is, could there be a computable partial function f (defined for an infinite number of values) which gives an upper bound for BB(n+1) in terms of BB(n)? (To make this more likely, weâ€™ll let it know both n and BB(n).)

More precisely: we want f(n, BB(n)) to upper bound BB(n+1) whenever f is defined for that pair of arguments. Otherwise, all we require about f is that for an infinite number of n, f(n, BB(n)) is defined.

(By a â€œcomputable partial functionâ€, I mean it has an algorithm which emits an answer or emits a symbol that means â€œf is not defined for that pair of argumentsâ€, and always halts.)

(If some f had this property, we could modify it to output BB(n+1) exactly, but somehow this question seems more intuitive to think about when I only ask f to upper bound it.)

Motivation: in thinking about whether Theorem X3 (aka Proposition 14) could be improved, itâ€™s reasonable to assume not (that is, for some values of n itâ€™s the best that happens in reality) and look for a contradiction. To start with, we can strengthen that even more, to make it even easier to find a contradiction. That hypothesis for contradiction might then be â€œthere is an algorithm which recognizes an infinite set of values of n for which BB(n+1) = BB(n) + 3â€. But maybe weâ€™d get lucky and merely the fact that it recognized any â€œcomputable upper boundâ€ would lead to a contradiction. Hence this question.

Bruce Smith Says:
Comment #267 August 2nd, 2020 at 5:07 pm
In my last comment, I think the n argument to f is redundant, since it can be computed from BB(n). (But it might make certain f easier to express.)

A related, simpler question is this one:

Could there be a computable total function g such that BB(n+1) â‰¤ g(BB(n)) infinitely often?

(Unlike for f, here we donâ€™t require g to *know* for which values of n that is the case.)

Scott Says:
Comment #268 August 2nd, 2020 at 6:22 pm
Toby Ord: Thanks for all these fascinating thoughts! Yes, it should absolutely be possible to define ordinals using fast-growing functions, not just the reverse. But for defining generalized BB functions, we need more than just the definition of the ordinal, no? We also need a coding scheme by which an oracle Turing machine can specify anything up to the ordinal, in order to command its oracle about which halting problem to solve. How would we use the fast-growing function definitions to get that?

Sniffnoy Says:
Comment #269 August 2nd, 2020 at 7:57 pm
Toby Ord #264:

Oh, I can answer some of your questions about ordinals and hyperoperations. Iâ€™ve looked at this before, and, well â€” basically, the reason that people donâ€™t tend to use hyperoperations with ordinals is that the results end up being pretty disappointing once you go beyond exponentiation.

Letâ€™s assume weâ€™re defining hyperoperations analogously to how one would on whole numbers, so that everything extends seamlessly. (Later weâ€™ll consider some alternatives â€” but I think Iâ€™ll post that as a separate comment if you donâ€™t mind.)

Letâ€™s look at Î±[4]Î², as Î² increases. First we have 1, then Î±, then Î±Î±, then Î±Î±Î±â€¦ then we have Î±[4]Ï‰, which will be some epsilon number; letâ€™s just call it Îµ. (I mean, specifically, itâ€™ll be the smallest epsilon number greater than Î±, but thatâ€™s not the point.)

But then we get that Î±[4](Ï‰+1)=Îµ as well. And then of course Î±[4](Ï‰+2)=Îµ also. And so forth, until you get Î±[4](Ï‰2)=Îµ as wellâ€¦ and ultimately you get Î±[4]Î² = Î±[4]Ï‰ for all Î²â‰¥Ï‰.

So, Î±[4]Î² turns out not to be that interesting as Î² ranges through all ordinals; itâ€™s only really interesting for Î²â‰¤Ï‰.

The same thing will of course happen for Î±[5]Î² as well, or for Î±[Î³]Î² for any Î³â‰¥5. Except here itâ€™s even worse; letâ€™s look at what happens if we make the additional assumption that Î± is infinite. Well then in that case weâ€™ll actually get Î±[5]Î² = Î±[4]Ï‰ for all Î²â‰¥2. Yikes! And then of course weâ€™ll get Î±[Î³]Î² = Î±[4]Ï‰ for any Î³â‰¥5, Î²â‰¥2.

Moreover, once Î³â‰¥Ï‰, then even if Î± and Î² are finite, well, assuming Î± and Î² are, say, more than 2 or so (to avoid trivialities), youâ€™ll get Î±[Ï‰]Î²=Ï‰ still.

So basically, your cases here are:
Î³â‰¤3: Successor, addition, multiplication, exponentiation; these are interesting
Î³=4: Tetration â€” this is interesting for finite Î², but becomes uninteresting once Î² is infinite
5â‰¤Î³<Ï‰: Further ordinary hyper operations â€” this is interesting for finite Î± and Î², but becomes uninteresting once either argument is infinite
Î³â‰¥Ï‰ â€” This is uninteresting

Or to put it differently, the interesting cases of hyper operations, using Roman letters to denote finite numbers, are:
1. Î±[n]Î² for nâ‰¤3
2. Î±[4]k
3. m[n]k

So basically, the reason that people only study higher hyper operations for whole numbers, rather than for ordinals, is that thatâ€™s pretty much the only interesting case. The only interesting case that misses is the case of Î±[4]k. So, yeah. Thatâ€™s why.

(There are also some definition issues when Î±=0; what is 0[4]Ï‰, for instance? But I suppose those can largely be ignored.)

(Also worth noting while above I assumed weâ€™re using the ordinary ordinal operations, there are various alternative notions of addition, multiplication, and exponentiation on ordinals (I wrote a paper on this ğŸ™‚ )â€¦ but it doesnâ€™t matter which ones you use here. The uninteresting cases above will be uninteresting regardless of which notion of exponentiation you use, and moreover will be insensitive to which one you use â€” not only will you get uninteresting results, youâ€™ll get the same uninteresting results! That said, which notion of exponentiation you use can affect your results in the Î±[4]k case, as can be seen by considering (Ï‰+2)[4]2.)

Anyway I didnâ€™t really explain in this comment why this happens â€” although a lot of the answer to that question is fairly obvious once the phenomenon is pointed out, I do think there is something worth remarking on here â€” but Iâ€™ll get to that in a later comment I think.

Sniffnoy Says:
Comment #270 August 2nd, 2020 at 7:59 pm
Toby Ord #264:

Oops, obviously that sequence in the third paragraph of my previous comment was supposed to start Î±, Î±^Î±, Î±^(Î±^Î±)â€¦ I guess stacked sup tags really donâ€™t work. Maybe I should just use this newfangled LaTeX feature. ğŸ˜›

Sniffnoy Says:
Comment #271 August 2nd, 2020 at 9:11 pm
Toby Ord #264:

OK, so, I want to now write more about my previous comment, and answer the question â€” why does this happen, and why can it not be avoided?

The first obvious answer to why this happen is, continuity. Like â€” the root of the problem is Î±[4](Ï‰+1), right? (Well, that and m[Ï‰]k, but that case is less interesting, and kind of obviously impossible to avoid. Although obviously continuity is the problem there as well.) We get Î±[4](Ï‰+1)=Î±[4]Ï‰ because Î±[3]Î², that is Î±Î², is continuous in Î², and Î² is where the nesting occurs.

Except, this raises the question â€” wait, why doesnâ€™t this happen for Î±[n](Ï‰+1), [n]â‰¤3? Why donâ€™t we get Î±[n](Ï‰+1)=Î±[n]Ï‰ there, too?

And the answer here has to do with the definition of Î±[n]Î² when n is that small. We define multiplication of ordinals to be interated right-addition, we define exponentiation of ordinals to be interated right-multiplication; in other words, itâ€™s in the left argument that the nesting occurs. And addition and multiplication are continuous in the right argument, not the left argument, so continuity isnâ€™t relevant here and doesnâ€™t cause the problem above.

But this then raises the question â€” well, wait. So for nâ‰¤3 weâ€™re iterating on the right, to ensure that we get the standard ordinal operations (and to avoid continuity problems), but then for nâ‰¥4 weâ€™re iterating on the left? Why the switch? Why not just do things consistently on the right, get rid of the continuity problem entirely?

Well, because that introduces a different problem, and Iâ€™d say a worse one. We have to switch to interating on the left in order to match the standard hyper operations on the whole numbers. For nâ‰¤3, the choice of direction doesnâ€™t matter for whole numbers, since addition and multiplication are both commutative, but for nâ‰¥4 it does. So if we want to match both the standard ordinal operations and the standard hyper operations, we have to iterate on the right for nâ‰¤3, but on the left for nâ‰¥4.

(â€¦I guess that doesnâ€™t address the case of what happens when n becomes an infinite ordinal â€” hm. I hadnâ€™t thought about that before. But letâ€™s ignore that case for nowâ€¦ finite n firstâ€¦)

OK, but why not just not match the usual hyper operations? Well, because the usual hyper operations are defined that way for a reason! If instead of going Î±, Î±^Î±, Î±^(Î±^Î±), Î±^(Î±^(Î±^Î±)), etc., you went Î±, Î±^Î±, (Î±^Î±)^Î±, ((Î±^Î±)^Î±)^Î±, etc., this would then just be Î±^(Î±^k) as k increases. Thatâ€™s a lot slower-growing and a lot less interesting (I mean, it can be directly expresses in terms of the previous operations). Thatâ€™sâ€¦ really not what we want from a hyper operation.

Of course, this once again raises the question, why does this not happen for nâ‰¤3?

Well, for exponentiation we have the iteration relation (Î±Î²)Î³ = Î±Î²Î³, which causes this problem. This problem doesnâ€™t come up for n=3 because the relevant expression there is (Î±Î²)Î³. There is an algebraic relation that applies here, associativity, but it doesnâ€™t, like, reduce things. Similarly with n=2, making multiplication from addition â€” yeah, addition is associative, but so what? And as for n=1, making addition from successor, once againâ€¦ yeah.

OK, but this still raises the question â€” why does exponentiation have a relation that reduces it like this while addition and multiplication donâ€™t?

â€¦and, well, it would be possible to give a detailed answer to this question, but I think Iâ€™m going to stop here. But the short answer here is that notice what exponentiation is reduced to â€” it is reduced to multiplication. Basically, wellâ€¦ addition and multiplication are special in a way. All the algebraic relations you get among the low hyper operations, they all involve addition or multiplication, and thatâ€™s due to these two (especially addition) having a certain applicability that exponentiation and the higher ones donâ€™t. So with exponentiation, even all the relations you get involving it, still involve addition or multiplication. So exponentiation can have a relation that reduces it to multiplication, but you wonâ€™t get that phenomenon with n=1 or n=2.

â€¦OK, thatâ€™s not really a complete answer, because that doesnâ€™t explain why the multiplication case canâ€™t reduce to addition, but still. Iâ€™m skipping a detailed answer because I donâ€™t think we all really need to be reminded where the basic algebraic relations between addition, multiplication, and exponentiation come from; anyone reading this probably already has a pretty good understanding of that, or can sit down and think about it if they havenâ€™t thought about it recently. So, you all can write that explanation yourself. But, I donâ€™t know, I thought the rest of this was maybe worth laying out explicitly!

Point is, the switch at n=4, which causes things to become uninteresting, is necessary to avoid a worse way in which things could become uninteresting.

Notionally you could switch back once n (Î³) becomes infinite to rescue things somewhat in that case â€” remember, what happens when n becomes infinite is really a distinct cause of uninterestingness â€” but I think that still causes things to become uninteresting, you just get slightly different values out of it. Iâ€™m not entirely sur though, I havenâ€™t really looked at that. Still seems kind of pointless if things are crappy inbetween, though.

Bruce Smith Says:
Comment #272 August 2nd, 2020 at 9:42 pm
Is there any known connection between â€œBusyBeaverologyâ€ and complexity classes like P, NP, EXP, etc?

For example â€” define the predicate R'(n, m) := (n â‰¤ R(m)), using R from my comment #251. Then if we encode n in unary and m in binary, I think a straightforward implementation of Râ€™ is in EXP (since it iterates over all T(n) and smaller, and runs each one for time up to m). (Unfortunately I donâ€™t see any way itâ€™s EXP-complete.) Is there any hope of proving an arbitrary implementation of Râ€™ is not in P?

But that is just a shot in the dark â€” my real question is the general one above. The reason I can imagine *some* connection is that, unlike in many discussions of computability, we do care something about the runtimes.

Sniffnoy Says:
Comment #273 August 3rd, 2020 at 2:45 am
Toby Ord #264:

Btw, part of my motivation for looking at ordinal hyperoperations was the same as yours â€” that the fast-growing hierarchy starts out as hyperoperations, so it seems like ordinal hyperoperations should go together with the fast-growing hierarchy, right? But as far as I can tell thatâ€™s not the case. :-/

So yeah, unfortunately, like I said above, Ï‰[4]Ï‰, Ï‰[5]Ï‰, Ï‰[6]Ï‰, and Ï‰[Ï‰]Ï‰, and indeed Ï‰[Î³]Ï‰ for any larger Î³, are all disappointingly just equal to Îµ0. :-/

Toby Ord Says:
Comment #274 August 3rd, 2020 at 5:42 am
Thanks so much Sniffoy for this very detailed explanation of what goes wrong with a direct application of the hyperoperations (especially with why things change at level 3 in the hierarchy). The upshot is indeed unfortunate for my unified approach to rates of growth, ordinals, and uncomputability.

Looking here, I see a couple of modified definitions of the hyperoperations for transfinite arithmetic that lose some elegance and some independent motivation, but that apparently basically get the job done. Iâ€™m glad to see people have looked into this obvious and natural idea! In these versions, the sequence of ordinals of the form Ï‰[k]Ï‰ roughly correspond to Î¦k(0) in the Veblen hierarchy, with Ï‰[Ï‰]Ï‰ roughly being Î¦Ï‰(0) and the supremum of the sequence Ï‰, Ï‰[Ï‰]Ï‰, Ï‰[Ï‰[Ï‰]Ï‰]Ï‰ â€¦ being the Feferman-Schutte ordinal.

Note that this would mean that in terms of fast growing function representations, the Feferman-Schutte ordinal is at about the same place in the ordinals as Grahamâ€™s number is in the naturals, since the latter is precisely equal to
3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[
3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[3[6]
3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]
3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3]3
(64 nestings of 3s around a 6)

So my approach of coding ordinals with their equivalent fast growing functions on the natural numbers is less good than Iâ€™d hoped, at it is less clear that the finite hyperoperations really are the same as their ordinal counterparts.

I donâ€™t think the method is sunk, though it is somewhat less shiny than if it had all worked as cleanly as Iâ€™d hoped.

Here is a thought, aimed at avoiding the slightly inelegant matching up of fast growing functions with ordinals. What about just ignoring the intermediate step of labelling levels in the hierarchy with ordinals at all? Instead, just label them with fast growing functions.

So: the levels are all of the form ğµğµğ‘“(ğ‘˜)(ğ‘›) where f(k) is a fast growing function (I use k to distinguish it from the variable that the BB function actually takes, which is n and which I will sometimes suppress).

ğµğµ0 is the busy beaver function for Turing machines
ğµğµğ‘“(ğ‘˜)+1 is the busy beaver function for Turing machines equipped with oracle ğµğµğ‘“(ğ‘˜)
Then for any sequence of BB functions ğµğµğ‘“ğ‘—(ğ‘˜), we can diagonalise out to make a level above all of them with ğµğµğ‘“ğ‘˜(ğ‘˜).

Then proceeding upwards through the hyperarithmetic hierarchy takes us through ğµğµğ‘˜,ğµğµğ‘˜+ğ‘˜,ğµğµğ‘˜âˆ—ğ‘˜,ğµğµğ‘˜ğ‘˜,ğµğµğ‘˜[4]ğ‘˜,ğµğµğ‘˜[ğ‘˜]ğ‘˜, etc.

The question is then around the rules for how we can build up these subscript functions. If we allow any (monotonic) recursive functions, then we can certainly have one for k nestings of square brackets, like k[k[k[k]k]k]k. If using the ordinals, weâ€™d have to worry about whether this really is precisely the Feferman-Schutte ordinal, but I think we donâ€™t have to worry about that here.

I conjecture that with recursive functions in the subscript, this hierarchy goes the same distance as the arithmetical hierarchy, but *potentially* does so in a clean way that is just built of fast growing functions.

Then one can ask about what happens if we can have subscripts corresponding to well defined and â€˜platonicâ€™ fast growing functions that arenâ€™t themselves recursive.

e.g. ğµğµğµğµ(ğ‘˜)(ğ‘›)
(not to be confused with ğµğµğµğµ(ğ‘›)(ğ‘›), which has already been discussed in Scottâ€™s paper and is just diagonalising over all finite levels)

Toby Ord Says:
Comment #275 August 3rd, 2020 at 5:59 am
Scott #268,
Yes, it should absolutely be possible to define ordinals using fast-growing functions, not just the reverse. But for defining generalized BB functions, we need more than just the definition of the ordinal, no? We also need a coding scheme by which an oracle Turing machine can specify anything up to the ordinal, in order to command its oracle about which halting problem to solve. How would we use the fast-growing function definitions to get that?

I donâ€™t think you quite need that if using BB functions, but I may be wrong.

My approach is to use the BB property that any function which grows faster suffices. Consider level Ï‰. The function I want to use to encompass all levels below that is ğµğµğ‘›(ğ‘›), which grows fast enough that we donâ€™t need to be able to pick out any particular level of BB function below this. If you want to ask about an o-machine at level 47, we donâ€™t extract out ğµğµ47(ğ‘›), we just use ğµğµğ‘›(ğ‘›) itself.

As far as I can see, the main flaws in this strategy could be if this doesnâ€™t work beyond some higher level for reasons I havenâ€™t considered, or if there is a problem where we really need to know in full generality how high n needs to be before ğµğµğ‘›(ğ‘›) forever exceeds ğµğµ47(ğ‘›). When dealing with a single jump, we donâ€™t need to worry about that, because there is some hard coded k you could put in where beyond that k, the faster growing function wins and we donâ€™t need to know it. But I suppose here there might need to be an effective method for finding it. In the case Iâ€™m actually considering, we know that 47 is such a k. I *think* it may be similarly easy to determine a lower bound for when this diagonalised BB function exceeds the lower level you are asking about, thought it might depend on the details (e.g. Iâ€™m now floating versions of this based on ordinals and based purely on fast growing functions and these may differ, with the latter version sounding easier to me as we just use its subscript function to find the value of k).

Scott Says:
Comment #276 August 3rd, 2020 at 12:25 pm
Bruce Smith #272: Those are excellent questions! The Busy Beaver function itself seems literally to shoot past the realm of complexity theory, into the stratosphere of computability theory. But Iâ€™d wondered myself about a question thatâ€™s clearly related to your question, although Iâ€™m not sure if itâ€™s identical. Namely, whatâ€™s the complexity of computing the Lazy Beaver function, LB(n)? Itâ€™s clear that it can be done in exp(n) time. Can one show that the problem is complete for unary problems in EXP?

Alas, to address one of your questions, proving EXP-hardness is virtually the only tool that we now have for proving unconditionally that a problem is not in P.

Scott Says:
Comment #277 August 3rd, 2020 at 12:35 pm
Toby Ord #275: I love your idea of just constructing the hierarchy of BBÎ± functions directlyâ€”using oracles for fast-growing functions to define even faster-growing functionsâ€”and thereby bypassing entirely the need for Turing machines that encode oracle notations. I think it works, and I donâ€™t know why I didnâ€™t think of it myself!

Concretely:
â€“ Given any ordinal BB function, BBÎ±, we can define BBÎ±+1 as the BB function for Turing machines with oracles for BBÎ±.
â€“ Given any countable sequence of ordinal BB functions, BBÎ±(1), BBÎ±(2), â€¦, we can define BBÎ²(n), where Î² is the limit ordinal of Î±(1), Î±(2), â€¦, as BBÎ±(n)(n).

One might wonder if, by proceeding far enough in this way, we could even get past the Church-Kleene ordinal. Unfortunately, the whole point of the Solovay result that Iâ€™ve now added to my survey (as Proposition 4) is that, if we did, then our fast-growing functions would no longer be hard to compute merely in virtue of how quickly they grew. For that property, unless Iâ€™ve misunderstood something, the â€œhyperarithmeticâ€ BB functions (i.e., those of the form BBÎ±, for some ordinal Î± below the Church-Kleene ordinal) do indeed seem to be the limit.

Sniffnoy Says:
Comment #278 August 3rd, 2020 at 1:11 pm
Scott #276:

Hereâ€™s an idea. Above it was noted that all the busy beavers seem to have the property that they halt on every input, not just on empty input, right? So letâ€™s actually define the function this way; BB'(n) will be the longest runtime on empty input for machines with n states that halt on every input. Obviously BB'(n)â‰¤BB(n), it seems like maybe theyâ€™re equal, but who knows.

Then BB'(n) can be straightforwardly generalized to a simple complexity class like P â€” BBâ€™P(n) would be the longest runtime on empty input for machines with n states that run in polynomial time.

â€¦except, now that I actually write that out, hm. I was thinking that, like, BBâ€™P(n) would, like, be much smaller than BB(n), but in fact the disconnect between the thing weâ€™re taking the maximum over (the specific case of empty input) and the condition weâ€™re imposing (which isnâ€™t affected by the runtime of any one input on its own) means that likely itâ€™s probably pretty close to BB(n), like BB(n-c) or something. Like you could use a constant number of states to check for empty input and then execute the busy beaver, right? While in the nonempty case just halting immediately.

Oh well, so much for that ideaâ€¦

Bruce Smith Says:
Comment #279 August 3rd, 2020 at 1:11 pm
{
Scott #276: â€œNamely, whatâ€™s the complexity of computing the Lazy Beaver function, LB(n)? Itâ€™s clear that it can be done in exp(n) time. â€¦â€

(I assume you mean if the input n is encoded in unary, right?)
}

Bruce Smith Says:
Comment #280 August 3rd, 2020 at 1:18 pm
(Nevermind, I was confused â€” I see there are only exp(n) members of T(n) in the sense of exp used in complexity theory.)

Bruce Smith Says:
Comment #281 August 3rd, 2020 at 1:51 pm
Actually I was confused on several levels at once â€”

â€“ yes, |T(n)| is in exp(n) (meaning O(2^poly(n)), right?), and so is its square, which bounds runtime of computing LB(n);

â€“ but you *do* have to encode n in unary anyway, if you want input size to be like n;

â€“ but you considered that obvious and ok to leave implicit, since after all, you said exp(n) rather than exp(log n)!

Anyway, that computation does run all those machines for up to about 2^(n log n) time (is that enough to cover each poly in O(2^poly(n))? I think not) â€” but since its output lumps them all together, I see no way to single out any one machine of interest, which you would seem to need to do to show this is EXP-complete. Nor can you use the implicit self-reference (in any way I can see), even though one of those machines is one that computes LB on a hardcoded n in binary (but thereby takes more than exp time, so is still running when this computation gives up on it). Even if the self-reference really worked, the lumping together would seem to make it unexploitable (since any machine of interest is probably not the last one to halt before LB(n)).

Bruce Smith Says:
Comment #282 August 3rd, 2020 at 2:01 pm
Re my comment #248: I did end up simulating my slight pessimization of Wythagorasâ€™ machine, to verify the pessimization really happens and works properly. Iâ€™d post my program here (a very short simulator in python), but the HTML <pre> tag is not working (at least in preview).

Bruce Smith Says:
Comment #283 August 3rd, 2020 at 2:30 pm
Wait, if we hypothesize that LP(n) for unary n is in P (ie runtime in poly(n)) (ignoring thatâ€™s its not a predicate for the moment), then can we try to make a contradiction like this:

there is also a P machine which hardcodes the n in binary, expands it to unary, runs the hypothetical machine, gets LB(n) as a number, knows exactly how long it already ran and subtracts that to get a runtime it still needs to exhibit, thinks â€œhmm, *that* runtime is < LB(n) so maybe I have a chance at getting it exactlyâ€, then actually does that, thereby running for exactly LB(n) for a contradiction? This is just one of all the machines simulated by LB, but all it has to do is fill in the hole left by all of them!

Bruce Smith Says:
Comment #284 August 3rd, 2020 at 2:37 pm
To make that work, one part is knowing the exact runtime of the hypothetical LB-in-poly(n)-machine. But our new machine can simulate it and count its steps, and know its own simulation of k steps took exactly ck steps for some small c. So as far as I can tell right now, that idea could actually be made to work! Of course Iâ€™m probably missing somethingâ€¦

Bruce Smith Says:
Comment #285 August 3rd, 2020 at 3:43 pm
Another detail is how that machine uses up the computed exact runtime. Everything covered so far can be done in c + O(log n) states (and quickly), which leaves it still almost n states to play with â€” but not quite n, so just the fact that this computed number is < LB(n) is not quite enough â€” and worse, itâ€™s a parameter on the tape, not something it can hardcode into the design of its remaining states at runtime!

But, having it as a number is really most of the issue. Here is what it can do (I imagine this is mostly repeating ideas in the existing work on limits on LB, though I havenâ€™t read those):

â€“ first itâ€™s important to realize that this machine doesnâ€™t have to do all the work. Its design can include guesses, and as long as we can prove at least one combination of guesses will work, the corresponding machine fills in the hole and achieves the contradiction. The others try and harmlessly fail by using up â€œwrong guesses about the total runtimeâ€.

â€“ So the main thing this machine does is count down from the computed desired runtime to 0. But of course that doesnâ€™t work as stated, since counting down takes more than one step per count.

â€“ But if it uses a properly coded algorithm for a prespecified fixed number-size (that size is coded into the machine in binary, and the size of *that* number is hardcoded as one of the â€œdesign guessesâ€ mentioned above), then it can count down in a way that takes the exact same time per count, which is O(size of original number). (To count down by 1, it scans over the entire number and back. To scan the right amount of tape, it pulls along a smaller counter that tells it where it is, or maybe it just premakes a special mark at the end, storing each main-number-bit in two cells.)

â€“ We can also require that the ratio of steps / count is a power of two, for convenience. Then instead of counting from the original number, it counts down from some truncation of it, by a hardcoded amount.

â€“ The remaining error is small and is just used up by a hardcoded â€œdesign guessâ€. Even this part still has n â€“ O(log n) â€“ c states to be coded in, so with luck, itâ€™s easy (by appealing to known limits on LB to show a machine to fix that error exists in that many states).

Bruce Smith Says:
Comment #286 August 3rd, 2020 at 3:57 pm
If this works at all, I guess it proves something much stronger than â€œLB with unary input is not in Pâ€, more like â€œit takes almost-exponential timeâ€. But I didnâ€™t try to work out the exact limit, and it might even depend on some details of how the contradicting-machine is constructed.

Bruce Smith Says:
Comment #287 August 3rd, 2020 at 4:06 pm
One thing I might have missed â€” my claim about simulating k steps in ck steps was not â€œcarefully worked outâ€, and if I try to do that now, just having noticed the issue, what Iâ€™m sure of is not quite as much. Iâ€™m pretty sure itâ€™s still useable, though. Probably itâ€™s easiest to just claim (1) the simulation of k steps takes within poly(k) steps (probably at worst O(k^2)) (2) the simulation leaves the exact number of steps it actually took (as opposed to k itself), as a binary number on the tape, after itâ€™s done (with the tape having the final tape of the sim on one side or in one â€œsimulated tapeâ€ (eg every third cell of the real tape, with also a guaranteed end-marker of the reached point in another cell), and this number of real steps as a binary number on the other side).

Bruce Smith Says:
Comment #288 August 3rd, 2020 at 4:09 pm
Furthermore, the details of how to construct that simulator, eg the max size of â€œnumber of steps counterâ€ it supports, are hardcoded into it as another of those â€œdesign guessesâ€.

Bruce Smith Says:
Comment #289 August 3rd, 2020 at 4:44 pm
Another way it could fail would be if LB(n) was too small! But I think you said itâ€™s known to be at least |T(n)|/c^n for some c. So this is no problem if we just want to prove it canâ€™t be computed in P, but I suppose it matters if we really try to optimize how high is the runtime we can prove it canâ€™t be done in. Could that runtime depend on the unknown actual value of LB? That is, â€œif LB is like this then the limit is this in terms of LB, otherwise itâ€™s this other absolute limitâ€?

Scott Says:
Comment #290 August 3rd, 2020 at 4:51 pm
Sniffnoy #278: One sneaky way that you might imagine saving your idea, would be to exploit a peculiarity of the Turing machine modelâ€”namely, that you might never actually know for certain whether youâ€™re dealing with the empty input, or whether your tape head has just been initialized in the middle of a giant field of 0â€™s, with 1â€™s in the far yonder distance! So you could, e.g., restrict attention to Turing machines that are constrained to run in polynomial time assuming that they ever find input delimeters, and then maximize their running time assuming that they in fact never find such delimeters.

Alas, besides being artificial, I donâ€™t think this approach can escape BB-style growth, because one could always satisfy the definition via a machine that simulates a Busy Beaver until it encounters input delimiters, at which point it halts. In fact, I believe youâ€™ll still have

BB'(n) â‰¥ BB(n â€“ cn/ log n)

via introspective encoding. But I certainly donâ€™t see how to do better than that.

Scott Says:
Comment #291 August 3rd, 2020 at 5:01 pm
Bruce Smith #282: Kudos on your â€œpessimizationâ€ of the Wythagoras machine! However, even if my survey werenâ€™t already â€œoff to the printer,â€ thereâ€™d be an interesting philosophical question about whether to credit you for having set a new n=7 record. For Wythagorasâ€™ lower bound, as stated in my survey, was ğµğµ(7)>102Ã—10101018,705,353, and your pessimization shouldnâ€™t affect that bound at all! ğŸ™‚

Job Says:
Comment #292 August 3rd, 2020 at 5:37 pm
Since BB(n) grows so fast, if it were to ever approach LB(m) for some huge m, much larger than n, that would be a problem right.

Because weâ€™d be able to use the m-n states to easily pad BB(n) up to LB(m), which is not possible?

Can we say that BB(n) is never anywhere near LB(m) for any m > n, no matter how large m gets?

Scott Says:
Comment #293 August 3rd, 2020 at 5:59 pm
Bruce Smith #281, #283-289 (!): Sorry for the delay! I wanted time to think about this.

I think your idea for showing by contradiction that thereâ€™s no poly(n)-time algorithm to compute LB(n) could plausibly work, and itâ€™s awesome! Let me restate the idea in my own words:

Suppose A is a polynomial-time algorithm to compute LB(n) given n. Let A correspond to a c-state Turing machine. Then for all n, there should be a c+O(log n)-state Turing machine that first writes n onto the tape, then runs A to compute LB(n), and finally stalls until itâ€™s run for precisely LB(n) steps. But since c+O(log n)<<n for large n, this would mean there was also an n-state machine that ran for precisely LB(n) steps, which contradicts the definition of the Lazy Beaver function, QED.

As you say, there are a few annoying details to be worked out here, including:

(1) How does A know exactly how many steps it itself ran for? (In practice, we might need to envelop A inside a simulating shell that runs for a still polynomial but larger and more â€œtransparentâ€ number of steps.)

(2) What gadgets do we use to force our new machine to run for exactly LB(n) steps, no more and no less? (I completely agree that this will probably involve first approximating the number of steps within a small additive constant, then switching over to some gadget that gives us LB(n) exactly.)

(3) How do we show that LB(n) is large enough for this strategy to succeed? (This, of course, should just be a matter of formalizing my argument for why LB(n)>|T(n)|/cn, which indeed relies on the same idea that I just sketched in (2), for getting precise control over runtimes.)

I also agree that, if this works at all, then it ought to work not merely to show LB(n) isnâ€™t computable in poly(n) time, but to show itâ€™s not computable in any amount of time much less than LB(n) itselfâ€”and hence, I think, that at least nn-O(n/log n) time is needed.

As you pointed out, there are some annoying issues with stating this potential result in terms of conventional complexity classes, although I donâ€™t much care. One could either say that LB (with n encoded in unary) is not in FP, or that LB (with n encoded in binary) is not in FEXP, requiring doubly-exponential time.

If this works, then the question of whether computing LB is complete for Unary-FEXP or FEXPEXP (depending on how you formalize it) becomes even more interesting for me than it had been. If itâ€™s not complete for a superpolynomial-time class, then weâ€™d get possibly the first example ever of a problem that can be proved unconditionally to be outside P, despite not being complete for such a class. On the other hand, like you, Iâ€™m at a loss right now for how to prove the problem complete.

(Note that itâ€™s not an issue here that EXP means 2poly(n) time, because completeness reductions can blow up the inputs by polynomial sizes anyway.)

Anyway, whatâ€™s your day job, and would you like to collaborate further on this? ğŸ™‚ (Iâ€™d invite you to visit Austin to work on it, but alas, that will probably have to await the post-covid eraâ€¦)

Scott Says:
Comment #294 August 3rd, 2020 at 6:04 pm
Job #292: No. LB(m) certainly will exceed any fixed value as m gets large, including BB(n) for any fixed n. This is not a problem. It simply means that LB(m) must be large enough, and incompressible enough, that if we tried to pad out an n-state Busy Beaver so that it ran for exactly LB(m) steps, weâ€™d necessarily add more than m-n states.

Bruce Smith Says:
Comment #295 August 3rd, 2020 at 6:44 pm
Scott #291, I agree that my improvement to Wythagorasâ€™s result is so low-level as to almost not matter. Itâ€™s not interesting as a significant change in the bound or as a â€œnew ideaâ€ for getting the bound to be so high â€” itâ€™s mainly interesting just as a demo of how you can often pessimize handmade machines by these sorts of low-level changes. But maybe it does matter that, at this moment, no known BB candidate has an unused instruction. (As I mentioned earlier, one of your conjectures can only be true if no BB machine has one.)

Bruce Smith Says:
Comment #296 August 3rd, 2020 at 6:46 pm
(Or at least, not if that unused instruction is in A1. I guess I donâ€™t fully know this for the other instruction slots! Interesting question.)

Bruce Smith Says:
Comment #297 August 3rd, 2020 at 6:59 pm
Scott #293: Thanks! (Iâ€™m relieved that if there is some mistake in there, itâ€™s not obvious enough for you to have noticed it immediately! ğŸ™‚

Fortunately my day job leaves me a reasonable amount of free time, certainly enough to collaborate remotely, which I would love to do!

One thing I wondered â€” I infer from wikipedia and also from complexity zoo that proving P != co-NP would imply P != NP. Is that right? If so, that makes the following question more interesting (though it might be interesting even if not):

â€“ note that a machine M of n states is a witness for s(M) being a possible runtime for n states, which can be verified in time s(M). But the machines weâ€™re talking about now are near-exponential time in terms of n. Might we be able to pad their inputs somehow (even more than by expressing n in unary) to bring their natural time limit closer to P? If so, is it interesting that we have a problem here (whether exact time m is possible for an n-state machine â€” at least in the case where m happens to be LP(n), I didnâ€™t check whether this all works for general m) which we might prove a minimum runtime for (in terms of n), but for which â€œyes, itâ€™s possibleâ€ has a presumably-much-faster-to-verify-than-to-find witness?

Job Says:
Comment #298 August 3rd, 2020 at 6:59 pm
It simply means that LB(m) must be large enough, and incompressible enough, that if we tried to pad out an n-state Busy Beaver so that it ran for exactly LB(m) steps, weâ€™d necessarily add more than m-n states.

OK i guess BB(n) is never less than LB(m-n) steps away from LB(m), for any m > n? ğŸ™‚

Since itâ€™s easy to produce an (m-n)-state machine that runs for less than LB(m-n), to be appended to BB(n), anything closer than that would result in a contradiction.

Bruce Smith Says:
Comment #299 August 3rd, 2020 at 7:06 pm
About my last question, I guess if you have that witness, it gives you about a quadratic speedup (number of machines to check is about the same as time you must run them when checking them), so probably this has no bearing on P != co-NP, since itâ€™s ok if those two polys have different degrees.

Scott Says:
Comment #300 August 3rd, 2020 at 7:08 pm
Bruce Smith #297: Yes, P=?NP and P=?coNP are simply the same question. P=NP would therefore imply NP=coNP, but NP=coNP isnâ€™t known to imply P=NP.

And yes, â€œthere exists an n-state TM that runs for exactly k stepsâ€ has a convenient witness in the form of the TM itself. But itâ€™s not clear how useful that is, since as you say, when k is large enough that this question is interesting, itâ€™s already nearly as large as the number of n-state TMs itself (in particular, exponential). I didnâ€™t understand your remark about paddingâ€”sure, we could pad to m>>n states, but then LB(m) will become much greater than our machineâ€™s running time, leaving us no better than when we started, no?

Bruce Smith Says:
Comment #301 August 3rd, 2020 at 7:10 pm
Job #298:

â€“ remember that you canâ€™t just append two machines and add their empty-tape runtimes, since the second one to run sees all the junk left by the first one on the tape. If not for this issue, we could prove BB(n+m) â‰¥ BB(n) + BB(m).

â€“ also remember that for LB, much or all of the hard part is exactly matching a desired runtime, not just getting close to it or exceeding it.

Bruce Smith Says:
Comment #302 August 3rd, 2020 at 7:23 pm
Scott #299, our posts which â€œcrossed in the mailâ€ come to the same conclusion, that this idea is not going to help with P != co-NP in any obvious way. And I think they conclude that for the same reason in different words. But even so, Iâ€™ll explain what I meant about padding.

I was talking about padding the way we encoded the same input n, to move the witness-verifying problem (in terms of its input size, rather than in terms of n) from EXP to P (and the witness-finding-and-verifying problem to something smaller than it is now, but, I hoped, not as small as P), rather than changing the value n itself.

But I realized I canâ€™t do this, since the natural runtimes are related by the slower one being about the square of the faster one, so if I adjust things to get the faster one in P, the slower one will also be in P, just with a higher degree of polynomial.

Bruce Smith Says:
Comment #303 August 3rd, 2020 at 7:42 pm
My answer to Job #298 suggested the following, which I think gives an example of a class of improved theorems of the form BB(n + c) â‰¥ BB(n) + k for various pairs of constants c and k â€” well, I think not quantitatively improved compared to Prop. 15 (though Iâ€™m not sure that failure is provable), but different enough to be worth mentioning:

define BBclean(n) as the longest runtime of an n-state machine run on an empty tape, where the machine is also required (when run on an empty tape) to leave the tape empty when itâ€™s done.

Then BB(n + c) â‰¥ BBclean(c) + BB(n). Proof â€” just append the machines with the clean one running first.

But BBclean(c) is probably larger than BB(c + the same extra stuff as in Theorem 16), since you can simulate the machine, track and mark the extent of its use of the tape, and then clean the tape after itâ€™s done. (I didnâ€™t prove this but I think itâ€™s pretty simple. In fact, it must have been proved as part of proving Theorem 16, since it would be hard to make use of a â€œdirty tapeâ€ even if it contained the number you wanted it to contain in a readable way.)

And for various constants c, that ought to give you k which grows a lot compared to c.

So Prop. 14 can be extended to still-small c but large k. But since the result is additive, itâ€™s probably still no match for Prop 15 as it stands now, whose result multiplies the runtime.

Job Says:
Comment #304 August 3rd, 2020 at 9:27 pm
Bruce #301,

remember that you canâ€™t just append two machines and add their empty-tape runtimes, since the second one to run sees all the junk left by the first one on the tape. If not for this issue, we could prove BB(n+m) â‰¥ BB(n) + BB(m).

I see, the tape states do get in the way of concatenating two machines so itâ€™s not that simple, too bad.

Also, question on your idea to show that LB(n) is not in P, using Scottâ€™s version:

Suppose A is a polynomial-time algorithm to compute LB(n) given n. Let A correspond to a c-state Turing machine. Then for all n, there should be a c+O(log n)-state Turing machine that first writes n onto the tape, then runs A to compute LB(n), and finally stalls until itâ€™s run for precisely LB(n) steps. But since c+O(log n)<<n for large n, this would mean there was also an n-state machine that ran for precisely LB(n) steps, which contradicts the definition of the Lazy Beaver function, QED.

Iâ€™m wondering how this would distinguish between a machine that actually computes LB(n) vs a fake one that just outputs precomputed values of LB(n) (for large enough n to show the contradiction), or even one that guesses LB(n) randomly.

Wouldnâ€™t the Turing machine construction in the proof also manage to take a fake or lucky polynomial-time LB(n) solver and successfully pad it to run for exactly LB(n)?

Iâ€™m assuming that would be a problem since we can already produce polynomial-time machines that are partial solvers for LB(n) and those should not lead to a contradiction.

Bruce Smith Says:
Comment #305 August 3rd, 2020 at 10:39 pm
My reasoning in #299 was quite wrong, and my claim in #302 that it was the same as Scottâ€™s reasoning was therefore probably also wrong. Iâ€™ll post more on that later, but the short version is, itâ€™s even easier to see that this wonâ€™t help with P != co-NP that I thought it was, and itâ€™s for a more fundamental reason.

OTOH there are some interesting generalizations of this â€” other functions than LB, which the same method can prove are not in P, or, I think, not in any TIME(f(n)) class you like (at least out of a wider choice of them â€” I better not claim too much before working out details!). Iâ€™ll post that too, shortly.

Bruce Smith Says:
Comment #306 August 3rd, 2020 at 11:05 pm
Job #304: â€œIâ€™m wondering how this would distinguish between a machine that actually computes LB(n) vs a fake one that just outputs precomputed values of LB(n) â€¦â€

[summary: your question inspired one or two interesting extensions of what we can prove!]

Thatâ€™s a really good question! I had not thought of it before. But the same proof shows this also could not happen. But it also shows more, which I didnâ€™t notice until you asked this question. (Itâ€™s possible Scott noticed it, since maybe something he said earlier hinted at it and I didnâ€™t get that at the time. Heâ€™ll have to tell us.)

So â€” the same proof that shows our contradicting-machine could not quickly compute m (the impossible runtime), also shows it could not hardcode m (in the space it has available in its states) (because doing so would lead to the same contradiction, since it could use the value just as well in that case).

At one level that is not surprising â€” we know m can be almost as large as |T(n)| in terms of bits (the formula in Scottâ€™s paper for min possible value of LB is not much smaller than T(n) once you take logs), so hardcoding a value of that size would potentially use up most or all of the n states.

But I didnâ€™t work out the details of that comparison â€” Scott can tell us whether this idea actually can improve his lower bound on LB(n), or he might say he already used that idea (implicitly or explicitly) in his proof of that bound. (I donâ€™t know his proof.)

But, beyond this bound on the size of m (it is too big to be hardcoded into n â€“ c â€“ log(n) states or whatever), this argument also shows it canâ€™t be *compressed* into that many states, in a way that is efficient enough to decompress!

This argument is not strong enough to show K(LB(n)) is that large, since it says nothing about a kind of compression that is very slow to decompress.

But it does show, not only does our hypothetical algorithm A have no way to compute m in a certain time, *no machine of a certain number of states* (which is almost n) is able to compute it in that same amount of time!

There is no requirement that these machines, for different n values, are related to each other.

So I think we have proved not only that an *algorithm* for LB canâ€™t have a faster runtime than a certain runtime function (which is much higher than polynomial but less than exponential) â€” weâ€™ve proved that a *family of turing machines, one for each value of n*, with each one limited to some number of states a bit less than n, canâ€™t do that!

AFAIK this is not a concept Iâ€™ve heard of before â€” unless itâ€™s equivalent to a family of *circuits*, which actually, it might be. (Scott or any other expert will know instantly, I think.)

I guess each of those TMs is like a universal TM with about n log n bits of advice, and a circuit is sort of like that too (circuit size == number of advice bits), so maybe it *is* the same concept. Rather than trying to be 100% sure of that, Iâ€™ll wait for Scott to confirm it!

But actually, circuits in complexity theory usually have one output bit, but in this situation we have moreâ€¦ and I think thatâ€™s inherent in the situation, since if we ran a circuit with one output log m times to get log m output bits (giving it more inputs so we could tell it which bit we wanted), that would take substantially more runtimeâ€¦ but maybe that wouldnâ€™t matterâ€¦.

Bruce Smith Says:
Comment #307 August 4th, 2020 at 2:00 am
Since itâ€™s getting late, Iâ€™ll post only the â€œgeneralizationâ€ â€” the rest will have to wait til tomorrow.

There are functions different from LB with the same way of being proved not in P (or in certain other time-complexity classes).

First, what properties of LB are we actually using? It is not so much that it produces an impossible runtime that matters â€” rather, it produces a value that it is impossible for us to get from an n-state TM, in whatever specified way we like (i.e. the value of f(M) for some f we specify), provided we can compute f(M) within time g(n).

There are only |T(n)| values we can get that way, so in some larger set of potential values V(n), there are some we canâ€™t get. If we define h(n) as â€œsome value in V(n) which we canâ€™t get as f(M) for M in T(n)â€ (but also make sure h is deterministic and in fact computable, eg the first such value), then we can look for a way to make an n-state TM which *does* make that value, to give a contradiction.

So far, we have not even cared that the domain of f is a set of TMs rather than (say) a set of numbers, or if it is TMs, that we get values from them by running them rather than (say) taking rot13 of their bitstring and repeating that until itâ€™s long enough. All that matters is that we specified f: T(n) -> V(n) and |V(n)| > |T(n)| and h picks one of the values not in the range of f. (In fact itâ€™s enough if h sometimes fails and picks a possible value, as long as it picks an impossible one infinitely often.)

But in the next part the nature of f starts to matter, since to get the contradiction we need to be able to program the TM so that, if h runs fast enough that the TM can use it to think about hâ€™s output for very long, this is enough to let the TM *cause* f(itself) to be the forbidden value.

So what can M (a TM) do to influence f(M)? For arbitrary f thatâ€™s complicated to answer, but the only kind of example I yet know of, that is interesting here, is where f runs the TM and does something with values related to running the TM which the TM can control in an intelligent way.

Those values include: the exact runtime, the tape contents when it halts, the final state when it halts. The final state is not important since the TM could easily write it on the tape if desired, so we can ignore it.

For LB(), f(M) = s(M) and M can, with difficulty, control s(M) within a certain range.

But another choice is, for example, â€œf(M) = the first n^2 bits of output tape content (relative to current tape pos after halting), provided M halts within time g(n)â€. (Or even, those bits at step g(n), whether or not M has halted by then.) (Note that number of n^2-bit strings > |T(n)|.)

(Note that this is even more like a â€œcomplexity functionâ€ than LB is. More on that below.)

So letâ€™s see how it works out with that f.

The function analogous to LB (called h) is, described roughly, â€œthe first n^2-bit string that no n-state TM can output within time g(n)â€.

As with LB, we can prove h is well-defined. Itâ€™s also computable, by an algorithm which runs all n-state TMs for time g(n).

(So Mâ€™s runtime does still matter â€” but it doesnâ€™t have to be part of the value of f(M).)

What about the contradiction?

Suppose we can compute h â€œquicklyâ€ (TBD) using some family of slightly-less-than-n-state TMs, one per n. (For example, the ones that hardcode n in binary and feed it to a fixed TM of c states which implements a fast algorithm for h.)

Then for large enough n we can make the contra-TM which computes h(n) (and afterwards has the forbidden output on its tape as a 2^n-bit string), moves it into the right position quickly enough, then halts, so that forbidden output is this TMâ€™s output, giving the contradiction.

(This is analogous to â€œmake sure my runtime is exactly mâ€, but is easier for the TM to do. In fact, if we define things properly, the post-h part is nothing! The contra-TM is then as simple as â€œhardcode n and feed that to the h-algorithmâ€.)

(We could do similar things for an f() which used both the runtime and the output, but I donâ€™t know if we gain anything beyond just using the output alone. We could also vary how h picks a value that f canâ€™t produce, among all such values â€” for example, instead of LB, we could use â€œthe first impossible runtime at least as large as |T(n)|^2â€, which is obviously between |T(n)|^2 and |T(n)|^2 + |T(n)| (inclusive).)

(We could also allow h to examine TMs of various numbers of states, and let g depend both on hâ€™s parameter n and on the actual number of states used, as well as on the output and/or exact runtime. That is, it could enforce a tradeoff so that to violate its prediction a TM would need to either be small enough or run fast enough but could trade these off according to a specific relation. And perhaps there are even more things we can do.)

==

The meaning of â€œquicklyâ€ above (about the runtime of h that leads to contradiction) is â€œin a runtime slightly less than g(n)â€, but I wonâ€™t analyze that more closely here, except to point out that since Mâ€™s job is easier if it only tries to control output, this runtime can probably get closer to g(n) in that case than in the LB case where f also cares about Mâ€™s exact runtime. For example, when f cares about exact runtime, M has to *simulate* h (within a time limit), but if f only cares about Mâ€™s output, then M only has to *run* h directly (within the same time limit, so fitting into it is easier).

==

So thatâ€™s enough about the generalization itself, unless I forgot something due to being tired now.

For which time-complexity classes could we use this? It seems like g(n) could get pretty large, since no matter how large it is, clearly only |T(n)| outputs are possible, so h still exists, and the nature of the contradiction stays about the same. In all cases the provable min runtime of h is a bit less (in some sense) than g(n).

It could also get pretty small, with the only limiting factor that the overhead in the TMâ€™s using the output of h (and simulating h if it needs to also control its own exact runtime) might start to get too large by comparison to get a good limit.

==

I mentioned above that a certain kind of h function is similar to a complexity measure on the values V(n). In fact, itâ€™s a pretty natural generalization of K(v) to ask not for the size of the smallest program that can *ever* compute v, but the smallest one that can do it in a certain time. And then this h(n) function is defined as â€œthe first v not computable by an n-state program in time g(n)â€, and what weâ€™re proving about h is, roughly, that it itself canâ€™t be computed in that time. Since it outputs a v that it just said canâ€™t be computed in that time, this is almost tautological! The only thing obscuring that is the dependence of some of these functions or limits on the allowed number of states of the machine that computes it.

So it wouldnâ€™t surprise me if this form of the idea (about TM output rather than TM exact runtime) (including the theorem about hâ€™s runtime in that case) has been discovered before, as part of an exploration of generalizations of K-complexity. But if not, itâ€™s even more interesting than if this was only true about the LB function!

Bruce Smith Says:
Comment #308 August 4th, 2020 at 2:07 am
Job #304, What did you mean by â€œâ€¦ we can already produce polynomial-time machines that are partial solvers for LB(n)â€¦â€?

I didnâ€™t quite notice that when I replied first. If we really could produce those, that *would* be a problem, but I am not aware that we can (if I understand properly what you mean â€” it sounds like â€œmachines that can compute LB for some values of nâ€, but maybe you meant something more like â€œmachines that can delay for some number of steps compatible with the output of LBâ€).

Bruce Smith Says:
Comment #309 August 4th, 2020 at 2:09 am
Hereâ€™s another formatting error in the preview â€” with no spaces in f( TM ) I get f(TM), which in preview looks like f with a superscript of TM in small print (ie as if f was a trademark).

Sniffnoy Says:
Comment #310 August 4th, 2020 at 3:50 am
Toby Ord #274:

Oh interesting. I hadnâ€™t seen that before. So letâ€™s seeâ€¦

The accepted (but not at all upvoted) answer, by Simply Beautiful Art, suggests remedying the problem by always taking the larger of the two values. Huh. Kind of inelegant, but I guess it is something nontrivial to study.

The highest-voted answer, by Mike Battaglia, gives a way of looking at it from which it does make sense to view the Veblen hierarchy as hyper operations. Interesting! Iâ€™d never thought of that before.

The next answer, by Simply Beautiful Art once again, only covers tetration and seemsâ€¦ pretty arbitrary? This seems worth ignoring to me.

And then the next answer, by Alec Rhea, seems to just be â€œiterate on the rightâ€ and ignores the problem that this makes things small. <shrug>

And then the final answer by Timothy just comments on the usual problems that come up when you try to define it. So thatâ€™s not helpful.